# Hybrid 8-bit Floating Point (HFP8) Training and Inference for Deep Neural Networks

Xiao Sun, et.al. @ IBM T. J. Watson Research Center

## 0. Abstract

Reducing the numerical precision of data and computation is extremely effective in accelerating deep learning training workloads. Towards this end, 8-bit floating point representations (FP8) were recently proposed for DNN training. However, its applicability was only demonstrated on a few selected models and significant degradation is observed when popular networks such as MobileNet and Transformer are trained using FP8. This degradation is due to the inherent precision requirement difference in the forward and backward passes of DNN training. Using theoretical insights, we propose a hybrid FP8 (HFP8) format and DNN end-to-end distributed training procedure. We demonstrate, using HFP8, the successful training of deep learning models across a whole spectrum of applications including Image Classification, Object Detection, Language and Speech without accuracy degradation. Finally, we demonstrate that, by using the new 8 bit format, we can directly quantize a pre-trained model down to 8-bits without losing accuracy by simply fine-tuning batch normalization statistics. These novel techniques enable a new generations of 8-bit hardware that are robust for building and deploying neural network models.

在加速深度学习训练workload时，降低数据和计算的数值精度是非常有效的。为此，最近提出了用于DNN训练的8-bit浮点表示(FP8)。但是，其应用性只在几个挑选的模型上进行了展示，当使用FP8训练流行的网络，如MobileNet和Transformer时，会观察到明显的降质。这种降质是由于DNN训练的前向和后向过程中，其内在的精度需求差异。使用理论上的洞见，我们提出了一种混合的FP8格式，和DNN端到端分布式训练过程。我们证明了，使用HFP8，在很大范围内的深度学习模型训练都是成功的，包括图像分类，目标检测，语言和语音模型，没有准确率的下降。最后，我们证明了，使用新的8 bit格式，我们可以将一个预训练的模型直接量化到8 bit，只要精调一下批归一化的统计结果，就不会有准确率的损失。这些新技术产生了新一代的8 bit硬件，对构建和部署神经网络模型是稳健的。

## 1. Introduction

As Deep Neural Networks (DNNs) evolve rapidly and as models get more complex, training times have increased significantly. To mitigate this challenge, efficient training through reduced precision exploitation has become increasingly important. Using reduced precision for data representations and general matrix multiplications (GEMM) can accelerate DNN training dramatically and save significant computing time and power. Indeed, GPUs can already perform mixed-precision training with 16-bit IEEE Half-Precision floating point formats for deep learning tasks [1]. Recently, a new (1-5-2) (sign-exponent-mantissa) floating-point 8-bit format (FP8), was used to successfully train popular ImageNet models [2] without much accuracy loss. In addition, 8-bit Fixed point formats (INT8) have also been explored to train ResNet50 successfully although 1 of the 3 GEMM computations was performed in higher precision [3]. In addition to DNN training, efficient low-precision deployment is critical in a wide range of edge inference use cases where cost and energy constraints can limit performance [4]. Towards that end, Trans-Precision inference, where models are trained in higher precision and deployed in lower precision formats, have become extremely important [5, 6, 7].

随着深度神经网络的迅速演化，模型变得越来越复杂，训练时间有显著增加。为弥合这种挑战，通过降低精度来进行高效的训练，这变得越来越重要。使用降低的精度来进行数据表示和GEMM，可以极大的加速DNN的训练，显著的节省计算时间和能耗。确实，GPUs已经可以对深度学习任务进行混合精度的训练，用的是16 bit IEEE半精度浮点格式。最近，一种新的浮点8-bit格式(1-5-2, sign-exponent-mantissa)成功的用于训练流行的ImageNet模型，没有很多的准确率损失。另外，8-bit定点格式(INT8)也用于成功的探索ResNet50的训练，虽然3个GEMM计算其中的一个是用更高的精度来进行的。除了DNN训练，高效的低精度部署，在很多边缘推理使用情况中，也是很关键的，其中代价和功耗的约束会限制性能。为此，跨精度的推理，其中模型在高精度格式下训练，在低精度格式下部署，已经变得非常重要。

While 8-bit training techniques have progressed rapidly, recent work [2, 3, 8, 9] have only demonstrated its applicability on a small subset of deep learning models-focused around convolution networks such as ResNet [10]. Indeed, plethora of challenges exist to extend FP8 training to a broader spectrum of applications such as image classification, object detection, speech and natural language processing while preserving model accuracy. Furthermore, in large-scale distributed training systems, FP8 acceleration of GEMM and Convolution operations within each learner makes the communication between learners at the weight update step a critical bottleneck. Alleviating this bottleneck using 8-bit communication schemes could substantially improve the end-to-end training performance for distributed DNN training. In addition, for low-precision inference, fixed point techniques involving costly retraining of networks for ultra-short bit-widths [11, 12, 13] as well as post-training quantization for simpler deployment of the INT8/INT4 inference models [7, 14, 15, 16] have been extensively explored, but the state-of-the-art techniques still lose significant model accuracy when they are applied to compact models like MobileNet [17] on large datasets (e.g., ImageNet). In comparison to the fixed-point representation, FP8 based schemes have a wider dynamic range and do not need to find the right quantization range for each layer and channel-serving post-training quantization more naturally.

虽然8-bit训练技术已经有了很大发展，但最近的工作只证明了在少数深度学习模型中的应用，比如像ResNet这样的卷积网络。确实，将FP8的训练拓展到更多的应用中，如图像分类，目标检测，语音和自然语言处理，同时保持模型的准确性，这存在很多挑战。而且，在大规模分布式训练系统中，GEMM和卷积运算在每个learner中的FP8加速，会使在权重更新时，在learner之间的通信成为关键的瓶颈。使用8 bit通信方案缓解这个瓶颈，能从根本上改进分布式DNN训练中端到端的训练性能。此外，对于低精度推理，定点技术会涉及到昂贵的网络重训练，对更简单的INT8/INT4推理模型的部署的训练后量化，也都进行了广泛的探索，但目前最好的技术在应用到大型数据集的紧凑模型中，如MobileNet，还是会损失显著的模型准确率。与定点表示比起来，基于FP8的方案有更大的动态范围，不需要对每层找到正确的量化范围和分通道的训练后量化。

### 1.1 Chanllenges and Related Works

**Training and inferencing smaller capacity models in 8-bit**: A key challenge in (1-5-2) FP8 training is the ability to train smaller capacity models without losing accuracy. While networks such as VGG [18] and ResNet [10] can be trained effectively, models like MobileNetV2 that have 1/7th the capacity of ResNet50 suffer significant degradation (∼ 1%) if trained in FP8, as shown in Fig.1(a). This training problem is further exacerbated when we reduce the layer width of MobileNetV2 by 0.5 and 0.35 — resulting in 2 ∼ 3% degradation. Furthermore, as discussed in the previous section, inferencing on the post-training quantized MobileNetV2 models [7] using INT8 and INT4 formats results in significant accuracy degradation (>2%). Recent work has identified the small variance of the depthwise convolution layers as the cause of such degradation [14], which has been partly addressed in [15] by retraining the networks with the adaptable dynamic range. Techniques that can resolve this low-precision training challenge for smaller capacity models and simultaneously avoid the issues in post-training quantization can be extremely important for Edge deployment use cases.

(1-5-2) FP8训练的一个关键挑战是，训练更小容量模型，而不损失准确率的能力。如果用FP8训练，可以对VGG和ResNet这样的网络进行高效的训练，而MobileNetV2这样的模型只有ResNet50这样模型的1/7的容量，则会有显著的降质(~1%)，如图1a所示。当我们将MobileNetV2的层宽度降低到0.5和0.35，这个训练问题会进一步加剧，得到2%~3%的降质。而且，就像在前一节中讨论的，在训练后量化的MobileNetV2模型上，使用INT8和INT4格式上进行推理，会得到显著的准确率下降(>2%)。最近的工作发现，depthwise卷积层的small variance是这种降质的原因，[15]通过用可调节动态范围的重训练网络来部分的解决了这个问题。可以解决这个小容量模型的低精度训练的挑战，并同时避免训练后量化的问题的技术，在边缘部署的使用情况下会是非常重要的。

**Applicability of 8-bit Training to Other Domains**: In Natural Language Processing(NLP) and Speech domains, popular networks built on LSTMs and Transformer blocks perform simple matrix multiplications using fully-connected (FC) layers rather than convolution operations. Training these networks using FP8 has proven to be a significant challenge. As an example, we’ve observed slow convergence and lower BLEU scores on the Transformer model on the WMT14 dataset trained using (1-5-2) FP8 precision, as shown in Fig.1(b). Furthermore, in many state of the art Speech and language models, the last FC layer has a very large dimension—corresponding to vocabulary size(typically 10-100 times larger than ImageNet) [19, 20]. As a result, the last FC layer consumes a significant fraction (> 20-30%) of the total computation time. Currently, 8-bit training solutions customized for convolution nets relax the last layer precision by keeping that layer in 16-bit (FP16) since last layers computed in FP8 have shown to increase classification error rates [2]. However, this solution is expensive for NLP and Speech tasks which have large last layers. In addition, Object Detection and semantic segmentation networks such as MaskRCNN [21] and SSD [22] that load a pre-trained ImageNet backbone model and fine-tune it with an Object Detection dataset have not been investigated within the framework of 8-bit training. Finally, Trans-Precision Inference in (1-5-2) FP8 (directly converted from FP32 trained models) results in significant accuracy degradation in many of these models as shown in the table of Fig.1(c). The goal of this paper is to enable an 8-bit training methodology that addresses all of the above challenges.

在NLP和语音的领域，在LSTMs和Transformer模块上构建起来的流行网络，进行的是简单的矩阵乘法运算，使用的是全连接层，而不是卷积运算。用FP8训练这些网络是一个很大的挑战。一个例子是，我们在WMT14数据集上用(1-5-2) FP8精度训练时，我们观察到收敛速度更慢，BLEU分数也更低，如图1b所示。而且，在很多目前最好的语音和NLP模型中，最后的FC层的维度很大，对应的是字典大小（一般比ImageNet大10-100倍）。结果是，最后的FC层占总计计算时间相当大的比例(> 20-30%)。目前，对卷积网络定制的8-bit训练方案，将最后层的精度进行松弛，将这个层保持在16-bit (FP16)，因为在FP8精度下计算的最后的层会增加分类错误率。但是，对于NLP和语音任务，这个解决方案是很昂贵的，因为最后的层会非常大。此外，目标检测和语义分割网络，比如Mask R-CNN和SSD，会载入预训练的ImageNet骨干模型，然后用目标检测数据集进行精调，这还没有用8-bit训练的框架进行研究。最后，(1-5-2) FP8的跨精度推理（从FP32训练模型直接转换而来），在很多模型会得到显著的准确率降质，如图1c中所示。本文的目标是找到一种解决上述挑战的8-bit训练方法。

**8-bit weight updates**: The weight update phase of low precision training requires a master copy of weights in higher precision to accumulate gradients across minibatches without losing critical information due to "swamping" [23]. In INT 8 training, FP32 is used for this master copy, resulting in increased latency due to the bandwidth needed for communication and AXPY (Y = AX + Y) computation in 32-bits. In FP8 training, even with stochastic rounding techniques, 16-bit (FP16) weights are still needed for the master copy to preserve convergence [2]. As a solution to this problem, weight averaging has been proposed to facilitate exchange of 8-bit weights (while keeping higher precision weights locally). This scheme, however, results in >4% accuracy degradation on ResNet18/ImageNet [24]. An ideal weight update scheme should compress gradients and only compute and communicate 8-bit weights during the training process.

低精度训练的权重更新阶段，需要高精度权重的master copy，以在minibatches之间累积梯度，同时不因为swamping损失关键信息。在INT8训练中，FP32用于master copy，由于用32-bits用于通信和AXPY计算需要带宽较大，导致延迟增加。在FP8训练中，即使用随机四舍五入技术，仍然需要16-bit (FP16)权重用作master copy，以保持收敛性。这个问题的一个解决方法是权重平均，以促进交换8-bit权重（同时保持本地更高精度的权重）。但是，这个方案会造成ResNet18/ImageNet的准确率有>4%的准确率下降。理想的权重更新方案，应当压缩梯度，在训练过程中只计算8 bit权重并进行通信。

### 1.2 Contributions

In this paper, we introduce a new hybrid FP8 format and technique that is applicable to both computations (training and inference) and communication to address all of these challenges. In comparison to the state-of-the-art FP8 training and INT8 inference solutions, our primary contributions include:

本文中，我们提出了一种新的混合FP8格式和技术，可以在训练和推理计算和通信中应用，解决了上述所有挑战。与目前最好的FP8训练和INT8推理解决方案相比，我们的主要贡献包括：

1. A novel hybrid FP8 format that uses 4 exponent bits and 3 mantissa bits (1-4-3 with an exponent bias) for forward propagation and 5 exponent bits and 2 mantissa bits (1-5-2) for backward propagation - achieving negligible accuracy degradation on previously problematic models including MobileNetV2 and Transformer.

一种新的混合FP8格式，在前向传播时使用4个exponent bits和3个mantissa bits (1-4-3)，对后向传播使用5个exponent bits和2个mantissa bits，在之前有问题的模型上的准确率下降几乎可以忽略，包括MobileNetV2和Transformer。

2. Demonstrated the robustness of the HFP8 format on a wide spectrum of DNN tasks including Image Classification, Object Detection, NLP and Speech—while fully preserving accuracy.

在大量DNN任务上展示了HFP8格式的稳健性，包括图像分类，目标检测，NLP和语音任务，同时完全保持了准确率。

3. Through theoretical analysis, we’ve identified BN statistics as the primary reason for accuracy loss in low-precision Trans-Precision inference and show that BN statistics could be fine tuned to fully recover model accuracy while using our 1-4-3 FP8 precision.

通过理论分析，我们发现BN统计数字是低精度跨精度推理中准确率损失的主要原因，展示了在使用我们的1-4-3 FP8准确率时，BN统计数字可以被精调，以完全恢复模型准确率。

4. Introduced a deterministic FP8 weight update scheme that can converge to baseline accuracies without using stochastic rounding along with a compatible all-reduce technique that takes advantage of low bit-width weights to speed up distributed learning.

提出了一种确定性的FP8权重更新方案，可以收敛到基准准确率，而不需要使用随机四舍五入，同时还有一个兼容的all-reduce技术，利用了低bit宽度权重，以加速分布式学习。

## 2. New Hybrid FP8 (HFP8) Formats and Computations

### 2.1 Impact of FP8 formats on Trans-Precision Inference (Post-Training Quantization)

In this section, we explore how different FP8 precision formats for activations and weights impact Trans-Precision Inference accuracy. Towards that end, we adapt the theoretical framework of Sakr et al. [25] to quantify the mismatch probability between a reduced precision neural network and its full-precision counterpart. Consider a neural network for a classification task such as MobileNetV2, with quantized weights (W + q_w) and activations (A + q_A), where each numerical output (Zi) after the feedforward pass may be corrupted by a quantization noise (q_zi). Using Taylor’s expansion and ignoring the cross-products of quantization noise terms, the total quantization noise qzi can be expressed as [25]:

本节中，我们探索了不同的FP8精度格式用在激活和权重值上，怎样影响跨精度推理的准确率。为此，我们采用了[25]的理论框架，来量化降低精度的神经网络和其完整精度版本的不匹配概率。考虑一个神经网络进行分类任务，如MobileNetV2，量化的权重w+qw和激活A+qA，其中前向过程后的每个数值输出Zi可能被量化噪声qzi所污染。使用Taylor展开，并忽略量化噪声项的点积，总计量化噪声可以表达为：

$$q_{zi} = \sum_{ah∈A} q_{ah} \frac {∂zi} {∂ah} + \sum_{wh∈W} q_{wh} \frac {∂zi} {∂wh}$$(1)

where A and W are index sets. By evaluating the probability of any pair of outputs (zi < zj) that flipped due to quantization errors Pr(zi + qzi > zj + qzj), the mismatch probability pm between the reduced precision network and its full precision baseline yields an upper bound - defined by the quantization error of each activation and weight multiplied by the corresponding gradients called “gain”. As the gains are network specific, we can evaluate them empirically using Eqn.1.

其中A和W是索引集。评估任意一对由于量化误差结果翻转的输出的概率(zi < zj)，Pr(zi + qzi > zj + qzj)，在降低精度的网络和其完整精度基准之间的误匹配概率pm会产生一个上限，是由每个激活和权重，乘以对应的梯度，称为gain，定义得到的量化误差。由于这些gains是随着网络的不同而不同的，我们可以通过经验用式1来评估。

Fig.2 shows the computed mismatch probability due to activation and weight quantizations for each layer of the MobileNetV2 (CIFAR-10) model. The results clearly show that by moving just one bit from the exponent (1-5-2) to the mantissa (1-4-3), the mismatch probability corresponding to both activations and weights decrease dramatically. This improvement comes from the fact that weights and activations are represented with higher fidelity using the extra mantissa bit.

图2展示了在MobileNetV2(CIFAR-10)模型中，每一层由于激活和权重的量化导致的计算误匹配概率。结果清晰表明，从exponent移动一个bit到mantissa，即(1-5-2)到(1-4-3)，对应到激活和权重的误匹配概率会显著降低。这种改进是来自于，使用额外的mantissa bit，权重和激活都可以表示为更高的精度。

However, since the total bit-width is limited to 8, reduction in the exponent bit-width can result in clamping of large weights and activations and/or truncation of small values to the minimum representable value in (1-4-3). Given the typical numerical distribution of these tensors during training, we found that underflow represents a more serious concern. To mitigate this effect, we introduce a fixed exponent bias that shifts the coverage range of the (1-4-3) FP8 format to [2^{-2^{ebit-1}-bias+1}, \frac{2^{mbit+1}-1}{2^{mbit}}×2^{2^{ebit-1}-bias}].

但是，由于总共的bit宽限制为8，exponent位宽的降低，会导致在(1-4-3)中，大的权重和激活值的饱和截断，和/或较小的值由最小的值来表示。训练中这些张量的典型数值分布是已知给定的，我们发现underflow是更严重的考虑。为弥补这种效果，我们引入了一个固定的exponent bias，将(1-4-3) FP8格式的覆盖范围移动到[2^{-2^{ebit-1}-bias+1}, \frac{2^{mbit+1}-1}{2^{mbit}}×2^{2^{ebit-1}-bias}]。

By choosing an exponent bias of 4, we intend to better align the (1-4-3) format with the distributions of activations and weights seen in a wide range of DNN layers and models. As verified in Fig.2, introducing an extra bias of 4 on the exponent further reduces the impact of quantization—specifically on the weights in the lower layers which appear to have much smaller magnitudes. In the (1-4-3) FP8 with bias=4 format, we reduce the maximum clamping value from 480 down to 30, large enough to cover the wide variety of networks that we have investigated. In exchange, we are able to represent smaller activations and weights down to 2^−11 (much lower than the 2^−7 truncation threshold in 1-4-3). For simplicity of notation, all the following (1-4-3) FP8 experiments have a default exponent bias of 4. These experiments indicate that the 5-bit exponent range 2^−15 − 2^16 is an overkill for DNN inference, and 4 bit exponents with a bias of 4 have sufficient range and fidelity to represent activations and weights for both training and Trans-Precision inference performance. Finally, we’ve verified that these conclusions extend to a large number of neural network models and topologies.

选择的exponent bias为4，我们的目的是，将(1-4-3)格式与大量DNN层与模型中的激活和权重的分布对齐。如图2所示，对exponent引入额外的bias 4，会进一步降低量化的影响，尤其是早更低层上的权重，其值的幅度更小。在(1-4-3) FP8 bias=4的格式中，我们将最大的截断值从480降低到30，这已经很大了，足以覆盖我们研究的大量网络。这样，我们就可以表示更小的激活和权重值，最小可以到2^-11（比1-4-3中的截断阈值2^-7要小的多）。对表达简化，下面的所有(1-4-3) FP8试验都有默认的exponent bias 4。这些试验表明，5 bit exponent范围2^-15到2^16对于DNN推理来说范围过大了，4 bit exponent带有bias 4已经有足够的范围来表示激活和权重，可以进行训练和跨精度推理。最后，我们验证了这些结论，可以拓展到大量神经网络模型和拓扑。

### 2.2 Impact of FP8 Formats on Model Training Accuracy

In addition to increasing mismatch probability, we note that quantization noise also degrades the Lipschitz property of loss surfaces, that is, the loss changes in a faster rate, and the magnitudes of the gradients are larger too. In Fig.3, we plot (a) the loss surfaces of a FP32 trained model and (b) a (1-5-2) FP8 trained model along two random directions with their coefficients scanned along the x and y axis [26]. The loss surface of the (1-5-2) trained model shows multiple saddle points and appears rougher - making gradient descent based training unstable as evidenced by the kinks in Fig.3(b). The mitigation of such kinks has also explained the effectiveness of Batch Normalization [27]. In contrast, by increasing the number of mantissa bits from 2 to 3 for the forward pass only (while keeping gradients and errors in 1-5-2), the loss-surface appears to be significantly improved in Fig.3(c), implying easier optimization. On the other hand, comparing the loss surfaces for training and test, we can see that FP8 quantizations do not impact generalization.

量化噪声除了增加不匹配概率，还会使损害损失平面的Lipschitz性质，即，损失的变化速度会增加，梯度的幅度也会变得更大。在图3中，我们画出了(a)一个FP32训练模型的损失曲线，(b)一个(1-5-2)FP8训练模型，有两个随机方向的系数沿着x轴和y轴进行扫描。(1-5-2)训练模型的损失平面有多个鞍点，而且更粗糙，这会使基于梯度下降的训练更不稳定，如图3b中的kinks所示。这些kinks的缓解，也解释了BN的有效性。比较起来，只在前向过程中将mantissa bits从2增加到3（梯度和误差里保持在1-5-2），在图3c中，损失平面似乎得到了显著的改进，说明优化起来更简单。另一方面，比较训练和测试的损失平面，我们可以看到FP8不会影响泛化。

Guided by these insights, we propose our Hybrid FP8 (HFP8) formats utilizing two different FP8 formats to customize the precision separately for the forward and backward passes of DNN training - improving the performance on training and Trans-Precision inference. The underlying reason for this choice is that forward and backward passes have different optimal balances between range and precision. While tensors in the forward pass prefer higher precision (and lower representational error), gradients in the backward pass prefer a higher dynamic range. We describe our HFP8 training methodology where weights and activations adopt the (1-4-3) format (bias=4) while tensors used in backpropagation continue to be represented using the (1-5-2) format (in combination with loss scaling techniques pioneered by [28])(see Fig.1 in Appendix A). Our experiment shows that this simple change can significantly improve the performance of both MobileNet and Transformer models (as shown in Fig.4(a) and (b)), in comparison to forward (1-5-2) based results that showed significant degradation in Fig.1. In the following sections, we will show that this improvement is universally applicable to both training and Trans-Precision inference (training results for Speech and Object Detection models are shown in Fig.4(c) and (d)).

受这些洞见指引，我们提出了我们的混合FP8 (HFP8)格式，利用两种不同的FP8格式，来对DNN训练过程中的前向和反向过程定制其精度，改进训练和跨精度推理中的性能。这个选择潜在的原因是，前向和反向过程在表示范围和精度上有着不同的最优均衡。前向过程中的张量，倾向于需要更高的精度（表示误差要更低），反向过程中的梯度，更倾向于更高的动态范围。我们描述我们的HFP8训练方法，其中权重和激活采用的是(1-4-3)格式(bias=4)，而用在反向传播中的张量仍然使用(1-5-2)格式来表示（与[28]中的损失缩放技术组合使用）（见附录A中的图1）。我们的试验表明，这个简单的变化可以显著的改进MobileNet和Transformer模型的性能（如图4a和4b所示），而图1中则显示，基于(1-5-2)的前向过程有显著的降质。下面的小节中，我们展示了，这个改进对训练和跨精度推理是同时可用的（图4c和4d中展示了语音和目标检测模型的结果）。

For errors and gradients in HFP8 back-propagation, we employ the (1-5-2) FP8 format, which has proven to be optimal across various deep learning tasks. However, unlike activations and weights, even 5-bit exponents are insufficient to represent the wide dynamic range seen in activation gradients. Therefore, loss scaling has been adopted to enable gradients to become large enough to be representable using the (1-5-2) format [2]. Nonetheless, it’s infeasible to seek a unique scaling factor that fits a wide range of different models and datasets. Towards that end, we adapted auto-adjusted scale factors for gradients and errors during HFP8 training using Apex [28] (details in Appendix B). Finally, through hardware design experiments, we’ve confirmed that floating-point units (FPUs) that can support both formats are only 5% larger than the original FPUs that only support 1-5-2.

对于HFP8反向传播中的误差和梯度，我们采用(1-5-2) FP8格式，这被证明了在各种深度学习任务中都是最优的。但是，与激活和权重不同的是，即使是5-bit的exponent仍然不足以表示表示在激活梯度中的宽动态范围。因此，采用了损失缩放来使梯度变得足够大，以用(1-5-2)格式来可表示。尽管如此，仍然不能找到唯一的缩放因子适应广泛范围的模型和数据集。为此，我们在HFP8训练的过程中采用了梯度和误差的自适应的缩放因子，使用的是Apex[28]（详见附录B）。最后，通过硬件设计试验，我们确认了，支持两种格式的FPU，比只支持1-5-2格式的原始FPU只大了5%。

### 2.3 Last Layer Precision and the SoftMax Function

For networks with large output dimensions (typically seen in Speech and NLP), the last FC and SoftMax layers contribute to a significant fraction of the total computation time due to large matrix multiplications and expensive exponential functions (especially if these need to be computed in FP16). In these layers, it therefore becomes critical to be able to use 8-bit computations.

对于输出维度很大的网络（一般在语音和NLP任务中比较典型），最后的FC和SoftMax层占总计算时间相当多的部分，因为矩阵相乘很大，幂函数也很昂贵（尤其是需要在FP16的精度进行计算）。在这些层中，如果能够使用8-bit计算，这就变得很关键了。

First, we note that when (1-4-3) FP8 is used along with (1-6-9) FP16 output precision no degradation on LSTM-based SWB300 and Transformer-based translation tasks is observed. In contrast, when the output precision of the FC layer is set to (1-4-3) as well, large loss in accuracy is observed (network diverges in SWB300 and ∼ 1 BLEU degradation in WMT En-De). This occurs because the largest output of the last FC layer may be quantized into the same values (bins) during conversion from 16 to 8-bit and therefore become indistinguishable to the ensuing SoftMax layer. Fig.5(a) and (b) shows the distribution of output activations before and after FP8 (1-4-3) quantization in the transformer model for WMT En-De translation(dout = 42720), showing that the largest numbers are poorly represented by 8-bit in Fig.5(b). Interestingly, we discovered that if the quantization step is performed after the max subtraction step (i.e. x − xmax) in SoftMax, this degradation in accuracy can be fully eliminated. In Fig.5(c), the x − xmax sub-step of SoftMax moves the largest values closest to 0, where data representation is strongest due to non-uniform nature of floating point representation. Furthermore, this technique also allows SoftMax to be performed using just 8-bits. Detailed discussions on the reduced precision SoftMax will be a focus of future work. Overall, the (1-4-3) HFP8 format in the last FC layer when combined with an output precision of 1-6-9 and the max-subtracted SoftMax function allows for efficient end-to-end HFP8 computations.

第一，我们注意到，当(1-4-3) FP8与(1-6-9) FP16输出精度进行一起使用，在基于LSTM的SWB300和基于Transformer的翻译任务观察不到降质。比较起来，当FC层输出精度设为(1-4-3)时，会观察到准确率上的很大损失（在SWB300中网络发散了，在WMT En-De中BLEU下降了~1）。这是因为最后的FC层的最大输出在从16 bit转换到8 bit时，会量化到同样的值，因此对于后面的SoftMax层变得不可辨识。图5a和5b展示了Transformer模型在WMT En-De翻译任务中(dout=42720)，输出激活在FP8 (1-4-3)量化前后的分布变化，表明最大的数不能很好的由8-bit表示（图5b）。有趣的是，我们发现，如果量化步骤在SoftMax的减去最大的步骤（即，x-xmax）后进行，准确率的降低就可以完全消除。在图5c中，SoftMax中的x-xmax子步骤，将最大值移到了接近0，在这里数据表示是最强的，因为浮点表示的非均匀特性。而且，这种技术还允许SoftMax只使用8-bit进行。未来我们会聚焦在降低精度的SoftMax的详细讨论。总体上，在最后的FC层中，(1-4-3) HFP8格式，与1-6-9的输出精度相结合，与减去最大值的SoftMax结合到一起，就可以进行高效的端到端的HFP8计算。

## 3. Trans-Precision Inference in FP8

Guided by the theoretical framework in Section 2.1, we investigate how inference accuracies are impacted when FP32 trained models are used directly with different FP8 formats for inference (i.e. without any retraining). Using MobileNetV2 trained on ImageNet as an example, we immediately observe that the (1-4-3) FP8 format is significantly more accurate than the (1-5-2) format—as shown in the first 2 rows of the Table in Fig.6(a). This is consistent with mismatch probability based predictions described earlier. However, even with the right FP8 format, we observe that we lose >5% in model accuracy in comparison to FP32. To reduce this gap, we provide 2 additional insights. The first key insight comes from a theoretical understanding of how quantization errors in weights and activations directly impact the accuracy of outputs of the succeeding Batch Normalization (BN) layer. Retuning the statistics (mean and variance) of the BN layer for the precision of interest (i.e. inference precision) has the potential to significantly reduce this error. As shown in Eqn.2, the quantization error at the output of a BN layer (Z - ZQ) can be expressed in terms of the variance of quantization error in BN input σ_Q^2 and the variance of precise input σ_Y^2 — assuming Q and Y are not correlated (please see Appendix C for a detailed derivation):

有了2.1节的理论框架引导，我们研究了，当FP32训练模型直接与不同的FP8格式一起使用用作推理（即，没有任何重新训练），对推理精度的影响。使用在ImageNet上训练的MobileNetV2作为例子，我们在图6a中表的前2行中立刻观察到，(1-4-3)格式比(1-5-2)格式明显更准确。这与之前所说的基于误匹配的预测是一致的。但是，即使有正确的FP8格式，我们观察到，与FP32相比，仍然损失了高于5%的模型准确率。为降低这个差距，我们给出另外2个洞见。第一个关键的洞见来自于，权重和激活中的量化误差，怎样直接影响后续BN层的输出的准确率。重新调整BN层对于感兴趣精度（即，推理精度）的统计数值（均值和方差），可以显著降低这个误差。如式2所示，BN层的输出的量化误差Z-ZQ，可以表示为BN输入σ_Q^2的量化误差的方差，和精确输入σ_Y^2的方差，假设Q和Y是不相关的。

$$E[||Z-Z_Q||_2] ≈ γ^2 \frac {σ_Q^2} {σ_Y^2}, original BN statistics; 2γ^2(1-\frac{1}{1+\frac{σ_Q^2} {σ_Y^2}}), retuning BN statistics$$(2)

Plotting this equation in Fig.6(b), we observe that E[||Z − Z_q||_2] increases linearly with σ^2_Q when preserving original BN parameters, but increases only sub-linearly when BN statistics are re-tuned. This reveals the fact that the impact of quantization at BN output would be suppressed once BN statistics are properly tuned. As shown in Rows 5 and 6 of Fig.6(a), re-tuning BN statistics using just 2% of a single epoch of the training dataset reduces this accuracy gap significantly.

在图6b中画出这个等式，我们观察到，当保持原始的BN参数时，E[||Z − Z_q||_2]随着σ^2_Q是线性增加的，而当BN统计数值重新调整过后，则是亚线性增加的。这说明了，BN统计数值得到合理的调整后，BN输出的量化影响会得到抑制。如图6a中表的第5和6行所示，只使用训练数据集一个epoch的2%的数据来重新调整BN统计数值，就可以将准确率的差距显著降低。

The second key insight obtained using Eqn.2 indicates that layers that have very low σ^2_Y have vastly magnified output errors. Plotting σ^2_Y as a function of layer number for MobileNetV2 (Fig.6(c)) leads us to note that depthwise (DW) convolution layers produce activations that have orders of magnitude smaller variance (σ^2) in comparison to traditional convolution layers [14]. We therefore expect the precision setting in these layers to strongly impact Trans-Precision inference accuracies. Since DW layers contribute to <3% of the overall compute in the MobileNet family of networks [17], we recommend setting the precision in these layers uniformly to FP16. As can be seen from Rows 3,4 (without BN re-tuning) and Rows 7,8 (with BN re-tuning), this precision setting in the DW layers substantially improves inference accuracies to within ∼ 1% of the baseline.

式2得到的第2个关键洞见是，σ^2_Y值很小的层，其输出误差被显著放大。将MobileNetV2中的σ^2_Y作为层数的函数画出来（图6c），我们发现DW卷积层产生的方差σ^2，其幅度与传统卷积层相比，要小几个数量级。我们因此期望，这些层中的精度设置，会强烈影响跨精度推理的准确率。由于DW层的计算量在MobileNet网络族中的整体占比小于3%，我们推荐在这些层中将精度统一设置成FP16。在第3，4行（没有进行BN重新调节）和第7，8行（有BN重新调节），DW层的这个精度设置极大的改善了推理准确率，与基准的差距在~1%以内。

A combination of these techniques – (a) picking the right precision format (1-4-3) for weights and activations of convolution and FC layers (b) setting the precision for DW layers to FP16 and (c) updating BN µ and σ^2 with minimal training data – allows MobileNetV2 to hit accuracies within ∼ 0.5% of the full precision baseline. Furthermore, we show that these techniques extend very well to other models; as shown in Table 1, FP8 1-4-3 with BN re-tuning can fully recover the baseline inference accuracies for the entire spectrum of networks studied. Note that for BN re-tuning, data does not need to be labeled and thus can be done at the edge devices.

下面这些技术组合起来：(a)对卷积层和FC层的权重和激活选择正确的精度格式(1-4-3)，(b)设置DW层的精度为FP16，(c)用最少的训练数据来更新BN的µ和σ^2，这会使MobileNetV2的准确率达到完整精确度基准的~0.5%以内。而且，我们证明了，这些技术在其他模型上的表现也很好；如表1所示，对所有研究的网络，FP8 1-4-3与BN重新调整，可以完全恢复基准推理准确率。注意，对于BN重新调节，数据不需要标注，因此可以在边缘设备上进行。

## 4. Hybrid FP8 Distributed Training Scheme and Results

As DNN compute functions are accelerated in each learner using HFP8, the communication cost among learners and memory bandwidth dominated tasks like weight updates become bottlenecks.

随着DNN计算函数在每个learner中使用HFP8被加速，在learner之间的通信代价和内存带宽占据了任务的主要部分，如权重更新，这成为了瓶颈。

Hardware performance estimations indicate that this communication could take up to ∼ 41 − 62% of the end-to-end training time for ResNet50 with HFP8 GEMM (Appendix D for details). As illustrated in Table 2, the conventional communication pattern used in deep learning algorithms exchanges gradients through ring-based all-reduce [29, 30] and then each learner updates the whole model locally. To take advantage of 8-bit weights in off-chip communication as well as to minimize local memory bandwidth, we modify the existing distributed learning scheme slightly—so that each of N learners updates only 1/Nth of the model after the reduce-scatter phase minimizing local memory transactions. When updating the model globally, the final 8-bit weights produced in each learner are distributed in the all-gather phase, thereby improving off-chip communication by 2× compared to conventional 16-bit gradient communication.

硬件性能估计表明，在用HFP8 GEMM的ResNet50的端到端的训练中，这些通信会占~41%-62%的时间。如表2所示，深度学习算法中传统的通信模式中，是通过基于ring的all-reduce来交换梯度，然后每个learner在本地更新整个模型。为在片外通信中利用8-bit权重，并且最小化局部存储带宽，我们略微修改了现有的分布式学习机制，这样N learner中的每一个，在reduce-scatter阶段后，只更新1/N的模型，最小化了局部存储事务。当全局的更新模型时，每个learner产生的最终的8-bit权重在all-gather阶段分布，与传统的16-bit梯度通信相比，改进了片外通信2x。

To improve the robustness of low-precision weight updates and to prevent "swamping" [2], we propose a deterministic round-off residual update scheme that stores the weight in 8-bit while saving the quantization errors locally as "round-off" residuals in FP16 as illustrated in Table 3. We study this round-off residual scheme on a wide range of DNN applications and show that it does not impact convergence (consistent with the rich body of theoretical work in this space [31, 32]). With 8-bit weight updates and a modified ring-distribution scheme, our technique improves end-to-end training time by 32 − 38% on ResNet50 (for details, see Appendix D).

为改进低精度权重更新的稳健性，防止swamping，我们提出了一种确定性的round-off残差更新方案，权重以8-bit进行存储，量化误差局部存储为round-off残差，格式为FP16，如表3所示。我们在大量DNN应用中研究了这种round-off残差方案，显示这并不影响收敛（与本领域中的大量理论工作一致）。有了8-bit权重更新，和修正的ring分布方案，我们的技术将ResNet50端到端训练时间提升了32-38%。

Finally, to demonstrate the wide applicability and the robustness of the HFP8 formats, 8-bit computations and round-off residual scheme, we tested it on a wide spectrum of deep learning models and datasets without changes to network architectures, data pre-processing, or hyper-parameters(details in Appendix E). As shown in Table 4, every single network tested achieved accuracy very close to the full precision baseline, including tasks that were problematic for previous FP8 endeavors (such as MobileNet and Transformers). More complex and challenging tasks, such as Object Detection, Speech and Machine Translation in HFP8 are demonstrated and for the first time show performance within 0.5% of the full precision baseline on large networks and datasets. Given the limited computational complexity in the first and last layers we set the precision in these layers to FP16 except for Speech and Transformer networks, where we use the same HFP8 settings on the large final FC layer and find no degradation.

最后，为表明HFP8格式，8-bit计算和round-off残差方案的广泛可应用性和稳健性，我们在很多深度学习模型和数据集上进行了测试，没有改变网络架构，数据预处理，或超参数。如表4所示，测试的每个网络，都得到了与完整精度基准很接近的精度，包括那些之前的FP8有问题的任务（比如MobileNet和Transformer）。更复杂和有挑战的任务，比如目标检测，语音和机器翻译也进行了试验和展示，在大型网络和数据集上，得到了与完整精度基准误差在0.5%的性能。由于前几层和最后几层中有限的计算复杂度，我们将这些层中设置精度为FP16，除了语音和Transformer网络，在这两个任务中，我们在大型最终FC层中使用HFP8，没有发现降质情况。

## 5. Conclusions

We have demonstrated DNN training with a new Hybrid FP8 format that adopts two different FP8 formats for forward and backward propagation. In addition, we introduced a novel round-off residual scheme which can significantly improve robustness of low-precision AXPY and reduce communication bandwidth requirements. We’ve confirmed the superior accuracy of this approach over previous 8-bit training proposals on a wide range of state of the art DNN models. In addition, we’ve presented new insights in Batch Normalization and depthwise Convolutional layers that demonstrate how the same FP8 format can be used for highly accurate Trans-Precision inference (starting from higher precision FP32 models). These novel techniques enable a new generation of 8-bit hardware systems that are robust for the entire spectrum of DNN training and inference applications.

我们证明了用新HFP8进行DNN训练，对前向传播和反向传播采用了两种不同的FP8格式。此外，我们提出了一种新的round-off残差方案，可以显著改进低精度AXPY的稳健性，降低通信带宽的需求。我们在很多目前最好的DNN模型中，确认了这种方法比之前的8-bit训练方案准确率更高。此外，我们提出了BN和DW卷积层的新洞见，证明了相同的FP8格式可以用于高度精确的跨精度推理（从高精度FP32模型开始）。这些新技术开启了新一代8-bit硬件系统，在很多DNN训练和推理应用中都很稳健。