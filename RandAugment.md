# RandAugment: Practical automated data augmentation with a reduced search space

Ekin D. Cubuk et. al. Google Research, Brain Team

## 0. Abstract

Recent work has shown that data augmentation has the potential to significantly improve the generalization of deep learning models. Recently, automated augmentation strategies have led to state-of-the-art results in image classification and object detection. While these strategies were optimized for improving validation accuracy, they also led to state-of-the-art results in semi-supervised learning and improved robustness to common corruptions of images. An obstacle to a large-scale adoption of these methods is a separate search phase which increases the training complexity and may substantially increase the computational cost. Additionally, due to the separate search phase, these approaches are unable to adjust the regularization strength based on model or dataset size. Automated augmentation policies are often found by training small models on small datasets and subsequently applied to train larger models. In this work, we remove both of these obstacles. RandAugment has a significantly reduced search space which allows it to be trained on the target task with no need for a separate proxy task. Furthermore, due to the parameterization, the regularization strength may be tailored to different model and dataset sizes. RandAugment can be used uniformly across different tasks and datasets and works out of the box, matching or surpassing all previous automated augmentation approaches on CIFAR-10/100, SVHN, and ImageNet. On the ImageNet dataset we achieve 85.0% accuracy, a 0.6% increase over the previous state-of-the-art and 1.0% increase over baseline augmentation. On object detection, RandAugment leads to 1.0-1.3% improvement over baseline augmentation, and is within 0.3% mAP of AutoAugment on COCO. Finally, due to its interpretable hyperparameter, RandAugment may be used to investigate the role of data augmentation with varying model and dataset size. Code is available online.

最近的工作表明，数据扩增有潜力显著改进深度学习模型的泛化性能。最近，自动扩增策略在图像分类和目标检测中得到了目前最好的结果。虽然这些策略经过优化改进了验证准确率，但也在半监督学习中得到了目前最好的结果，改进了图像的常见降质的稳健性。大规模采用这些方法的一个障碍是，需要一个单独的搜索阶段，增加了训练复杂度，可能显著增加计算代价。另外，由于单独的搜索阶段，这些方法不能基于模型或数据集规模调整正则化的力度。自动扩增策略通常通过在小型数据集上训练小模型得到，然后用于训练更大的模型。本文中，我们去除了这些障碍。RandAugment的搜索空间减少了很多，可以在目标任务中直接训练，不需要单独的代理任务。而且，由于参数化，可以对不同的模型和数据集大小，定制正则化力度。RandAugment可以在不同任务和数据集中使用，开箱即用，在CIFAR-10/100，SVHN和ImageNet上超过了所有之前的自动扩增方法。在ImageNet数据集上，我们得到了85.0%的准确率，比之前最好的结果改进了0.6%，比基准扩增改进了1.0%。在目标检测中，RandAugment带来了1.0%-1.3%的改进，在COCO上比AutoAugment改进了0.3%。最后，由于其超参数可解释，RandAugment可能用于研究数据扩增在不同模型和数据集大小上的角色。代码已开源。

## 1. Introduction

Data augmentation is a widely used method for generating additional data to improve machine learning systems, for image classification [43, 23, 7, 54], object detection [13], instance segmentation [10], and speech recognition [21, 16, 36]. Unfortunately, data augmentation methods require expertise, and manual work to design policies that capture prior knowledge in each domain. This requirement makes it difficult to extend existing data augmentation methods to other applications and domains.

数据扩增是一种广泛使用的方法，用于生成额外的数据，来改进机器学习系统，如图像分类，目标检测，实例分割和语音识别。不幸的是，数据扩增方法需要专业知识，和手工工作来设计策略，以捕获每个领域的先验知识。这种需求使得将现有的数据扩增方法拓展到其他领域和应用比较困难。

Learning policies for data augmentation has recently emerged as a method to automate the design of augmentation strategies and therefore has the potential to address some weaknesses of traditional data augmentation methods [5, 57, 20, 25]. Training a machine learning model with a learned data augmentation policy may significantly improve accuracy [5], model robustness [32, 52, 41], and performance on semi-supervised learning [50] for image classification; likewise, for object detection tasks on COCO and PASCAL-VOC [57]. Notably, unlike engineering better network architectures [59], all of these improvements in predictive performance incur no additional computational cost at inference time.

学习数据扩增的策略最近成为一种自动设计扩增策略的方法，因此可以处理传统数据扩增方法的一些缺陷。用学习得到的数据扩增策略来训练一个机器学习模型，可能会显著改进准确率，模型稳健性，和在半监督学习上的图像分类性能；类似的，对COCO和PASCAL-VOC上的目标检测任务也是这样。值得注意的是，与设计更好的网络架构不同，所有这些预测性能的改进，在推理时间上没有带来额外的计算代价。

In spite of the benefits of learned data augmentation policies, the computational requirements as well as the added complexity of two separate optimization procedures can be prohibitive. The original presentation of neural architecture search (NAS) realized an analogous scenario in which the dual optimization procedure resulted in superior predictive performance, but the original implementation proved prohibitive in terms of complexity and computational demand. Subsequent work accelerated training efficiency and the efficacy of the procedure [30, 38, 28, 29], eventually making the method amenable to a unified optimization based on a differentiable process [30]. In the case of learned augmentations, subsequent work identified more efficient search methods [20, 25], however such methods still require a separate optimization procedure, which significantly increases the computational cost and complexity of training a machine learning model.

即使学习到的数据扩增策略有很多好处，计算需求以及两个单独的优化过程增加的复杂度是非常大的。NAS的提出实现了一个类似的场景，其中对偶优化过程得到了非常好的预测性能，但原始实现在复杂度和计算需求上都很大。后续的工作加速了训练效率和过程的效能，最后使得这个方法可以基于可微分的过程进行统一优化。在学习到的扩增上，后续的工作得到了更多高效的搜索方法，但是这样的方法仍然需要一个单独的优化过程，这显著增加了计算代价和训练一个机器学习模型的复杂度。

The original formulation for automated data augmentation postulated a separate search on a small, proxy task whose results may be transferred to a larger target task [59, 58]. This formulation makes a strong assumption that the proxy task provides a predictive indication of the larger task [28, 2]. In the case of learned data augmentation, we provide experimental evidence to challenge this core assumption. In particular, we demonstrate that this strategy is sub-optimal as the strength of the augmentation depends strongly on model and dataset size. These results suggest that an improved data augmentation may be possible if one could remove the separate search phase on a proxy task.

自动数据扩增的原始方法要在一个小的代理任务上进行单独的搜索，其结果可能会迁移到一个更大的目标任务。这种形式有一个很强的假设，即代理任务是更大任务的预测性指示。在学习到的数据扩增的情况下，我们给出试验证据来挑战这个核心假设。特别是，我们证明了这个策略并不是最优的，因为扩增的强度强烈依赖于模型和数据集的大小。这个结果说明，如果去掉在代理任务上的单独搜索过程，也可能得到改进的数据扩增策略。

In this work, we propose a practical method for automated data augmentation – termed RandAugment – that does not require a separate search. In order to remove a separate search, we find it necessary to dramatically reduce the search space for data augmentation. The reduction in parameter space is in fact so dramatic that simple grid search is sufficient to find a data augmentation policy that outperforms all learned augmentation methods that employ a separate search phase. Our contributions can be summarized as follows:

本文中，我们提出一个实际的方法进行自动数据扩增，称为RandAugment，不需要单独的搜索。为去掉单独的搜索，我们发现需要极大的降低数据扩增的搜索空间。参数空间的缩减实际上非常大，这样简单的网格搜索足以找到一个数据扩增策略，超过所有采用了单独搜索过程学到的扩增方法。我们的贡献可以总结如下：

- We demonstrate that the optimal strength of a data augmentation depends on the model size and training set size. This observation indicates that a separate optimization of an augmentation policy on a smaller proxy task may be sub-optimal for learning and transferring augmentation policies. 我们证明了，数据扩增的最优强度，依赖于模型和训练集的大小。这种观察说明，在更小的代理任务上的扩增策略的单独优化，对于学习和迁移扩增策略可能不是最优的。

- We introduce a vastly simplified search space for data augmentation containing 2 interpretable hyperparameters. One may employ simple grid search to tailor the augmentation policy to a model and dataset, removing the need for a separate search process. 我们提出了一个极大简化的搜索空间进行数据扩增，包含两个可解释的超参数。可以采用简单的网格搜索来对一个模型和数据集定制扩增策略，去掉了单独的搜索过程的需要。

- Leveraging this formulation, we demonstrate state-of-the-art results on CIFAR [22], SVHN [34], and ImageNet [6]. On object detection [27], our method is within 0.3% mAP of state-of-the-art. On ImageNet we achieve a state-of-the-art accuracy of 85.0%, a 0.6% increment over previous methods and 1.0% over baseline augmentation. 利用这种形式，我们在CIFAR，SVHN和ImageNet上得到了目前最优的结果。在目标检测中，我们的方法与目前最好结果相差0.3%。在ImageNet上，我们得到了85.0%的目前最好准确率，比之前的方法提高了0.6%，比基准扩增方法提高了1.0%。

## 2. Related Work

Data augmentation has played a central role in the training of deep vision models. On natural images, horizontal flips and random cropping or translations of the images are commonly used in classification and detection models [53, 23, 13]. On MNIST, elastic distortions across scale, position, and orientation have been applied to achieve impressive results [43, 4, 49, 42]. While previous examples augment the data while keeping it in the training set distribution, operations that do the opposite can also be effective in increasing generalization. Some methods randomly erase or add noise to patches of images for increased validation accuracy [8, 55], robustness [46, 52, 11], or both [32]. Mixup [54] is a particularly effective augmentation method on CIFAR-10 and ImageNet, where the neural network is trained on convex combinations of images and their corresponding labels. Object-centric cropping is commonly used for object detection tasks [31], whereas [9] adds new objects on training images by cut-and-paste.

数据扩增在训练深度学习模型中角色非常关键。在自然图像中，水平翻转和随机剪切或平移，在分类和检测模型中经常使用。在MNIST中，不同尺度、位置和方向上的弹性形变的应用，得到了非常好的结果。之前的例子扩增数据的同时，将其保持在训练集的分布中，相反的动作对增加泛化也会有用。一些方法对图像块随机增加或去除噪声，以增加验证准确率，稳健性，或两者都有。Mixup是一种在CIFAR-10和ImageNet上特别有效的扩增方法，其中神经网络在图像及其对应的标签的凸组合上进行训练。以目标为中心的剪切，在目标检测任务中也经常使用，在训练图像上通过剪切粘贴的方式增加新目标。

Moving away from individual operations to augment data, other work has focused on finding optimal strategies for combining different operations. For example, Smart Augmentation learns a network that merges two or more samples from the same class to generate new data [24]. Tran et al. generate augmented data via a Bayesian approach, based on the distribution learned from the training set [48]. DeVries et al. use transformations (e.g. noise, interpolations and extrapolations) in the learned feature space to augment data [7]. Furthermore, generative adversarial networks (GAN) have been used to choose optimal sequences of data augmentation operations[39]. GANs have also been used to generate training data directly [37, 33, 56, 1, 44], however this approach does not seem to be as beneficial as learning sequences of data augmentation operations that are pre-defined [40].

除了用单个运算来扩增数据，其他工作从组合不同的运算上发现最优策略。比如，Smart Augmentation学习了一个网络，将同一类别的两个或更多样本合并，以生成新的数据。Tran等通过一种贝叶斯方法生成扩增数据，基于从训练集上学习得到的分布。DeVries等使用一些在学习到的特征空间中的变换（如，噪声，内插和外插）来扩增数据。另外，GAN也用于选择数据扩增运算的最优序列。GANs也用于直接生成训练数据，但这种方法与学习数据扩增运算序列的方法比，并没有那么好。

Another approach to learning data augmentation strategies from data is AutoAugment [5], which originally used reinforcement learning to choose a sequence of operations as well as their probability of application and magnitude. Application of AutoAugment policies involves stochasticity at multiple levels: 1) for every image in every minibatch, a sub-policy is chosen with uniform probability. 2) operations in each sub-policy has an associated probability of application. 3) Some operations have stochasticity over direction. For example, an image can be rotated clockwise or counter-clockwise. The layers of stochasticity increase the amount of diversity that the network is trained on, which in turn was found to significantly improve generalization on many datasets. More recently, several papers used the AutoAugment search space and formalism with improved optimization algorithms to find AutoAugment policies more efficiently [20, 25]. Although the time it takes to search for policies has been reduced significantly, having to implement these methods in a separate search phase reduces the applicability of AutoAugment. For this reason, this work aims to eliminate the search phase on a separate proxy task completely.

另一种学习数据扩增策略的方法是AutoAugment，使用强化学习来选择一个运算序列以及其应用概率和强度。AutoAugment策略的应用包括多个层次上的随机性：1)对每个minibatch中的每幅图像，用均匀概率选择一个子策略；2)在每个子策略中的运算的应用概率是相关联的；3)一些运算在方向上有随机性。比如，一幅图像可以顺时针或逆时针旋转。带有随机性的层，增加了网络训练的多样性，这在很多数据集上显著改进了泛化性。最近，几篇文章使用AutoAugment搜索空间和改进优化算法的公式，以更加高效的发现AutoAugment策略。虽然搜索策略的时间已经显著降低，但仍然还是要在一个单独的搜索过程中实现这些方法，这降低了AutoAugment的应用性。基于这个原因，本文的目标是完全去掉在单独的代理任务中进行搜索的需要。

Some of the developments in RandAugment were inspired by the recent improvements to searching over data augmentation policies. For example, Population Based Augmentation (PBA) [20] found that the optimal magnitude of augmentations increased during the course of training, which inspired us to not search over optimal magnitudes for each transformation but have a fixed magnitude schedule, which we discuss in detail in Section 3. Furthermore, authors of Fast AutoAugment [25] found that a data augmentation policy that is trained for density matching leads to improved generalization accuracy, which inspired our first order differentiable term for improving augmentation (see Section 4.7).

在RandAugment中的一些思想，是受到了最近在改进搜索数据扩增策略的启发。比如，PBA发现，随着训练过程的进行，扩增的最佳幅度逐渐增加，这启发了我们不要搜索每个变换的最佳幅度，而是要有一个固定的幅度方案，我们在第3部分详述。另外，Fast AutoAugment的作者发现，为密集匹配训练得到的数据扩增策略，可以带来改进的泛化准确率，这启发了我们改进扩增的一阶可微分项。

## 3. Methods

The primary goal of RandAugment is to remove the need for a separate search phase on a proxy task. The reason we wish to remove the search phase is because a separate search phase significantly complicates training and is computationally expensive. More importantly, the proxy task may provide sub-optimal results (see Section 4.1). In order to remove a separate search phase, we aspire to fold the parameters for the data augmentation strategy into the hyper-parameters for training a model. Given that previous learned augmentation methods contained 30+ parameters [5, 25, 20], we focus on vastly reducing the parameter space for data augmentation.

RandAugment的最初目标是去掉在代理任务上的单独搜索阶段。我们希望去掉这个搜索阶段的原因，是因为这个单独的搜索阶段使训练变得非常复杂，计算量非常大。更重要的是，代理任务不一定会得到最优结果（见4.1节）。为去掉单独的搜索阶段，我们希望将数据扩增策略的参数放入训练一个模型的超参数中。鉴于之前学习的扩增方法包括30+个参数，我们聚焦在为数据扩增极大降低参数空间。

Previous work indicates that the main benefit of learned augmentation policies arise from increasing the diversity of examples [5, 20, 25]. Indeed, previous work enumerated a policy in terms of choosing which transformations to apply out of K=14 available transformations, and probabilities for applying each transformation:

之前的工作表明，学习的扩增策略的主要好处是来自于，增加了样本的多样性。确实，之前的工作罗列了从K=14中可用变换中选择哪些变换来应用的策略，以及使用这些变换的概率：

• identity • autoContrast • equalize • rotate • solarize • color • posterize
• contrast • brightness • sharpness • shear-x • shear-y • translate-x • translate-y

In order to reduce the parameter space but still maintain image diversity, we replace the learned policies and probabilities for applying each transformation with a parameter-free procedure of always selecting a transformation with uniform probability 1/K. Given N transformations for a training image, RandAugment may thus express K^N potential policies.

为降低参数空间，但仍然保持图像多样性，我们将应用每个变换的学习到的策略和概率，替换成一种无参数的过程，永远以均匀概率1/K选择一个变换。给定一个训练图像的N个变换，RandAugment因此可以表达K^N个潜在的策略。

The final set of parameters to consider is the magnitude of the each augmentation distortion. Following [5], we employ the same linear scale for indicating the strength of each transformation. Briefly, each transformation resides on an integer scale from 0 to 10 where a value of 10 indicates the maximum scale for a given transformation. A data augmentation policy consists of identifying an integer for each augmentation [5, 25, 20]. In order to reduce the parameter space further, we observe that the learned magnitude for each transformation follows a similar schedule during training (e.g. Figure 4 in [20]) and postulate that a single global distortion M may suffice for parameterizing all transformations. We experimented with four methods for the schedule of M during training: constant magnitude, random magnitude, a linearly increasing magnitude, and a random magnitude with increasing upper bound. The details of this experiment can be found in Appendix A.1.1.

最终的参数集是每个扩增变形的幅度。按照[5]，我们采用相同的线性尺度来指示每个变换的强度。简要来说，每个变换都有一个整数尺度0到10，其中10表示给定变换的最大尺度。一个数据扩增策略由每个变换的一个整数构成。为进一步降低参数空间的大小，我们观察到，对每个变换学习到的幅度，在训练时都遵循类似的方案（如[20]中的图4），于是假设，单个全局形变M就足以对所有变换进行参数化。我们对训练过程中M的方案，试验了四种方法：常数幅度，随机幅度，线性增加的幅度和一个增加上限的随机幅度。这个试验的细节如附录A.1.1所示。

The resulting algorithm contains two parameters N and M and may be expressed simply in two lines of Python code (Figure 2). Both parameters are human-interpretable such that larger values of N and M increase regularization strength. Standard methods may be employed to efficiently perform hyperparameter optimization [45, 14], however given the extremely small search space we find that naive grid search is quite effective (Section 4.1). We justify all of the choices of this proposed algorithm in this subsequent sections by comparing the efficacy of the learned augmentations to all previous learned data augmentation methods.

得到的算法包含两个参数N和M，可以简单的用两行Python代码表示（图2）。两个参数都是可解释的，即N和M值越大，正则化强度越大。可以采用标准方法来高效的进行超参数优化，但鉴于搜索空间很小，我们发现简单的网格搜索就很不错（见4.1节）。我们在后续章节中详述提出的算法的各种选项，将学习到的扩增与之前学习的到的数据扩增方法进行比较。

## 4. Results

To explore the space of data augmentations, we experiment with core image classification and object detection tasks. In particular, we focus on CIFAR-10, CIFAR-100, SVHN, and ImageNet datasets as well as COCO object detection so that we may compare with previous work. For all of these datasets, we replicate the corresponding architectures and set of data transformations. Our goal is to demonstrate the relative benefits of employing this method over previous learned augmentation methods.

为探索数据扩增的空间，我们用核心的图像分类和目标检测任务进行试验。特别是，我们聚焦在CIFAR-10，CIFAR-100，SVHN和ImageNet，COCO目标检测数据集上，这样我们可以与之前的工作进行比较。对所有这些数据集，我们复制对应的架构和数据变换集合。我们的目标是证明，采用这种方法比之前的学习得到的扩增方法要相对较好。

### 4.1. Systematic failures of a separate proxy task

A central premise of learned data augmentation is to construct a small, proxy task that may be reflective of a larger task [58, 59, 5]. Although this assumption is sufficient for identifying learned augmentation policies to improve performance [5, 57, 36, 25, 20], it is unclear if this assumption is overly stringent and may lead to sub-optimal data augmentation policies.

学习数据扩增的一个中心假设是，构建一个小的代理任务，可以反应一个更大的任务。虽然这个假设足以识别学习到的扩增策略可以改进性能，但这个假设是否过于严格，则不太明确，这可能导致数据扩增策略并不是最优的。

In this first section, we challenge the hypothesis that formulating the problem in terms of a small proxy task is appropriate for learned data augmentation. In particular, we explore this question along two separate dimensions that are commonly restricted to achieve a small proxy task: model size and dataset size. To explore this hypothesis, we systematically measure the effects of data augmentation policies on CIFAR-10. First, we train a family of Wide-ResNet architectures [53], where the model size may be systematically altered through the widening parameter governing the number of convolutional filters. For each of these networks, we train the model on CIFAR-10 and measure the final accuracy compared to a baseline model trained with default data augmentations (i.e. flip left-right and random translations). The Wide-ResNet models are trained with the additional K=14 data augmentations (see Methods) over a range of global distortion magnitudes M parameterized on a uniform linear scale ranging from [0, 30].

第1节中，我们对这个假设进行挑战，即在小的代理任务上学习对学习数据扩增是合适的。特别是，我们沿着两个单独的维度来探索这个问题，一般局限在一个小的代理任务上进行：模型大小和数据集大小。为探索这个假设，我们系统的衡量了数据扩增策略在CIFAR-10上的效果。首先，我们训练了一族Wide-ResNet架构，其中模型大小通过加宽参数来变化，这个参数决定了卷积滤波器的数量。对于每个网络，我们在CIFAR-10上训练这个模型，衡量最终的准确率，与用默认数据扩增（即，左右翻转和随机平移）训练的基准模型进行比较。Wide-ResNet模型用K=14个数据扩增进行训练，全局形变幅度M变化很多，取值范围为[0, 30]中的均匀线性尺度。

Figure 3a demonstrates the relative gain in accuracy of a model trained across increasing distortion magnitudes for three Wide-ResNet models. The squares indicate the distortion magnitude with which achieves the highest accuracy. Note that in spite of the measurement noise, Figure 3a demonstrates systematic trends across distortion magnitudes. In particular, plotting all Wide-ResNet architectures versus the optimal distortion magnitude highlights a clear monotonic trend across increasing network sizes (Figure 3b). Namely, larger networks demand larger data distortions for regularization. Figure 1 highlights the visual difference in the optimal distortion magnitude for differently sized models. Conversely, a learned policy based on [5] provides a fixed distortion magnitude (Figure 3b, dashed line) for all architectures that is clearly sub-optimal.

图3a展示了三个Wide-ResNet模型，在逐渐增加的形变幅度上训练，其准确率的相对增加。方形表示了获得最高准确率的形变幅度。注意，尽管有测量的噪声，图3a展示了，不同形变幅度的系统性趋势。特别是，画出了所有Wide-ResNet架构与最优形变幅度的关系图，强调了增加了网络大小，有单调的形变幅度趋势（图3b）。即，更大的网络需要更大的数据形变来进行正则化。图1强调了对不同大小的模型，最优形变幅度的视觉差异。相反的，基于[5]的学习到的策略则对所有的架构给出了固定的形变幅度（图3b的虚线），这显然不是最优的。

A second dimension for constructing a small proxy task is to train the proxy on a small subset of the training data. Figure 3c demonstrates the relative gain in accuracy of Wide-ResNet-28-10 trained across increasing distortion magnitudes for varying amounts of CIFAR-10 training data. The squares indicate the distortion magnitude with that achieves the highest accuracy. Note that in spite of the measurement noise, Figure 3c demonstrates systematic trends across distortion magnitudes. We first observe that models trained on smaller training sets may gain more improvement from data augmentation (e.g. 3.0% versus 1.5% in Figure 3c). Furthermore, we see that the optimal distortion magnitude is larger for models that are trained on larger datasets. At first glance, this may disagree with the expectation that smaller datasets require stronger regularization.

构建一个小型代理任务的第二个维度是，在训练数据的一个小型子集上训练代理。图3c展示了Wide-ResNet-28-10在不同的CIFAR-10训练数据量上逐渐增加形变幅度得到的相对准确率变化。方形指示了得到最高准确率的形变幅度。注意，尽管测量有噪声，图3c展示了不同形变幅度有着系统性的趋势。我们首先观察到，在更小的训练集上训练得到的模型，会从数据扩增中得到更多的提升（如，图3c中的3.0% vs 1.5%）。而且，我们看到了，最优形变幅度对于更大的数据集也是更大的。第一眼看上去，这与期望的更小的数据集需要更强的正则化不太一致。

Figure 3d demonstrates that the optimal distortion magnitude increases monotonically with training set size. One hypothesis for this counter-intuitive behavior is that aggressive data augmentation leads to a low signal-to-noise ratio in small datasets. Regardless, this trend highlights the need for increasing the strength of data augmentation on larger datasets and the shortcomings of optimizing learned augmentation policies on a proxy task comprised of a subset of the training data. Namely, the learned augmentation may learn an augmentation strength more tailored to the proxy task instead of the larger task of interest.

图3d展示了，最优形变幅度随着训练集大小的增加逐渐递增。对这种反直觉行为的一个假设是，太激进的数据扩增在小型数据集中会带来更低的SNR。尽管如此，这种趋势强调了，在更大型数据集上加强数据扩增的强度的需要，和在小型代理任务上优化学习到的扩增策略的缺点。即，学习得到的扩增可能学习到为代理任务定制的扩增强度，而不是为感兴趣的大型任务。

The dependence of augmentation strength on the dataset and model size indicate that a small proxy task may provide a sub-optimal indicator of performance on a larger task. This empirical result suggests that a distinct strategy may be necessary for finding an optimal data augmentation policy. In particular, we propose in this work to focus on a unified optimization of the model weights and data augmentation policy. Figure 3 suggest that merely searching for a shared distortion magnitude M across all transformations may provide sufficient gains that exceed learned optimization methods [5]. Additionally, we see that optimizing individual magnitudes further leads to minor improvement in performance (see Section A.1.2 in Appendix).

扩增强度对数据集和模型大小的依赖，说明了小型代理任务并不是大型任务性能的最佳指示。这种经验结果说明，需要不同的策略来寻找最佳数据扩增策略。特别是，我们提出聚焦在模型权重和数据扩增策略的统一优化。图3说明，只搜索所有形变的共享的形变幅度M，足以得到超过学习的优化方法的收益。另外，我们看到了，优化单个幅度，还会带来性能的小幅度提升。

Furthermore, Figure 3a and 3c indicate that merely sampling a few distortion magnitudes is sufficient to achieve good results. Coupled with a second free parameter N, we consider these results to prescribe an algorithm for learning an augmentation policy. In the subsequent sections, we identify two free parameters N and M specifying RandAugment through a minimal grid search and compare these results against computationally-heavy learned data augmentations based on proxy tasks.

而且，图3a和3c说明，对形变幅度进行几个采样，就足以得到很好的结果。与另一个自由参数N一起，我们认为这些结果就可以使一个算法学习一个扩增策略。在后续的章节中，我们使两个自由参数N和M通过很小的网格搜索来得到RandAugment，并将这些结果与计算量很大的学习的数据扩增方法进行比较。

### 4.2. CIFAR

CIFAR-10 has been extensively studied with previous data augmentation methods and we first test this proposed method on this data. The default augmentations for all methods include flips, pad-and-crop and Cutout [8]. N and M were selected based on the validation performance on 5K held out examples from the training set for 1 and 5 settings for N and M, respectively. Results indicate that RandAugment achieves either competitive (i.e. within 0.1%) or state-of-the-art on CIFAR-10 across four network architectures (Table 2). As a more challenging task, we additionally compare the efficacy of RandAugment on CIFAR-100 for Wide-ResNet-28-2 and Wide-ResNet-28-10. On the held out 5K dataset, we sampled 2 and 4 settings for N and M, respectively (i.e. N={1, 2} and M={2, 6, 10, 14}). For Wide-ResNet-28-2 and Wide-ResNet-28-10, we find that N=1, M=2 and N=2, M=14 achieves best results, respectively. Again, RandAugment achieves competitive or superior results across both architectures (Table 2).

CIFAR-10在之前的数据扩增方法中已经进行了广泛的研究，我们首先在这个数据集上测试提出的方法。对所有方法，默认的扩增包括翻转，pad-and-crop和Cutout。N和M是基于在训练集中5K保留的样本上的验证性能选择的。结果表明，RandAugment在CIFAR-10上对四个网络架构，都获得了非常好或最好的结果（表2）。作为一个更有挑战性的任务，我们额外比较了RandAugment在CIFAR-100上对Wide-ResNet-28-2和Wide-ResNet-28-10的结果。在保留的5K数据集上，我们对N和M分别采样了2和4个设置（即，N={1, 2}, M={2, 6, 10, 14}）。对于Wide-ResNet-28-2和Wide-ResNet-28-10，我们发现N=1, M=2和N=2, M=14分别获得了最好的结果。再一次，RandAugment对两种架构获得了有竞争力或更好的结果（表2）。

### 4.3. SVHN

Because SVHN is composed of numbers instead of natural images, the data augmentation strategy for SVHN may differ substantially from CIFAR-10. Indeed, [5] identified a qualitatively different policy for CIFAR-10 than SVHN. Likewise, in a semi-supervised setting for CIFAR-10, a policy learned from CIFAR-10 performs better than a policy learned from SVHN [50].

由于SVHN是由数字而不是自然图像组成的，对SVHN的数据扩增策略与CIFAR-10可能会显著不同。确实，[5]对SVHN和CIFAR-10得到的策略就非常不同。类似的，在CIFAR-10的半监督设置中，从CIFAR-10学习得到的策略比从SVHN学习得到的策略要更好。

SVHN has a core training set of 73K images [34]. In addition, SVHN contains 531K less difficult “extra” images to augment training. We compare the performance of the augmentation methods on SVHN with and without the extra data on Wide-ResNet-28-2 and Wide-ResNet-28-10 (Table 2). In spite of the large differences between SVHN and CIFAR, RandAugment consistently matches or outperforms previous methods with no alteration to the list of transformations employed. Notably, for Wide-ResNet-28-2, applying RandAugment to the core training dataset improves performance more than augmenting with 531K additional training images (98.3% vs. 98.2%). For, Wide-ResNet-28-10, RandAugment is competitive with augmenting the core training set with 531K training images (i.e. within 0.2%). Nonetheless, Wide-ResNet-28-10 with RandAugment matches the previous state-of-the-art accuracy on SVHN which used a more advanced model [5].

SVHN的核心训练集有73K图像。另外，SVHN包含531K不那么难的额外图像，来扩增训练。我们比较了有或没有额外数据下，扩增方法对Wide-ResNet-28-2和Wide-ResNet-28-10下的性能（表2）。尽管SVHN和CIFAR非常不同，RandAugment还是与之前的方法类似，或超过了之前的方法，采用的变形的列表并没有改变。值得注意的是，对Wide-ResNet-28-2，对核心训练数据集采用RandAugment改进的性能，超过了对531K额外训练数据进行扩增得到的性能(98.3% vs. 98.2%)。对Wide-ResNet-28-10，RandAugment对核心训练数据集进行扩增得到的性能，与对531K训练数据进行扩增得到的性能类似（即，在0.2%内）。尽管如此，Wide-ResNet-28-10采用RandAugment与之前在SVHN得到的最好效果类似。

### 4.4. ImageNet

Data augmentation methods that improve CIFAR-10 and SVHN models do not always improve large-scale tasks such as ImageNet. For instance, Cutout substantially improves CIFAR and SVHN performance [8], but fails to improve ImageNet [32]. Likewise, AutoAugment does not increase the performance on ImageNet as much as other tasks [5], especially for large networks (e.g. +0.4% for AmoebaNet-C [5] and +0.1% for EfficientNet-B5 [47]). One plausible reason for the lack of strong gains is that the small proxy task was particularly impoverished by restricting the task to ∼10% of the 1000 ImageNet classes.

改进CIFAR-10和SVHN的数据扩增方法，对大型数据集并不一定改进性能，如ImageNet。比如，Cutout显著改进了CIFAR和SVHN的性能，但没有改进ImageNet的性能。类似的，AutoAugment在ImageNet上改进的性能，并没有其他任务那么多，尤其是对于大型网络(e.g. +0.4% for AmoebaNet-C [5] and +0.1% for EfficientNet-B5 [47])。缺少很强的提升的一个可行原因是，小型代理任务是将任务限制到了1000个ImageNet类别的~10%上。

Table 3 compares the performance of RandAugment to other learned augmentation approaches on ImageNet. RandAugment matches the performance of AutoAugment and Fast AutoAugment on the smallest model (ResNet-50), but on larger models RandAugment significantly outperforms other methods achieving increases of up to +1.3% above the baseline. For instance, on EfficientNet-B7, the resulting model achieves 85.0% – a new state-of-the-art accuracy – exhibiting a 1.0% improvement over the baseline augmentation. These systematic gains are similar to the improvements achieved with engineering new architectures [59, 28], however these gains arise without incurring additional computational cost at inference time.

表3比较了RandAugment的性能与其他学习得到的扩增方法在ImageNet上的效果。RandAugment在最小的模型上(ResNet-50)与AutoAugment和Fast Augment性能类似，但在更大的模型上，RandAugment显著超过了其他方法，比基准增加了1.3%。比如，在EfficientNet-B7上，得到的模型获得了85.0%的结果，一个最新的最好准确率，比基准扩增提升了1.0%。这些系统性的提升与新架构的提升类似，但这些提升并没有增加推理时的计算代价。

### 4.5. COCO

To further test the generality of this approach, we next explore a related task of large-scale object detection on the COCO dataset [27]. Learned augmentation policies have improved object detection and lead to state-of-the-art results [57]. We followed previous work by training on the same architectures and following the same training schedules (see Appendix A.3). Briefly, we employed RetinaNet [26] with ResNet-101 and ResNet-200 as a backbone [17]. Models were trained for 300 epochs from random initialization.

为进一步测试本方法的泛化性，下一步我们探索了相关的大规模目标检测任务，在COCO数据集上。学习得到的扩增策略改进了目标检测的性能，得到了目前最好的结果。我们按照之前的工作，训练了相同的架构，按照相同的训练方案。简要来说，我们采用了ResNet-101和ResNet-200为骨干的RetinaNet。模型从随机初始化开始训练了300 epochs。

Table 4 compares results between a baseline model, AutoAugment and RandAugment. AutoAugment leveraged additional, specialized transformations not afforded to RandAugment in order to augment the localized bounding box of an image [57]. In addition, note that AutoAugment expended ∼15K GPU hours for search, where as RandAugment was tuned by on merely 6 values of the hyperparameters (see Appendix A.3). In spite of the smaller library of specialized transformations and the lack of a separate search phase, RandAugment surpasses the baseline model and provides competitive accuracy with AutoAugment. We reserve for future work to expand the transformation library to include bounding box specific transformation to potentially improve RandAugment results even further.

表4比较了基准模型，AutoAugment和RandAugment的结果。AutoAugment利用了额外的，专门的变换，以扩增一个图像中的局部边界框，这在RandAugment中是没有的。另外，注意AutoAugment花费了~15K GPU小时进行搜索，而RandAugment只调整了超参数的6个值（见附录A.3）。尽管RandAugment的专用变换库更小，缺少单独的搜索阶段，但仍然超过了基准模型，与AutoAugment性能类似。我们未来再拓展变换库，以增加边界框专用的变换，以进一步改进RandAugment。

### 4.6. Investigating the dependence on the included transformations

RandAugment achieves state-of-the-art results across different tasks and datasets using the same list of transformations. This result suggests that RandAugment is largely insensitive to the selection of transformations for different datasets. To further study the sensitivity, we experimented with RandAugment on a Wide-ResNet-28-2 trained on CIFAR-10 for randomly sampled subsets of the full list of 14 transformations. We did not use flips, pad-and-crop, or cutout to only focus on the improvements due to RandAugment with random subsets. Figure 4a suggests that the median validation accuracy due to RandAugment improves as the number of transformations is increased. However, even with only two transformations, RandAugment leads to more than 1% improvement in validation accuracy on average.

RandAugment再不同的任务和数据集上使用相同的变换列表得到了目前最好的结果。这个结果表明，RandAugment在不同数据集上对变换的选择基本上是不敏感的。为进一步研究敏感性，我们用RandAugment对Wide-ResNet-28-2在CIFAR-10上的训练进行试验，对14种变换进行随机采样得到子集。我们没有使用翻转，pad-and-crop，或cutout，以聚焦在由不同子集的RandAugment得到的改进上。图4a说明，随着变换数量的增加，由于RandAugment导致的中值验证准确率也在增加。但是，即使只用了2个变换，RandAugment也带来了平均超过1%的性能改进。

To get a sense for the effect of individual transformations, we calculate the average improvement in validation accuracy for each transformation when they are added to a random subset of transformations. We list the transformations in order of most helpful to least helpful in Table 5. We see that while geometric transformations individually make the most difference, some of the color transformations lead to a degradation of validation accuracy on average. Note that while Table 5 shows the average effect of adding individual transformations to randomly sampled subsets of transformations, Figure 4a shows that including all transformations together leads to a good result. The transformation rotate is most helpful on average, which was also observed previously [5, 57]. To see the effect of representative transformations in more detail, we repeat the analysis in Figure 4a for subsets with and without (rotate, translate-x, and posterize). Surprisingly, rotate can significantly improve performance and lower variation even when included in small subsets of RandAugment transformations, while posterize seems to hurt all subsets of all sizes.

为得到单个变换的效果，我们计算了每个变换加入到随机子集的变换中时，得到的验证准确率的平均改进。我们在表5中以最有帮助到最没有帮助的顺序列出了变换。我们看到，几何变换得到的帮助最大，一些色彩变换平均带来的效果是负的。注意，表5给出了将单个变换加速到变换的随机子集中时的平均效果，图4a却表明，所有变换在一起得到了很好的结果。旋转变换是最有帮助的，之前也看到了这个结果。为更加细节的看到有代表性的变换的效果，我们在图4a中，对有和没有这些变换的子集，重复了这个分析。令人惊奇的是，旋转可以显著改进性能，降低变化，即使在很小子集的RandAugment变换中时，但posterize似乎却伤害了所有大小的子集的性能。

### 4.7. Learning the probabilities for selecting image transformations

RandAugment selects all image transformations with equal probability. This opens up the question of whether learning K probabilities may improve performance further. Most of the image transformations (except posterize, equalize, and autoContrast) are differentiable, which permits back-propagation to learn the K probabilities [30]. Let us denote α_ij as the learned probability of selecting image transformation i for operation j. For K=14 image transformations and N=2 operations, α_ij constitutes 28 parameters. We initialize all weights such that each transformation is equal probability (i.e. RandAugment), and update these parameters based on how well a model classifies a held out set of validation images distorted by αij. This approach was inspired by density matching [25], but instead uses a differentiable approach in lieu of Bayesian optimization. We label this method as a 1st-order density matching approximation.

RandAugment以相等的概率选择所有图像变换。这就提出一个问题，是否学习K个概率会可能进一步改进性能呢？多数图像变换（除了posterize, equalize和autoContrast）都是可微分的，这使得反向传播可以学习K个概率。我们用α_ij表示对运算j选择图像变换i的学习到的概率。对于K=14个图像变换和N=2个运算，α_ij构成了28个参数。我们初始化了所有权重，每个变换是等概率的，以一个模型对保留验证集分类性能如何来更新这些参数。这个方法是由density matching启发得到的，但使用了一种可微分的方法，代替了贝叶斯优化。我们将这种方法称为一阶density matching近似。

To test the efficacy of density matching to learn the probabilities of each transformation, we trained Wide-ResNet-28-2 and Wide-ResNet-28-10 on CIFAR-10 and the reduced form of CIFAR-10 containing 4K training samples. Table 6 indicates that learning the probabilities α_ij slightly improves performance on reduced and full CIFAR-10 (RA vs 1st). The 1st-order method improves accuracy by more than 3.0% for both models on reduced CIFAR-10 compared to the baseline of flips and pad-and-crop. On CIFAR-10, the 1st-order method improves accuracy by 0.9% on the smaller model and 1.2% on the larger model compared to the baseline. We further see that the 1st-order method always performs better than RandAugment, with the largest improvement on Wide-ResNet-28-10 trained on reduced CIFAR-10 (87.4% vs. 86.8%). On CIFAR-10, the 1st-order method outperforms AutoAugment on Wide-ResNet-28-2 (96.1% vs. 95.9%) and matches AutoAugment on Wide-ResNet-28-10. Although the density matching approach is promising, this method can be expensive as one must apply all K transformations N times to each image independently. Hence, because the computational demand of KN transformations is prohibitive for large images, we reserve this for future exploration. In summary, we take these results to indicate that learning the probabilities through density matching may improve the performance on small-scale tasks and reserve explorations to larger-scale tasks for the future.

为测试density matching学习每个变换的概率的效果，我们在CIFAR-10和只包含4K训练样本的缩减版CIFAR-10上训练了Wide-ResNet-28-2和Wide-ResNet-28-10。表6表明，学习概率α_ij略微改进了性能。在缩减版CIFAR-10上，一阶方法改进了准确率，比基准高了3.0%。在CIFAR-10上，与基准相比，一阶方法在较小模型上改进了0.9%，在大一些的模型上改进了1.2%。我们还看到，一阶方法一直比RandAugment要好，最大的改进是Wide-ResNet-28-10在缩减版CIFAR-10上训练的结果。在CIFAR-10上，一阶方法在Wide-ResNet-28-2中超过了AutoAugment，在Wide-ResNet-28-10上与AutoAugment类似。虽然density matching方法是很有希望的，但这个方法比较昂贵，因为需要将K个变换对每幅图像独立应用N次。因此，由于KN个变换在较大图像上的计算需求是很高的，我们以后再进行探索。总结起来，这些结果说明，学习这些概率在小规模任务上可能改进性能，未来再探索再大规模任务中的表现。

## 5. Discussion

Data augmentation is a necessary method for achieving state-of-the-art performance [43, 23, 7, 54, 13, 36]. Learned data augmentation strategies have helped automate the design of such strategies and likewise achieved state-of-the-art results [5, 25, 20, 57]. In this work, we demonstrated that previous methods of learned augmentation suffers from systematic drawbacks. Namely, not tailoring the number of distortions and the distortion magnitude to the dataset size nor the model size leads to sub-optimal performance. To remedy this situation, we propose a simple parameterization for targeting augmentation to particular model and dataset sizes. We demonstrate that RandAugment is competitive with or outperforms previous approaches [5, 25, 20, 57] on CIFAR-10/100, SVHN, ImageNet and COCO without a separate search for data augmentation policies.

数据扩增是获得目前最好性能的必须方法。学习数据扩增策略，帮助这种策略的设计自动化，也得到了目前最好的结果。在本文中，我们证明了，之前的学习扩增的方法有系统性的缺陷。即，没有对数据集大小和模型大小，来定制形变的数量和形变的幅度，得到的性能也不是最优的。为弥补这种情况，我们对特定的模型和数据集大小扩增提出了一种简单的参数化方法。我们证明了，在CIFAR-10/100, SVHN, ImageNet和COCO数据集上，不需要单独搜索数据扩增策略，RandAugment与之前的方法是类似的，或超过了之前的方法。

In previous work, scaling learned data augmentation to larger dataset and models have been a notable obstacle. For example, AutoAugment and Fast AutoAugment could only be optimized for small models on reduced subsets of data [5, 25]; population based augmentation was not reported for large-scale problems [20]. The proposed method scales quite well to datasets such as ImageNet and COCO while incurring minimal computational cost (e.g. 2 hyperparameters), but notable predictive performance gains. An open question remains how this method may improve model robustness [32, 52, 41] or semi-supervised learning [50]. Future work will study how this method applies to other machine learning domains, where data augmentation is known to improve predictive performance, such as image segmentation [3], 3-D perception [35], speech recognition [19] or audio recognition [18]. In particular, we wish to better understand if or when datasets or tasks may require a separate search phase to achieve optimal performance. Finally, an open question remains how one may tailor the set of transformations to a given tasks in order to further improve the predictive performance of a given model.

在之前的工作中，将学习到的数据扩增缩放到大型数据集和模型上，是一个障碍。比如，AutoAugment和Fast AutoAugment只能对小模型在缩减版的数据集上进行优化；PBA对大型问题也没有进行试验。提出的方法对大型数据集如ImageNet和COCO的效果很好，带来的计算代价是非常小的（2个超参数），但得到了很好的性能提升。另一个开放的问题是，这种方法对提升模型稳健性或半监督学习的帮助有多大。未来的工作会研究这种方法怎样应用到其他机器学习领域中，其中数据扩增已经可以改进预测性能，如图像分割，3D感知，语音识别或音频识别。特别是，我们希望更好的理解，数据集或任务什么时候需要一个单独的搜索阶段来得到最佳性能。最后，一个开放的问题是，怎样对给定的任务定制变换集合，以进一步改进给定模型的预测性能。