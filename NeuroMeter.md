# NeuroMeter: An Integrated Power, Area, and Timing Modeling Framework for Machine Learning Accelerators

Tianqi Tang et. al. @ UCSB & Google

## 0. Abstract

As Machine Learning (ML) becomes pervasive in the era of artificial intelligence, ML specific tools and frameworks are required for architectural research. This paper introduces NeuroMeter, an integrated power, area, and timing modeling framework for ML accelerators. NeuroMeter models the detailed architecture of ML accelerators and generates a fast and accurate estimation on power, area, and chip timing. Meanwhile, it also enables the runtime analysis of system-level performance and efficiency when the runtime activity factors are provided. NeuroMeter’s micro-architecture model includes fundamental components of ML accelerators, including systolic array based tensor units (TU), reduction trees (RT), and 1D vector units (VU). NeuroMeter has accurate modeling results, with the average power and area estimation errors below 10% and 17% respectively when validated against TPU-v1, TPU-v2, and Eyeriss.

随着机器学习ML在人工智能时代变得无处不在，需要ML专用工具和框架进行架构研究。本文提出了NeuroMeter，一个用于ML加速器的集成的功耗，面积和时序建模框架。NeuroMeter对ML加速器的详细架构进行建模，对功耗，面积和芯片时序生成快速和准确的估计。同时，还可以在给定运行时行为因素时，对系统级的性能和效率进行运行时分析。NeuroMeter的微架构建模包括ML加速器的基础组成部分，包括基于脉动阵列的张量单元(TU)，缩减树(RT)，和1D向量单元(VU)。NeuroMeter的建模结果非常准确，当对TPUv1，TPUv2和Eyeriss进行验证时，其平均功耗和面积估计误差分别低于10%和17%。

Leveraging the NeuroMeter’s new capabilities on architecting manycore ML accelerators, this paper presents the first in-depth study on the design space and tradeoffs of “Brawny and Wimpy” inference accelerators in datacenter scenarios with the insights that are otherwise difficult to discover without NeuroMeter. Our study shows that brawny designs with 64x64 systolic arrays are the most performant and efficient for inference tasks in the 28nm datacenter architectural space with a 500mm^2 die area budget. Our study also reveals important tradeoffs between performance and efficiency. For datacenter accelerators with low batch inference, a small (∼16%) sacrifice of system performance (in achieved Tera OPerations per Second, aka TOPS) can lead to more than a 2x efficiency improvement (in achieved TOPS/TCO). To showcase NeuroMeter’s capability to model a wide range of diverse ML accelerator architectures, we also conduct a follow-on mini-case study on implications of sparsity on different ML accelerators, demonstrating wimpier accelerator architectures benefit more readily from sparsity processing despite their lower achievable raw energy efficiency.

利用NeuroMeter的新能力确定众核ML加速器架构上，本文对设计空间，和在数据中心场景中的强和弱推理加速器的折中，提出了第一次深度研究，如果没有NeuroMeter，这些洞见是很难发现的。我们的研究表明，在28nm数据中心架构空间中，在500mm^2的芯片面积预算下，强的设计，有64x64的脉动阵列，性能最好，对推理任务最高效。我们的研究还表明，在性能和效率之间有重要的折中。对小批次推理的数据中心加速器中，牺牲较少的系统性能(~16% TOPS)，会得到超过2x的效能改进(TOPS/TCO)。为展示NeuroMeter对众多ML加速器架构进行建模的能力，我们还进行了后续的案例研究，稀疏性对不同的ML加速器的影响，证明了更弱的加速器架构从稀疏性处理中受益更多，尽管得到的原始能耗效率更低。

**Index Terms** — accelerator, hardware modeling, deep learning

## 1. Introduction

As Machine learning (ML) becomes pervasive in the era of artificial intelligence, we have witnessed a “Cambrian explosion” of ML accelerators with a plethora of different accelerators being proposed and/or implemented from both academia and industry [47]. These ML accelerators are designed for a wide range of use cases, ranging from cloud to edge devices; they are designed as either standalone accelerators or near-memory processors [37]. With significantly different scenarios, performance and efficiency targets, the design space of ML accelerators is very large and complex, which in turn drives the clear need for a fast and accurate modeling of power, area, and chip timing of ML accelerator architectures. On the other hand, architecture level analytical modeling frameworks, such as CACTI [43], McPAT [39], Wattch [15], and GPUWattch [38], have been proven to be very useful for the architecture community. These modeling frameworks provide fast, accurate, and easy-to-understand modeling results of power, area, and timing on cache, memory, CPU, and GPU. However, with the recent boom of new ML accelerators, the community lacks an architectural analytical modeling framework for ML accelerators.

随着机器学习在人工智能时代变得无处不在，我们目睹了ML加速器的寒武纪大爆发，学术界和工业界提出/实现了很多不同的加速器。这些ML加速器设计用于广泛的使用场景，从云端到边缘设备；它们设计作为独立加速器，或近内存处理器。在非常不同的场景，性能和效率目标下，ML加速器的设计空间非常大非常复杂，这又驱动了对ML加速器架构的功耗，面积和芯片时序的准确快速建模的需求。另一方面，架构级的解析建模框架，如CACTI，McPAT，Wattch和GPUWattch，已经证明对于架构团体非常有用。这些建模框架对缓存，内存，CPU和GPU的功耗，面积和时序给出了快速，准确和容易理解的建模结果。但是，随着最近新的ML加速器的兴起，团体缺少ML加速器的架构解析建模框架。

This paper introduces NeuroMeter, an integrated power, area, and timing modeling framework for ML accelerators. NeuroMeter advances the state-of-the-art from at least three aspects. Firstly, unlike prior ML accelerator modeling frameworks that either model power, area, or timing in isolation or require EDA tools, NeuroMeter is the first framework to simultaneously model power, area, and timing analytically at the architecture level. Secondly, NeuroMeter supports detailed modeling of critical architectural components of ML accelerators, including 2D systolic arrays, reduction trees, 1D vector units, vector register files, and beyond. Thirdly, compared to previous modeling frameworks such as McPAT, NeuroMeter increases the architectural abstraction level. For example, it only requires users to configure high level architecture; meanwhile, it automatically scales and configures dependent hardware resources. As another example, it only requires users to configure high-level design targets, such as TOPS; meanwhile, it automatically searches for the optimal clock rate. To ensure accuracy, we have conducted a rigorous validation on NeuroMeter results on both the component level and the whole-chip level. Our validation shows that NeuroMeter achieves high modeling accuracy, with overall power and area estimation errors below 10% and 17% respectively when validated against TPU-v1 [30], TPU-v2 [29], and Eyeriss [17]. When combined with an external performance simulator via its flexible and extensible interface, NeuroMeter enables a comprehensive study of architecture, system performance (TOPS), power efficiency (TOPS/Watt), and cost efficiency (TOPS/TCO). With its new capabilities, NeuroMeter empowers architects with a fast yet accurate modeling framework for exploring emerging manycore ML accelerators in a large architectural design space.

本文提出了NeuroMeter，一种集成的ML加速器的功耗，面积和时序建模框架。NeuroMeter从至少3个方面推进了目前最好的成绩。第一，之前的ML加速器建模框架，要么单独的对功耗，面积或时序进行建模，或需要EDA工具，NeuroMeter与之不同，它是第一个同时对功耗，面积和时序解析的在架构级进行同时建模的框架。第二，NeuroMeter支持ML加速器关键架构组成部分详细的建模，包括2D脉动阵列，缩减树，1D向量单元，向量寄存器组，等等。第三，与之前的建模框架，如McPAT相比，NeuroMeter增加了架构抽象层次。比如，它只需要用户配置高层次架构；同时，自动缩放和配置有依赖的硬件资源。另一个例子，它只需要用户来配置高层次设计目标，比如TOPS；同时，自动搜索最优的时钟频率。为确保准确性，我们在NeuroMeter结果上进行严格的验证，在组成部分级别，和整芯片级。我们的验证表明，NeuroMeter获得了很高的建模准确率，余TPUv1，TPUv2和Eyeriss进行验证时，功耗和面积的整体估计误差分别低于10%和17%。当与外部性能模拟器通过其灵活和可扩展的接口进行结合时，NeuroMeter可以对架构，系统性能(TOPS)，功耗效率(TOPS/Watt)，价格效率(TOPS/TCO)进行综合的研究。有了这些新能力，NeuroMeter使架构师有了一个快速准确的建模框架，在大型架构设计空间中探索正在出现的众核ML加速器。

The first contribution of this paper is that NeuroMeter significantly advances the state-of-the-art, enhancing the architectural modeling ecosystem of ML accelerators for the community. Recent work such as Accelergy [55] and Timeloop [44] provide an ecosystem for architecture level ML accelerator modeling. Timeloop [44] is an automatic design exploration tool, requiring fast energy consumption evaluations. Such fast energy consumption evaluations are supported by a high-level modeling tool, Accelergy [55], which relies on CACTI and lookup-table based energy models. However, the community still lacks an accurate analytical architecture modeling for the whole accelerator architecture to analytically model all accelerator components in the way CACTI does for memory arrays. NeuroMeter bridges this gap by providing a consistent analytical modeling methodology for the entire accelerator chip, building a strong foundation for Accelergy, Timeloop, among others [31] [36] [51], to form a robust and coherent ecosystem. Meanwhile, with its modular structure, NeuroMeter can also be used as a standalone framework, if the users choose to.

本文的第一个贡献是，NeuroMeter显著推进了目前最好的成绩，增强了ML加速器的架构建模生态系统。最近的工作，如Accelergy和Timeloop为架构级ML加速器建模提供了一个生态环境。Timeloop是一个自动设计探索工具，需要快速能耗评估。这种快速功耗评估是由一个高层次建模工具Accelergy支持的，而Accelergy又依赖于CACTI和基于查找表的能量模型。但是，团体仍然缺少一种对整体加速器架构的精确解析架构建模，以解析的建模所有加速器组成部分，与CACTI对内存阵列建模的方式一样。NeuroMeter填补了这个空白，对整个加速器芯片给出了一个一致的解析建模方法学，为Accelergy，Timeloop和其他的[31,36,51]构建一个强壮的基础，形成了一个稳健的一致的生态系统。同时，NeuroMeter有模块化的结构，也可以用作单独的框架。

The second contribution of this paper is the in-depth and comprehensive design space exploration on ML accelerators. With the recent “Cambrian explosion” of ML accelerators, two clear design paths have emerged. One path is a “Brawny” design that uses a few large cores such as Google’s TPU (single core with a 256x256 systolic array in TPU-v1 [30]; dual cores, each with one 128x128 systolic array in TPU-v2 [21]), while the other path is a “Wimpy” design that uses a sea of small cores such as nVidia’s Volta (640 TensorCores with 64 FMAs per clock per TensorCore [19]) and Ampere (512 TensorCores with 1024 FMAs per clock per TensorCore and hardware supports for structural sparsity [3]). While both design paths have proven to be successful and inspired many subsequent designs, there is no in-depth quantitative understanding about the essence and rationale of either design path. To bridge this gap, we conduct comprehensive and consistent studies on the design space and tradeoffs of “Brawny and Wimpy” for datacenter inference accelerators. Our study reveals that for datacenter chips with a 500mm2 silicon area budget, a dual-core accelerator with four 64x64 systolic arrays per core has superior efficiency and performance on inference tasks among 28nm design points, despite relatively lower utilization. Moreover, our study also reveals important tradeoffs among different design targets. For example, for datacenter accelerators with low batch inference, a small (∼ 16%) sacrifice of performance (achieved TOPS) can lead to more than 2x improvement of efficiency (achieved TOPS/TCO).

本文的第二个贡献是，在ML加速器上深度的综合的设计空间探索。随着最近ML加速器的寒武纪大爆发，出现了两个清晰的设计路径。一种路径是强设计，使用几个大核，如Google的TPU（在TPUv1中是单核，有256x256个脉动阵列；在TPUv2中是双核，每个是128x128脉动阵列），另一条路径是弱设计，使用很多小核，比如nVidia的Volta（640个TensorFlowCores，每个TensorCores每个时钟有64个FMAs），和Ampere（512个TensorCores，每个TensorCore每个时钟1024个FMAs，对结构性稀疏性有硬件支持）。这两种设计路径都被证明是成功的，启发了很多后续设计，对每种设计路径的精华和逻辑，都没有深度的量化的理解。为填补这个空白，我们对在数据中心推理加速器的强和弱的折中和设计空间，进行了综合和连续的研究。我们的研究表明，对于数据中心芯片，有500mm^2的硅面积预算，双核加速器，每个核有4个64x64的脉动阵列，在28nm的设计点上在推理任务上有优异的性能和效率，尽管其利用率相对较低。而且，我们的研究还表明，在不同的设计目标上有重要的折中。比如，对于数据中心加速器，在批次数量较小的推理上，牺牲较少的性能(~16% TOPS)可以得到超过2x的效率提升(TOPS/TCO)。

Based on these choices of accelerator architectures, we also conduct a follow-on mini-case study on energy efficiency (TOPS/Watt) implications of sparsity on both systolic-array and reduction-tree based ML accelerators to showcase NeuroMeter’s capability to model diverse ML accelerator architectures. Our results show that despite their relatively lower energy efficiency, it is easier for wimpier accelerator architectures to benefit from sparsity processing.

基于加速器架构的这些选择，我们还进行了后续的案例研究，即稀疏性对脉动阵列和基于缩减树的ML加速器的功耗效率影响，表明NeuroMeter对建模多个ML加速器架构的能力。我们的结果表明，尽管其功耗效率相对较低，但更弱的加速器架构更容易从稀疏处理中获益。

The rest of the paper is organized as follows: Sec. II gives the overview, modeling methodology, and validation of NeuroMeter; Sec. III leverages NeuroMeter to conduct the case study on brawny and wimpy manycore ML accelerators in the datacenter inference scenarios; Sec. IV conducts a sparse-oriented mini-case study to showcase NeuroMeter’s functionality to model diverse architectures and support various workloads; Sec. V discusses the related work; and Sec. VI concludes the paper with a summary on NeuroMeter and the insights discovered from our two case studies.

本文组织如下：第2部分给出概览，建模方法学，和NeuroMeter的验证；第3部分利用NeuroMeter来对强和弱众核ML加速器在数据中心推理场景中进行案例研究；第4部分进行面向稀疏性的案例研究，表明NeuroMeter建模多个架构，支持多个workloads的能力；第5部分讨论了相关的工作；第6部分对文章得出了结论，总结了NeuroMeter和从我们的两个案例研究中发现的洞见。

## 2. NeuroMeter: Overview, Modeling, Methodology, and Validation

NeuroMeter is an integrated power, area, and timing modeling framework for ML accelerators. Fig. 1 gives an overview of NeuroMeter, and highlights its input/output interface. There are two types of inputs to NeuroMeter: 1) the accelerator hardware configuration (mandatory) for NeuroMeter to construct and optimize the target accelerator; 2) the runtime statistics (optional) for NeuroMeter to conduct runtime analysis. NeuroMeter by default outputs the power, area, and timing of target ML accelerators based on their specified hardware configuration. With the help of an external application-level performance simulator, NeuroMeter enables system performance and efficiency analysis as well.

NeuroMeter是一个用于ML加速器的集成的功耗，面积和时序建模框架。图1给出了NeuroMeter的概览，强调了其输入输出接口。NeuroMeter有两种类型的输入：1)加速器硬件配置（强制输入），NeuroMeter用于构建和优化目标加速器；2)运行时统计（可选输入），NeuroMeter用于进行运行时分析。NeuroMeter默认基于其指定的硬件配置，输出目标ML加速器的功耗，面积和时序。在外部应用级性能仿真器的帮助下，NeuroMeter可以进行系统性能和效率分析。

NeuroMeter allows users to specify the parameters at the architecture, circuit, and technology level, as well as the optimization targets and constraints, as shown in Fig. 1. Besides the essential parameters, such as the core count, clock rate, power supply voltage, and technology node, it only requires the user to provide the high-level configurations of critical hardware components without worrying about the low-level configurations. For example, when the user configures the computing components of the accelerator, they only need to configure critical parameters, such as the tensor unit’s array height/width, the data type of the multiplication-accumulation unit, and the type of inner array interconnection, the tool itself will automatically help the user figure out the dependent hardware components, including the vector register file, the scalar unit, and the interconnection overheads between different components. When the user configures the on-chip memory, they only need to configure the parameters of capacity, block size, target latency, and the target throughput. The tool will automatically set the low-level parameters (such as the number of banks, the number of the read/write ports) via its internal optimizer.

NeuroMeter允许用户指定架构，电路和技术级的参数，以及优化目标和约束，如图1所示。除了必不可少的参数，比如核数量，时钟频率，供电电压和技术节点，只需要用户提供关键硬件组成部分的高层次配置，不需要担心低层次配置。比如，当用户配置加速器的计算组成部分，只需要配置关键参数，比如张量单元的阵列宽度/高度，MAC单元的数据类型，内部阵列互联的类型，工具自身就会自动帮助用户搞定相关的硬件组成部分，包括向量寄存器组，标量单元，在不同组成部分之间的互联开销。当用户配置片上内存时，只需要配置容量，block大小，目标延迟和目标吞吐量的参数。工具会通过其内部优化器自动设置底层参数（比如banks的数量，读/写端口的数量）。

By default, NeuroMeter requires the input of system-level performance (i.e., peak TOPS) as the optimization target (or a minimal value of it as a design constraint). TOPS/Watt and TOPS/TCO are also allowed to feed in as alternative optimization targets or design constraints. Given the system-level performance constraints, NeuroMeter conducts the component-level timing analysis using an Elmore delay model [23]. Once a design is found to meet the optimization targets and design constraints, NeuroMeter finalizes an internal chip representation to get the chip-level thermal design power (TDP), silicon area, and their component-level breakdowns. NeuroMeter also outputs the timing information of the electrical signal propagation delay (e.g., Elmore Delay) and the cycle time per component to help the user find out the hardware critical path.

默认情况下，NeuroMeter需要系统级性能的输入（如，峰值TOPS）作为优化目标（或者其最小值作为设计约束）。TOPS/Watt和TOPS/TCO也可以送入作为其他优化目标，或设计约束。给定系统级性能约束，NeuroMeter进行组成部分级的时序分析，使用Elmore延迟模型。一旦发现一个设计满足优化目标和优化约束，NeuroMeter完成内部芯片表示，得到芯片级的热设计功耗(TDP)，硅片面积，和其组成部分级的分解。NeuroMeter还输出电子信号传播延迟的时序信息（如，Elmore延迟）和每个组成部分的周期时间，以帮助用户找到硬件关键路径。

When given the runtime statistics of the target ML model running on the accelerator, NeuroMeter also combines the inputs of runtime statistics on hardware utilization and activity factors for micro-architecture components with the chip-level TDP and silicon area to generate the end-to-end runtime estimation of performance1, power, and efficiency of the target accelerators running specified ML models. NeuroMeter decouples the performance simulation from the architecture modeling, so that it can be flexibly paired with any external performance simulation framework for comprehensive ML accelerator research.

当给定目标ML模型在加速器上运行的运行时统计后，NeuroMeter还能将输入的运行统计的硬件利用和微架构组成部分的活跃因子，与芯片级TDP和芯片面积结合到一起，生成端到端的目标加速器运行指定ML模型的性能，功耗和效率的运行时估计。NeuroMeter将性能仿真与架构建模进行了解耦，这样可以灵活的与任何外部性能仿真框架进行配对，进行综合的ML加速器研究。

### 2.1. Architecture-Level Modeling

NeuroMeter follows a top-down modeling methodology. As shown in Fig. 2(a), high-level blocks are divided into lower-level sub-blocks and finally mapped onto the circuit-level models of compute logic units, memory arrays, and hierarchical wires, with backend technology device and wiring parameters. At the chip architecture level, NeuroMeter models a multicore ML accelerator. Fig. 2(b) gives an example of a multicore accelerator with a 2D-mesh Network-on-Chip (NoC), while other types of NoCs are also supported, including bus, ring, and H-tree. Other peripheral blocks, including Memory Controllers (MCs) and DMA controllers, are also modeled.

NeuroMeter遵循的是自上而下的建模方法学。如图2a所示，高层模块分成了更低层的子模块，最终映射到电路级的模型，即计算逻辑单元，内存阵列，层次连接线，有后端技术设备和连线参数。在芯片架构层次，NeuroMeter建模了一个多核ML加速器。图2b给出了多核加速器和2D-mesh NoC的例子，也支持其他类型的NoCs，包括总线，ring，和H-tree。其他外围模块，包括内存控制器和DMA控制器，也进行了建模。

At the core architectural level, NeuroMeter breaks down a single core into an Instruction Fetch Unit (IFU), a Load-and-Store Unit (LSU), an EXecution Unit (EXU), and a Scalar Unit (SU) for control. An IFU in ML accelerators is usually lightweight, unlike the complicated front-end circuit in high performance general-purpose processors. An LSU in ML accelerator includes on-chip memory (Mem) and data/control paths to off-chip memory. The most critical component is EXU, which is further broken down into multiple functional units, i.e., 2D systolic array based Tensor Unit (TU), Reduction Tree (RT), 1D Vector Unit (VU), Vector Register file (VReg), and Central Data Bus (CDB). Each unit is discussed below.

在核心架构层次，NeuroMeter将单核分解成了取指令单元(IFU)，LSU，执行单元(EXU)，和标量单元(SU)用于控制。ML加速器中的IFU通常是轻量级的，与高性能通用处理器中的复杂的前端电路不同。ML加速器中的LSU包括片上内存(Mem)和到片外存储的数据/控制通路。最关键的组成部分是EXU，会进一步分解成多个功能单元，即，基于2D脉动阵列的张量单元(TU)，缩减树(RT)，1D向量单元(VU)，向量寄存器组(VReg)，中央数据总线(CDB)。每个单元在下面进行讨论。

**Tensor Unit (TU)** is a generic systolic array made up of three parts, (1) an array of systolic cells (SCs), each one of which consists of a multiplication-accumulation (MAC) unit and a D-Flip-Flop (DFF) or SRAM based local buffer; (2) wires connecting nearby systolic cells; (3) DFF/SRAM based I/O FIFOs. Our tool supports TUs with various types of interconnections between systolic cells and I/O FIFOs. Fig. 2(c) exemplifies two types of inner-TU interconnections, including unicast as in Google’s TPU-v1, and multicast (or X/Y bus) as in Eyeriss. For systolic arrays (or unicast TUs) we support modeling of both weight-stationary and output-stationary dataflow with a flexible systolic cell configuration. At the circuit level, MAC units inside the systolic cells are presimulated through EDA tools, while the DFF/SRAM based local buffers, I/O FIFO, and the cell-to-cell interconnections are modeled analytically. Fig. 2(d) illustrates the multicast inner-TU interconnection as an example, i.e., the interconnection is decomposed into several segments of wires that are abstracted into the π-RC model; the output resistance of the I/O FIFO and the input resistance of the systolic cells are extracted as the drive and the load of the RC path respectively.

张量单元TU是一个通用脉动阵列，由3部分组成，(1)脉动单元(SC)阵列，每个单元是由一个MAC单元和一个基于DFF或SRAM的局部缓冲器组成；(2)连接附近的脉动单元的连接线；(3)基于DFF/SRAM的I/O FIFOs。我们的工具支持在脉动单元和I/O FIFOs之间各种类型互联的TUs。图2c给出了两种类型的TU内部互联，包括Google TPUv1中的unicast，和Eyeriss中的multicast（或X/Y总线）。对于脉动阵列（或unicast TUs），我们支持weight-stationary和output-stationary数据流的建模，有灵活的脉动单元配置。在电路级，脉动单元中的MAC单元通过EDA工具进行预仿真，而基于DFF/SRAM的局部缓冲器，I/O FIFO和单元到单元的互联，是解析建模的。图2d描述了multicast inner-TU互联的一个例子，即，互联分解成几个抽象成π-RC模型的线段；I/O FIFO的输出阻抗和脉动阵列的阻抗，分别提取为RC路径的驱动和负载。

**Reduction Tree (RT)** is made up of three parts, (1) a N-input 1D MAC array (which is similar as in VU); cascaded by (2) a log(N)-layered adder tree; (3) the (optional) DFFs between the two nearby layers to satisfy the timing constraints if needed. In the default configuration, we assume that each layer uses an array of 2-by-1 adders in the adder tree. The users can customize the type of the adder and the level of the adder tree according to their design requirements. The RT is broadly used in sparsity-aware accelerator designs [36] [48] [57] since it has more flexible workload mapping compared with the 2D array based TU.

缩减树(RT)是由3部分组成，(1)一个N输入的1D MAC阵列（与VU类似）；级联的是(2)log(N)层的加法器树；(3)可选的DFFs，在两个邻近的层之间，如果需要的话，可以满足时序约束。在默认的配置中，我们假设，每个层使用加法器树中的2x1的加法器的阵列。用户可以根据其设计需求来定制加法器的类型，和加法器树的层次。RT在感知稀疏性的加速器设计中广泛使用，因为与基于2D阵列的TU，其workload映射非常灵活。

**Vector Unit (VU)** processes 1D vector operations, such as pooling, activation, and different variants of normalization. It also merges the partial sums when one TU is not large enough to hold the whole Conv2D or MatMul operator without tiling. Moreover, in some ML accelerators [26] without 2D TUs, VUs are the main processing elements. Such accelerators can be well supported by NeuroMeter. The vector register file (VReg) is the center for data exchange inside VU as well as between VU and TU. In the default architectural configuration, the number of the VU lanes and the vector width of VReg match the TU array length; and each TU/VU has private read/write VReg ports. For the core with single VU and single TU, VReg is configured as 4 read ports and 2 write ports to support dual issue width. Meanwhile, multiple TUs can be configured to share one group of read/write VReg ports. In that case, the external performance tool has to exclude the mapping based on independent data to different TUs, or include the extra cost when data broadcast is not applicable.

向量单元(VU)处理1D向量运算，比如池化，激活，和归一化的不同变体。当TU在没有tiling的情况下不足以保持整个Conv2D或MatMul算子，VU还要融合部分和。而且，在一些没有2D TUs的ML加速器中，VUs是主要的处理元素。NeuroMeter也可以很好的支持这样的加速器。向量寄存器组(VReg)是VU内部数据交换的中心，也是VU和TU之间的数据交换中心。在默认架构配置中，VU lanes的数量，和VReg的向量宽度，与TU的阵列宽度匹配；每个TU/VU都有私有的读/写VReg端口。对于有单个VU和单个TU的核，VReg配置为4读端口和2写端口，以支持双发射宽度。同时，可以配置多个TUs，共享一族读/写VReg端口。在这种情况下，外部性能工具需要排除基于独立的数据到不同的TUs的映射，或在数据广播不可应用时，包含额外的代价。

**Scalar Unit (SU)** is mainly used for auxiliary operations in the control flow, e.g., address calculation. Leveraging McPAT’s configuration, SU is by default configured as a simplified “ARM Cortex-A9 core” which only has the instruction fetch unit (w/o branch prediction logic), integer register file, ALU, and LSU, with the rest of the core removed. It can also be easily reconfigured to other architectures.

标量单元SU主要用于控制流中的辅助运算，如，地址计算。利用McPAT的配置，SU默认配置为简化的ARM Cortex-A9核，只有IFU（没有分支预测逻辑），整数寄存器组，ALU和LSU，核的其他部分都移除掉了。也可以很容易的重新配置为其他架构。

**On-chip Memory (Mem)** models the storage units, which hold the weights and feature maps on the chip. It can be configured as a software-managed scratchpad memory, which is commonly used in many ML ASICs, or a cache hierarchy. The cell type of Mem can be selected from DFF, SRAM, and eDRAM. According to the throughput requirements, Mem is always configured as multi-banked. Based on the architectural configurations, Mem can be modeled as a unified structure where weights and activations are stored together as in TPUv1, or as a dedicated structure where each bank has its own functionality as in Eyeriss.

片上内存(Mem)建模存储单元，保持着芯片中的权重和特征图。可以配置为软件管理的便签内存，在很多ML ASICs中经常使用，或是一个缓存层次结构。Mem的单元类型可以从DFF，SRAM和eDRAM中选择出。根据吞吐量要求，Mem配置为multi-banked。基于架构配置，Mem可以建模为一个统一的结构，在TPUv1中权重和激活存储到一起，或建模为专用结构，在Eyeriss中每个bank都有其各自的功能。

**Central Data Bus (CDB)** models the interconnection between different components within the core, especially the wires connecting VReg and other functional components, including TU, VU, and Mem. Wires are assumed to route around the functional components, and their length is estimated by the square root of the functional component area. When the length is large, wires are pipelined to meet the throughput requirement.

中央数据总线CDB对核中不同组成部分的互联进行建模，尤其是连接VReg和其他功能组成部分的连线，包括TU，VU和Mem。连线假设在功能组成部分周围进行布线，其长度由功能组成部分的面积的平方根进行估计。当长度很大时，连线要满足吞吐量要求。

### 2.2. Circuit and Technology-Level Modeling

NeuroMeter models the power, area, and timing of the hardware components analytically and simultaneously. Similar to McPAT, NeuroMeter maps the architectural components to basic logic gates and regular circuit blocks, including computing arrays (e.g., TU, VU), memory arrays (e.g., DFF, SRAM, and eDRAM), interconnects (e.g., router, link, and bus), and regular logic (e.g., decoder and dependency-checking unit). These circuit blocks are then mapped to fundamental analytical RC ladder/trees and layout models to compute timing, area, and energy at different technology nodes.

NeuroMeter对硬件组成部分的功耗，面积和时序进行解析和同时建模。与McPAT类似，NeuroMeter将架构组成部分映射到基本逻辑门和常规电路模块，包括计算阵列（如TU，VU），内存阵列（如，DFF，SRAM和eDRAM），互联（如，router，link和bus），和常规逻辑（如，解码器和依赖检查单元）。这些电路模块然后映射到基本解析RC ladders/trees和layout模型，以在不同的技术节点计算时序，面积和功耗。

However, an analytical approach does not work well for complex structures that have custom layouts, such as the MAC logic in the TU, VU, and SU. For these components, NeuroMeter currently takes an empirical modeling approach, which utilizes curve fitting to build a parameterizable numerical model for the area and power of complex components. The empirical model is based on synthesis results from Design Compiler using the RTL models from Berkeley Hardware Floating Point Unit Library [2] with the technology backend of FreePDK [13] [42] libraries.

但是，解析方法对复杂的结构效果可能不好，比如有定制的layout，如TU，VU和SU中的MAC逻辑。对于这些组成部分，NeuroMeter目前采取经验建模方法，对复杂组成部分的面积和功耗，利用曲线拟合来构建参数化的数值模型。经验模型是基于DC的综合结果，使用的是Berkeley Hardware Floating Point Unit Library的RTL模型，技术后端是FreePDK库。

### 2.3. Validation

The primary focus of NeuroMeter is fast yet accurate power and area modeling at the architectural level when given the target system performance (i.e., peak TOPS). To ensure the accuracy of NeuroMeter, we conduct rigorous validations at both the component level and the whole chip level. At the component level, we validate NeuroMeter’s power, area, and timing results against the synthesis results from Chisel [11] with the FreePDK45 library. The validation against EDA tools shows that NeuroMeter’s prediction is within a 15% area error margin, which provides strong confidence for our component level modeling accuracy. As power highly depends on the block activity factors, we rigorously validate it at the chip level assuming average power traces. At chip level, we validate against TPU-v1 [30], TPU-v2 [29], and Eyeriss [17]. NeuroMeter demonstrates satisfying modeling accuracy, with about 10% and 17% error margins on overall power and area respectively against the three real ML accelerators. It is important to note that chip-to-chip power variation in modern microprocessors [14] is comparable to the magnitude of the power validation errors of NeuroMeter.

NeuroMeter的主要焦点是，在给定目标系统性能（如，峰值TOPS）时，快速但准确的架构级功耗和面积建模。为确保NeuroMeter的准确率，我们进行了组成部分级和整芯片级的严格验证，我们与用了FreePDK45库的Chisel综合结果比较，验证了NeuroMeter的功耗，面积和时序结果。与EDA工具的比较验证表明，NeuroMeter的预测面积误差在15%以内，为我们组成部分级的建模准确率提供了很强的信心。由于功耗高度依赖于模块活跃度因子，我们在芯片级进行严格验证，假设平均power traces。在芯片级，我们与TPUv1，TPUv2和Eyeriss进行验证。NeuroMeter表现出了令人满意的建模准确率，与3个真实的ML加速器相比，其总体功耗和面积误差分别在10%和17%左右。必须要指出，现代微处理器的芯片与芯片间的功耗变化，与NeuroMeter的功耗验证误差幅度是类似的。

Fig. 3 shows TPU-v1’s validation results of power and area, at a 28nm technology node with a 700MHz target clock rate. At the chip level, the modeling results of overall power (i.e., TDP) and area have <5% and <10% error respectively, compared with the published TDP (75W) and area (<331mm^2). At the component level, TPU-v1 contains four major parts: (1) a MAC-based Systolic Array for matrix multiplication; (2) a Unified Buffer & Weight FIFO for activation and weights; (3) an Accumulator Buffer for partial sums; and (4) an Activation Pipeline for other operations. NeuroMeter models the systolic array by the TU with a unicast interconnection; models the unified buffer, accumulator buffer, and the weight FIFO by the Mem; and models the activation pipeline with the VU. As shown in Fig. 3(a), NeuroMeter produces accurate area modeling results (within 2% relative error) for the systolic array and the accumulator buffer; but over-estimates the relative area of the unified buffer by ∼10%, which may be due to the lack of knowledge of optimized placement-and-routing for the interconnect between systolic array and unified buffer in TPU-v1. We also model the peripheral interfaces including DRAM port (6.0% v.s. 2.8%) and PCIe interface (3.0% v.s. 1.8%). We currently do not model host interface, controller, and misc I/O, with 5% in total. The unknown components in TPU-v1 occupy ∼21% of the chip area, and we use the same percentage as white space in our area overall estimation. Although no published data exists to compare against, the NeuroMeter power breakdown is shown in Fig. 3(b), where the systolic array is the biggest power consumer with 56% of the total chip power.

图3展示了TPUv1的功耗和面积验证结果，技术节点为28nm，目标时钟频率为700MHz。在芯片级，与发布的TDP(75W)和面积(<331mm^2)相比，总体功耗(TDP)和面积的建模结果误差分别小于5%和10%。在组成部分级，TPUv1包含4个主要部分：(1)基于MAC的脉动阵列，用于矩阵乘法；(2)用于激活和权重的统一缓冲区和权重FIFO；(3)累加器缓冲区，用于部分和；(4)激活流水，用于其他运算。NeuroMeter用带有unicast互联的TU，来建模脉动阵列；用Men建模统一缓冲区，累加缓冲区和权重FIFO；用VU建模激活流水。如图3a所示，NeuroMeter对脉动阵列和累加器缓冲区给出了准确的面积建模结果（相对误差在2%以内），但过高估计了统一缓冲区的相对面积（约10%），这可能是因为缺少TPUv1中关于脉动阵列和统一缓冲区之间的优化的布局布线知识。我们还建模了外围接口，包括DRAM端口(6.0% v.s. 2.8%)和PCIe接口(3.0% v.s. 1.8%)。我们目前不建模宿主接口，控制器，和各种I/O，总体大概5%。TPUv1中未知的组成部分占据了约21%的芯片面积，我们在面积总体估计中使用相同的比例作为白色空间。虽然没有发布的数据来进行对比，NeuroMeter的功耗分解如图3b所示，其中脉动阵列是最大的功耗部分，占总芯片功耗的56%。

Fig. 4 shows the area validation of TPU-v2 at an assumed 16nm technology node with a 700MHz target clock. At the chip level, the modeling results of area (513mm^2) have at most 17% error compared with the published area (< 611mm^2); and the modeling results of TDP (255W) have ∼9.1% error compared with the published TDP (280W). Similar to TPUv1, NeuroMeter models the MXU, Vector Unit, and Vmem by systolic array based TU, VU, and Mem respectively. We would like to highlight that our simulation results show that TPU-v2 requires two read ports and one write ports per bank, and this is automatically searched by NeuroMeter with the given throughput requirement. Furthermore, we also modeled the Inter-Chip Interconnection (ICI) link and switch (12% vs 5%) by the components of Network Interface Unit (NIU) and NoC given the bisectional bandwidth at 496Gb/s per direction. Other peripheral components, including HBM ports (9% v.s 5%) and PCIe Controllers (2% vs 2%) are also modeled. We currently do not model transpose unit, RPU, and misc datapath, with 11% in total. The unknown components (which probably includes the inter-component interconnection) in TPU-v2 occupy ∼21% of the chip area, and we use the same percentage as white space in our overall area estimation.

图4展示了对TPUv2的面积验证，假设是16nm的技术节点，700MHz目标时钟频率。在芯片级，面积建模结果(513mm^2)与发布的面积(<611mm^2)相比，有最多17%的误差；TDP的建模结果(255W)与发布的TDP(280W)相比，有大约9.1%的误差。与TPUv1类似，NeuroMeter分别用基于脉动阵列的TU，VU和Mem，建模了MXU，向量单元，和Vmem。我们要强调，我们的仿真结果表明，TPUv2每个bank需要两个读端口和一个写端口，这是NeuroMeter在给定的吞吐量要求下自动搜索得到的。而且，我们还建模了芯片间互联(ICI)link and switch (12% vs 5%)，用的是网络接口单元(NIU)和NoC，给定每个方向的bisectional带宽为496Gb/s。其他的外围组成部分，包括HBM端口(9% v.s 5%)和PCIe控制器(2% vs 2%)也进行了建模。我们目前没有建模转置单元，RPU和各种数据通路，总计11%。TPUv2中未知的组成部分（很可能包括组成部分间的互联）占据了芯片面积的大约21%，我们在总体面积估计中使用相同的百分比为白色空间。

Fig. 5 shows Eyeriss’s validation results of power and area, at a 65nm technology node with a 200MHz target clock rate. As shown in Fig. 5(a) and (b), the area modeling of the single PE and the overall results have <5% and <15% error respectively. At the single PE level, Eyeriss’s PE is modeled by NeuroMeter’s systolic cells in the TU. At the chip level, Eyeriss’s three major components, i.e., PE Array, Global Buffer, and MultiCast NoC, are modeled by the TU, Mem, and inner-TU connection respectively as introduced in Sec. II-A. Other chip-level components, including Run-Length Code & ReLU, Config Scan Chain, and Top-Level Ctrl, are also modeled. As shown in Fig. 5(b), the relative area breakdown of PE array is overestimated by ∼7%, which may result from the limited knowledge of the exact MAC logic model in use. The relative area breakdown of the global buffer is under-estimated by ∼7%, which may be due to the insufficient knowledge of the outside-bank overhead. Compared with TPU-v1, the area breakdown of the PE array in Eyeriss is much larger than that of the systolic array in TPU-v1. Both of them are modeled by the TU in NeuroMeter, but Eyeriss introduces a heavier local buffer design, i.e., every PE has the local scratchpad memory and register files to support the row-stationary dataflow.

图5展示了Eyeriss的功耗和面积验证结果，技术节点为65nm，目标时钟频率为200MHz。如图5a和b所示，单PE和总体的面积建模结果误差分别小于5%和15%。在单PE级，Eyeriss的PE是由NeuroMeter的TU中的脉动阵列建模的。在芯片级，Eyeriss的三个主要组成部分，即，PE阵列，全局缓冲区，和MultiCast NoC，分别是由TU，Mem和inner-TU连接建模的。其他芯片级的组成部分，包括Run-Length Code & ReLU, Config Scan Chain, Top-Level Ctrl，也进行了建模。如图5b所示，PE阵列的相对面积分解过高估计了大约7%，这可能是对使用中的精确的MAC逻辑模型所知有限导致的。全局缓冲区的相对面积分解过低估计了大约7%，这可能是因为outside-bank开销了解不充分导致的。与TPUv1相比，Eyeriss中的PE阵列面积分解比TPUv1中的脉动阵列要大的多。两者都是由NeuroMeter的TU建模的，但是Eyeriss引入了更重的局部缓冲区设计，即，每个PE都有局部便签内存和寄存器组，以支持row-stationary数据流。

We also validate the runtime power against the report from Eyeriss when running publicly available ML models. As shown in Fig. 5(c) and (d), the overall power has 11% over-estimation and 13% under-estimation respectively when running AlexNet-Conv1 and AlexNet-Conv5 layers. The differences of the runtime power in these two layers may result from the insufficient knowledge of the zero-skipping and clock-gating operation in Eyeriss. To be consistent with the published data, the power consumption breaks down into the following six components, including (1) MAC logic, (2) local buffer (Spad Mem), (3) PE I/O FIFO, (4) PE controller, (5) multicast NoC, and (6) global buffer; and the first five components are the internal structures of the PE array. The unmodeled components include chip I/O pads and top-level control and are not shown in Fig. 5. Since NeuroMeter does not model the clock network as a separate component, we amortize the power breakdown of the clock network into other components. Similar to the TDP in TPU-v1, the PE array in Eyeriss takes the major proportion of the runtime power consumption. Unlike TPU-v1’s TDP, the global buffer in Eyeriss takes a much smaller proportion. This shows the difference between TDP and the runtime power consumption.

我们还在运行公开可用的ML模型时，对Eyeriss的报告，验证了运行时功耗。如图5c和5d所示，在运行AlexNet-Conv1和AlexNet-Conv5层时，总体功耗分别有11%的过高估计和13%的过低估计。这两层不同的运行时功耗，可能是由于对Eyeriss中zero-skipping和clock-gating操作了解不充分导致的。为与发表数据一致，功耗分解成下面6个组成部分，包括(1)MAC逻辑，(2)局部缓冲区(Spad Mem)，(3)PE I/O FIFO，(4)PE控制器，(5)multicast NoC，(6)全局缓冲区；前5个组成部分是PE阵列的内部结构。未建模的组成部分包括芯片的I/O pads和顶层控制，没有在图5中展示。由于NeuroMeter没有将时钟网络作为独立组成部分建模，我们将时钟网络的功耗分摊到其他组成部分中。与TPUv1中的TDP类似，Eyeriss中的PE阵列占据了运行时功耗的主要部分。与TPUv1的TDP不一样的是，Eyeriss中的全局缓冲区所占比例要小的多。这表明了TDp和运行时功耗的不一样之处。

## 3. Case Study on Brawny and Wimpy Manycore Machine Learning Accelerators

Of the many types of ML accelerators that have emerged, one type can be classified as having relatively “Brawny” core designs that use a few large systolic arrays such as Google’s TPU (a single core with a 256x256 systolic array [30] in TPUv1 or dual cores with one 128x128 systolic array per core in TPU-v2 [21]). Another class are designs based on relative “Wimpy” cores that use a sea of small computing arrays or vector processing units such as nVidia’s Volta architecture (640 TensorCores with 64 FMAs per clock per TensorCore [19]) and Ampere architecture (512 TensorCores with 1024 FMAs per clock per TensorCore and hardware supports for structural sparsity [3]). Intuitively, the brawny design is believed to have an advantage of high performance, especially when the tensor size is large enough; while the wimpy design is believed to have an advantage of high utilization without sacrificing performance by using sophisticated compiler and runtime software. However, there is no in-depth and comprehensive study to quantify these hypotheses, partly because of the lack of tools.

已经涌现的众多ML加速器中，一种类型可以归类为，有相对较强的核设计，使用了几个大型脉动阵列，比如Google的TPU（在TPUv1中有一个核有256x256个脉动阵列，或在TPUv2中有2个核，每个有128x128个脉动阵列）。另一类是基于相对较弱的核的设计，使用众多小型计算阵列或向量处理单元，比如nVidia的Volta架构（640个TensorCores，每个TensorCore每个时钟有64个FMAs），和Ampere架构（512个TensorCores，每个TensorCore每个时钟有1024个FMAs，有硬件支持结构稀疏性）。直觉上说，强设计性能会强，尤其是在张量规模足够大时；而弱设计通过使用复杂的编译器和运行时软件，利用率高，不会牺牲性能。但是，没有深度和综合的研究来量化这些假设，部分因为缺少工具。

To bridge this gap and to showcase the capability of NeuroMeter, we conduct detailed analyses to compare brawny and wimpy manycore ML accelerator designs. Interestingly, the brawny v.s. wimpy design tradeoffs have been a critical topic in CPU design and date back to decades ago as summarized in previous work [12]. We hope our work can foster a comprehensive and systematic study of brawny and wimpy design tradeoffs on the ML accelerator frontier.

为弥补这个空白，展示NeuroMeter的功能，我们进行详细分析，来比较强和弱的众核ML加速器设计。有趣的是，强弱设计折中是CPU设计中的关键话题，可以追溯到几十年以前，[12]对此进行了总结。我们希望我们的工作可以促进在ML加速器前沿的综合和系统的强弱设计折中研究。

In the study described in this section, the brawny accelerator architecture uses fewer cores with large systolic array based TU(s) per core, while the wimpy accelerator architecture uses more cores with small systolic array based TU(s) per core. The rest of the on-chip resources are scaled proportionally as the systolic array size changes. While NeuroMeter models both training and inference accelerators, we focus on the inference accelerators in this paper and leave the study of training accelerators to future work.

在本节的研究中，强加速器架构使用更少的核，每个核中是基于大型脉动阵列的TUs，而弱加速器架构使用更多的核，每个核是基于小型脉动阵列的TUs。其余的片上资源是随着脉动阵列大小的变化成比例的变化的。NeuroMeter对训练和推理加速器都进行建模，但我们在本文中关注推理加速器，训练加速器的研究留给未来的工作。

### 3.1. Experiment Methodology and Setup

In our study, we follow the general architecture of manycore ML accelerators shown in Fig. 6. All cores are connected by a 2D mesh NoC. Each core has a systolic array based tensor unit (TU) for matrix operations; meanwhile, each core also has a vector unit (VU) for vector operations. Each core may also have a scalar unit (SU) for control path because of the high throughput of TUs in the core. Each core has a portion of the distributed on-chip memory (Mem). A vector register file (VReg) is the data exchange hub among TU, VU, and Mem. The central data bus (CDB) connects VReg and other components inside the core.

在我们的研究中，我们遵循图6中所示的众核加速器一般性架构。所有的核是由一个2D网格NoC连接在一起的。每个核都有一个基于脉动阵列的张量单元TU，进行矩阵运算；同时，每个核还有一个向量单元VU进行向量运算。每个核还可能有一个标量单元SU用于控制路径，因为核中TUs的通量很高。每个核都有一部分分布式片上内存Mem。向量寄存器组VReg是TU，VU和Mem之间的数据交换中心。中央数据总线CDB将VReg和核中的其他部分连接到一起。

1) Architecture Design Space and Chip Modeling: Since brawny and wimpy is a continuous spectrum in the design space, we denote each architectural design point by a four-element tuple (X, N, Tx, Ty), where X is the TU length that defines how brawny or wimpy the architecture is; N is the number of TU in each core; Tx and Ty are the 2D mesh NoC topology to connect all the cores. Given each tuple of such a design point, NeuroMeter automatically scales and sets the dependent hardware parameters such as the number of VU lanes, the VReg issue width, and VReg port count accordingly as shown in Fig. 6.

架构设计空间和芯片建模：由于强和弱在设计空间中是一个连续谱，我们将每个架构设计点表示为一个4元组(X, N, Tx, Ty)，其中X是TU的长度，定义了架构有多强或多弱；N是在每个核中TU的数量；Tx和Ty是2D mesh NoC拓扑，连接所有的核。对一个设计点给定一个元组，NeuroMeter自动缩放和设置有依赖的硬件参数，比如VU lanes的数量，VReg发射宽度，和VReg端口数量，如图6所示。

To some extent, chip architecting can be considered as an optimization problem, where we try to maximize performance under a given budget on chip area and power. Thus, we pick reasonable optimization targets and design constraints to make the design space exploration manageable. Particularly, as shown in Table I, for datacenter inference accelerators, we constrain the die area to 500mm^2 and TDP to 300W based on recent data center ML accelerators [21] [30]. The memory subsystem is configured with 32MB of software managed on-chip memory distributed to all cores and 700GB/s off-chip HBM bandwidth, similar to Google’s TPU-v2/v3 [46]. Note that TPU-v2/v3 are designed for both training and inference [21]. We then use NeuroMeter to sweep the design space to optimize the TOPS for each design point of (X, N, Tx, Ty) with dependent hardware parameters automatically scaled proportionally to the design point parameters as shown in Fig. 6 and Table I.

在一定程度上，芯片架构可以认为是一个优化问题，其中我们尝试在给定的芯片面积和功耗的运算下，最大化性能。因此，我们选择合理的优化目标和设计约束，使设计空间探索可行。特别是，如表I所示，对于数据中心推理加速器，我们基于最近的数据中心ML加速器，将芯片面积限制在500mm^2，TDP限制在300W以内。内存子系统配置为32MB软件管理的片上内存，分配给所有核，片外HBM带宽为700GB/s，与Google TPU-v2/v3类似。注意，TPU v2/v3设计用于训练和推理。我们使用NeuroMeter来扫描设计空间，来对每个设计点(X,N,Tx,Ty)优化TOPS，有依赖的硬件参数自动按比例缩放到设计空间参数上，如图6和表I所示。

Before setting the ranges of the implicit hardware parameters in Fig. 6, we explore a larger design space of systolic array centric architectures, including a larger number of TUs per core, multiple TUs sharing VReg read/write ports, and other types of inner-TU interconnection. We prune the design points that exceed the area/power budgets or have extremely low performance. We only take the design points that meet the perf/power/area requirements into the second round for further workload-aware analysis. To make the design space manageable, we finally set the range of TU length (X) from 4 to 256. NeuroMeter automatically sets one VU per core with its lane number the same as the TU array length. NeuroMeter reserves two read ports and one write port in the VReg for each functional unit. The number of TUs in each core (N) determines how many total ports are required for each VReg, where a large N leads to an overhead explosion of VReg. For example, with eight 4x4 TUs per core, the VReg area and power overhead is 12.7% and 24.9% of the core. To avoid such an overhead explosion of VReg, N is capped at 4. The distributed on-chip memory is automatically multi-banked by NeuroMeter to satisfy the timing constraints determined by the target TOPS and clock frequency. The total core count (the product of Tx and Ty) is maximized to achieve the peak TOPS target while under the area and power constraint. For the convenience of evenly partitioning the neural network model, we assume Tx and Ty to be the power-of-2 numbers. To make the overall layout close to square, we assume that Tx is equal to or half of Ty.

在设置图6中隐式硬件参数的范围之前，我们探索以脉动阵列为中心的架构的更大设计空间，包括每个核中更大数量的TUs，多个TUs共享VReg读/写端口，其他类型的inner-TU互联。我们将超过面积/功耗预算的设计点，或性能很低的设计点修剪掉。我们只采用满足性能/功耗/面积要求的设计点，带到第二轮，进行进一步的关于workload的分析。为使设计空间容易管理，我们最终设置TU的长度X范围为4到256。NeuroMeter自动设置每个核一个VU，lane数量与TU阵列长度一样。NeuroMeter对每个功能模块在VReg中保留了2个读端口和1个写端口。每个核中的TUs数量N决定了对每个VReg需要多少端口，其中N值大会导致VReg开销爆炸。比如，每个核有8个4x4 TUs，VReg面积和功耗开销是核的12.7%和24.9%。为避免这样的VReg开销爆炸，N设置为4。片上内存的分布NeuroMeter会自动设置为multi-bank，以满足目标TOPS和时钟频率的时序约束。总计核数量（Tx和Ty的乘积）最大化，以获得峰值TOPS，同时在面积和功耗约束下。为方便的将神经网络模型平均分割，我们假设Tx和Ty是2的幂次方。为使得总体布局接近方形，我们假设Tx等于Ty，或Ty的一半。

2) Machine Learning Models: Our datacenter case study uses three widely adopted CNN models, including ResNet-50 (abbrev. ResNet) [27], Inception-v3 (abbrev. Inception) [54], and NasNet-A-Large (abbrev. NasNet) [58]. Table II summarizes the characteristics of these ML models, including the compute (#MAC Op/frame), the peak transient memory footprint per frame (#Data), and the model size (#Param, quantized into Integer8).

机器学习模型：我们的数据中心案例研究使用广泛采用的CNN模型，包括ResNet-50，Inception-v3，和NasNet-A-Large。表II给出了这些ML模型的特征，包括计算量，峰值瞬时内存占用，和模型大小。

3) Performance Simulation and Efficiency Modeling: We use TF-Sim [9] to simulate the performance of the ML models running on the target accelerators. TF-Sim first takes the computational graph (e.g., tfGraph [7]) of a given ML model and the same architecture configurations previously used as the inputs to NeuroMeter. Then, the simulator generates the performance of the ML model running on the target accelerators and the statistics for architecture components. The component level statistics are fed to NeuroMeter for computing runtime power and energy. The end-to-end performance (e.g., throughput and latency of inference) is used together with NeuroMeter’s output on chip area and (runtime) power to compute energy efficiency and cost efficiency. The cost efficiency (i.e., TOPS/TCO) is approximated as TOPS/mm4/Watt, where power (Watt) is an approximation of operational expenditures (OpEx) and area squared (mm4) is an approximation of capital expenditures (CapEx) because silicon die cost grows roughly as the square of the die area [28].

性能仿真和高效建模：我们使用TF-Sim来仿真在目标加速器上运行的ML模型的性能。TF-Sim将给定ML模型的计算图（如，tfGraph），和之前使用的作为NeuroMeter输入的的相同的架构设置，作为输入。然后，仿真器生成ML模型运行在目标加速器的性能，和架构组成部分的统计。组成部分级的统计结果，送入NeuroMeter，以计算运行时功耗和能耗。端到端的性能（如，推理的吞吐量和延迟）与NeuroMeter的输出一起用在芯片面积和运行时功耗，以计算能耗效率和代价效率。代价效率（如，TOPS/TCO）近似为TOPS/mm4/Watt，其中功耗Watt是运算消耗的近似，面积的平方是资本消耗的近似，因为硅片价格大致随着芯片面积的平方增长。

TF-Sim supports advanced runtime graph scheduling and optimization, following the best practices in modern ML compiler/runtime such as XLA [1]. Especially for wimpy architectures, TF-Sim considers how to reduce the extra overhead of partial sum merging and weight/activation broadcast when a single TU is not large enough to map the whole operation without tiling. Moreover, TF-Sim also supports optimizations to improve parallelism, such as Space-to-Batch [5], Space-to-Depth [6], and double memory buffering. Fig. 7 shows the significant improvement of the simulated performance with the supported software optimizations, especially on small batch sizes. For wimpy designs, the operation is always too large to map on single TU without tiling. The mapping strategy considers how to reduce the extra overhead of partial sum merging and weight/activation broadcast.

TF-Sim支持高级运行时图规划和优化，遵循现代ML编译器/运行时的最佳实践，如XLA。尤其是对于弱架构，TF-Sim在单个TU不是很大，在没有tiling的情况下无法映射整个运算的情况下，考虑怎样降低部分和合并，和权重/激活广播的额外开销。而且，TF-Sim还支持优化来改进并行性，比如Space-to-Batch, Spaceto-Depth, 和双倍内存缓冲。图7展示了在支持的软件优化下，仿真性能的显著改进，尤其是在小批次大小上。对于弱设计，在单个TU没有tiling的情况下，运算一直太大，不能映射。映射策略考虑怎样降低部分和合并，和权重/激活广播的额外开销。

### 3.2. Results: Datacenter Inference Accelerator

## 4. Mini-Case Study on Sparsity Implications on Different ML Accelerator Architechures

## 5. Related Work

CACTI [43] is the first analytical modeling framework for cache and memory arrays. McPAT [39] uses the same analytical modeling methodology and builds up the modeling framework for manycore general-purpose processors. NeuroMeter leverages the same methodology and techniques used in CACTI and McPAT.

CACTI是第一个缓存和内存阵列的解析建模框架。McPAT使用相同的解析建模方法，对众核通用处理器构建建模框架。NeuroMeter使用了与CACTI和McPAT相同的方法学和技术。

Eyeriss [17], Eyeriss-v2 [18], and MAESTRO [35] provide dataflow analysis and modeling framework for ML accelerators. NNest [31] provides a generalized spatial architecture framework for exploring the design space of ASIC-based ML inference accelerators. Scale-Sim [51] provides a cycle-accurate performance simulator for systolic CNN accelerators through on-chip and off-chip memory access traces. Interstellar [56] uses Halide’s algorithm and scheduling primitives [49] to express different ML accelerator architectures. Aladdin [52], Minerva [50], and PolySA [20] provide different frameworks with (semi) HLS-level capabilities. Timeloop [44] and Accelergy [55] together provide an ecosystem to model ML accelerators. Besides providing modeling tools, previous work like NVDLA [8] open-source the RTL codes of typical ML accelerator designs; and this kind of work boosts the modeling ecosystem from another perspective. With simultaneously and analytically modeling power, area, and timing of key ML accelerator micro-architectures, NeuroMeter advances the state-of-the-art and provides foundational support for the modeling ecosystem.

Eyeriss，Eyeriss-v2和MAESTRO对ML加速器给出了数据流分析和建模框架。NNest给出了一个一般化的空域架构框架，探索基于ASIC的ML推理加速器的设计空间。Scale-Sim对脉动CNN加速器通过片上和片外内存访问痕迹，来给出周期精确的性能仿真器。Interstellar使用Halide算法和调度原语，来表达不同的ML加速器架构。Aladdin，Minerva和PolySA给出了不同的框架，有（半）HLS级的能力。Timeloop和Accelergy一起提供了一个生态环境来建模ML加速器。除了给出建模工具，之前的工作如NUDLA将典型ML加速器设计的RTL代码开源；这种工作从另一种角度加速了建模生态环境。NeuroMeter有同时并解析的对关键ML加速器微架构的功耗，面积和时序进行建模的能力，这推进了目前最好的状态，对建模生态给出了基础性的支持。

In the era of artificial intelligence, a plethora of ML accelerator architectures are proposed. It has been an open question of how one can make the design choices among the architectures based on systolic arrays [17] [21] [30], reduction trees [48,57], or SIMD vectors [19] for various scenarios and different workloads (e.g., training v.s. inference, dense v.s. sparse, datacenter v.s. edge, and beyond). NeuroMeter is able to model these popular emerging ML accelerator micro-architectures, which can help foster an even more comprehensive study on the ML accelerator frontier.

在人工智能的时代，提出了很多ML加速器架构。在基于脉动阵列，缩减数或SIMD向量的架构中，对各种场景和不同的workloads（如，训练vs推理，密集vs稀疏，数据中心vs边缘端，等等），如何选择不同的设计选项，这曾是一个开放的问题。NeuroMeter可以建模这些流行的ML加速器微架构，可以帮助促进ML加速器的更加综合的研究。

Brawny v.s. wimpy study [12] has been conducted extensively, with aspects including latency [22], throughput [40], energy efficiency [41], interconnect [33], heterogeneity [32] [53], and workload characteristics [16] in the CPU design space. With the growing ML workloads and the increasing deployment of ML inference accelerators in the datacenter, a similar brawny v.s. wimpy question has been raised in domain specific hardware. Kung et. al [34] have studied the latency of accelerators with different systolic array sizes.

强弱研究，在CPU设计空间中已经很广泛的进行，包括延迟，吞吐量，能耗效率，互联，异质性，和workload的特点。随着ML workloads的增多，ML推理加速器在数据中心中部署的越来越多，在领域专用硬件中也提出了类似的强弱问题。Kung等用不同的脉动阵列大小上研究了加速器的延迟。

## 6. Conclusion

NeuroMeter is an architectural analytical framework for simultaneously modeling power, area, and timing for emerging ML accelerators. It models all major architectural components of emerging ML accelerators, including TU, VU, on-chip Mem, NoC, MemCtrl, host interface, and beyond. Moreover, its analytical model of TU and VU captures the key difference between emerging ML accelerators and the mainstream CPUs. Its analytical modeling methodology generates fast and accurate modeling results without relying on EDA tools. Validations show a reasonable agreement between NeuroMeter and published data for both datacenter-oriented (TPU-v1/v2) and mobile/edge-oriented (Eyeriss) state-of-the-art ML accelerators. NeuroMeter empowers architects with a fast yet accurate exploration of the large and diverse design space of modern manycore ML accelerators. When combined with performance simulations via its flexible and extensible interface, NeuroMeter enables broader architecture study with comprehensive metrics such as TOPS/Watt, TOPS/TCO.

NeuroMeter是一个架构解析框架，可以同时对ML加速的功耗，面积和时序进行建模。可以对ML加速器的所有主要架构组成部分进行建模，包括TU，VU片上存储，NoC，MemCtrl，宿主接口，等等。而且，其TU和VU解析模型包含了ML加速器和主流CPUs的关键差异。其解析建模方法，生成了快速和准确的建模结果，不需要依赖于EDA工具。验证结果表明，NeuroMeter和发表的数据之间吻合的很好，在数据中心向(TPUv1,v2)和移动/边缘向的(Eyeriss)加速器上都得到了这样的结果。NeuroMeter使架构师可以快速准确的探索现代众核ML加速器的大型多样的设计空间。当与性能仿真通过其灵活可扩展的接口结合到一起，NeuroMeter可以对综合度量进行更广泛的架构研究，如TOPS/Watt，TOPS/TCO。

By combining the power, area, and timing results of NeuroMeter with performance simulation, we explore the manycore ML accelerator design, including wimpy and brawny cores. Our study shows that brawny designs with 64x64 systolic arrays are the most performant and efficient for inference tasks in the 28nm datacenter architectural space with a 500mm2 die area budget. Our study also reveals important tradeoffs between performance and efficiency. For datacenter accelerators with low batch inference, a small (∼16%) sacrifice of performance can lead to more than a 2x efficiency improvement (in achieved TOPS/TCO). To showcase NeuroMeter’s capability to model a wide range of accelerator architectures, we also conduct a mini-case study on energy efficiency (TOPS/Watt) implications of sparsity on different ML accelerators. Our results show that despite its relatively low energy efficiency, it is easier for wimpier accelerator architectures to benefit from sparsity processing.

将NeuroMeter的功耗，面积和时序结果，与性能仿真结合到一起，我们探索了众核ML加速器设计，包括强弱核。我们的研究表明，64x64脉动阵列的强设计性能好，在28nm数据中心架构空间中，在500mm2的芯片面积预算下，对推理任务非常高效。我们的研究还解释了性能和效率之间重要的折中。对于批次量小的推理中，数据中心加速器牺牲较小的性能(~16%)可以得到很多的效率提升(TOPS/TCO)。为表明NeuroMeter建模广泛的加速器架构的能力，我们还进行了mini案例研究，稀疏性在不同ML加速器上的能耗效率研究。我们的结果表明，尽管能耗效率相对较低，但更弱的加速器架构更容易从稀疏性处理中受益。