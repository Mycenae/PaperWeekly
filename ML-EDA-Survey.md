# Machine Learning for Electronic Design Automation: A Survey

GUYUE HUANG et. al. @ Tsinghua University

With the down-scaling of CMOS technology, the design complexity of very large-scale integrated (VLSI) is increasing. Although the application of machine learning (ML) techniques in electronic design automation (EDA) can trace its history back to the 90s, the recent breakthrough of ML and the increasing complexity of EDA tasks have aroused more interests in incorporating ML to solve EDA tasks. In this paper, we present a comprehensive review of existing ML for EDA studies, organized following the EDA hierarchy.

éšç€CMOSå°ºå¯¸ç¼©å°æŠ€æœ¯çš„å‘å±•ï¼ŒVLSIè®¾è®¡å¤æ‚åº¦é€æ¸å¢åŠ ã€‚è™½ç„¶MLæŠ€æœ¯åœ¨EDAä¸­çš„åº”ç”¨å¯ä»¥è¿½æº¯å†å²åˆ°90så¹´ä»£ï¼ŒMLæœ€è¿‘çš„çªç ´å’ŒEDAå¢åŠ çš„å¤æ‚åº¦ï¼Œå¼•èµ·äº†å¾ˆå¤šé‡‡ç”¨MLæ¥è§£å†³EDAä»»åŠ¡çš„å…´è¶£ã€‚æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å›é¡¾äº†MLåœ¨EDAä¸­çš„ç ”ç©¶ï¼ŒæŒ‰ç…§EDAå±‚æ¬¡ç»“æ„æ¥è¿›è¡Œç»„ç»‡ã€‚

## 1. Introduction

As one of the most important fields in applied computer/electronic engineering, Electronic Design Automation (EDA) has a long history and is still under heavy development incorporating cutting-edge algorithms and technologies. In recent years, with the development of semiconductor technology, the scale of integrated circuit (IC) has grown exponentially, challenging the scalability and reliability of the circuit design flow. Therefore, EDA algorithms and software are required to be more effective and efficient to deal with extremely large search space with low latency.

ä½œä¸ºåº”ç”¨è®¡ç®—æœº/ç”µå­å·¥ç¨‹çš„ä¸€ä¸ªæœ€é‡è¦é¢†åŸŸï¼ŒEDAæœ‰å¾ˆé•¿çš„å†å²ï¼Œè€Œä¸”ä»ç„¶åœ¨ä½¿ç”¨å°–ç«¯ç®—æ³•å’ŒæŠ€æœ¯è¿›è¡Œå¾ˆå¤šå¼€å‘ã€‚åœ¨æœ€è¿‘å‡ å¹´ä¸­ï¼Œéšç€åŠå¯¼ä½“æŠ€æœ¯çš„å‘å±•ï¼ŒICçš„è§„æ¨¡å‘ˆæŒ‡æ•°å¢é•¿ï¼ŒæŒ‘æˆ˜äº†ç”µè·¯è®¾è®¡æµç¨‹çš„å¯æ‰©å±•æ€§å’Œå¯é æ€§ã€‚å› æ­¤ï¼ŒEDAç®—æ³•å’Œè½¯ä»¶éœ€è¦æ›´åŠ æœ‰æ•ˆï¼Œæ•ˆç‡æ›´é«˜ï¼Œæ¥ç”¨å¾ˆçŸ­çš„æ—¶é—´å¤„ç†æå¤§çš„æœç´¢ç©ºé—´ã€‚

Machine learning (ML) is taking an important role in our lives these days, which has been widely used in many scenarios. ML methods, including traditional and deep learning algorithms, achieve amazing performance in solving classification, detection, and design space exploration problems. Additionally, ML methods show great potential to generate high-quality solutions for many NP-complete (NPC) problems, which are common in the EDA field, while traditional methods lead to huge time and resource consumption to solve these problems. Traditional methods usually solve every problem from the beginning, with a lack of knowledge accumulation. Instead, ML algorithms focus on extracting high-level features or patterns that can be reused in other related or similar situations, avoiding repeated complicated analysis. Therefore, applying machine learning methods is a promising direction to accelerate the solving of EDA problems.

æœºå™¨å­¦ä¹ ä»Šå¤©åœ¨æˆ‘ä»¬çš„ç”Ÿæ´»ä¸­çš„è§’è‰²è¶Šæ¥è¶Šé‡è¦ï¼Œåœ¨å¾ˆå¤šåœºæ™¯ä¸­è¢«å¹¿æ³›ä½¿ç”¨ã€‚MLæ–¹æ³•ï¼ŒåŒ…æ‹¬ä¼ ç»Ÿç®—æ³•å’Œæ·±åº¦å­¦ä¹ ç®—æ³•ï¼Œåœ¨å¤„ç†åˆ†ç±»ï¼Œæ£€æµ‹å’Œè®¾è®¡ç©ºé—´æ¢ç´¢é—®é¢˜ä¸­å±•ç°äº†éå¸¸å¥½çš„æ€§èƒ½ã€‚å¦å¤–ï¼ŒMLæ–¹æ³•åœ¨å¾ˆå¤šNPCé—®é¢˜ä¸­å¾ˆå¯èƒ½ç”Ÿæˆé«˜è´¨é‡çš„è§£ï¼Œè¿™äº›é—®é¢˜åœ¨EDAä¸­å¾ˆå¸¸è§ï¼Œè€Œä¼ ç»Ÿæ–¹æ³•éœ€è¦è€—è´¹å¤§é‡çš„æ—¶é—´å’Œèµ„æºæ¥æ±‚è§£è¿™äº›é—®é¢˜ã€‚ä¼ ç»Ÿæ–¹æ³•é€šå¸¸ä»å¤´è§£å†³æ¯ä¸ªé—®é¢˜ï¼Œç¼ºå°‘çŸ¥è¯†ç§¯ç´¯ã€‚è€ŒMLæ–¹æ³•èšç„¦åœ¨æå–é«˜å±‚æ¬¡ç‰¹å¾æˆ–æ¨¡å¼ï¼Œåœ¨ç›¸å…³æˆ–ç±»ä¼¼çš„æƒ…å†µä¸­å¯ä»¥å¤ç”¨ï¼Œé¿å…äº†é‡å¤çš„å¤æ‚åˆ†æã€‚å› æ­¤ï¼Œåº”ç”¨æœºå™¨å­¦ä¹ æ–¹æ³•æ˜¯åŠ é€ŸEDAé—®é¢˜è§£å†³çš„ä¸€ä¸ªæœ‰å¸Œæœ›çš„æ–¹å‘ã€‚

In recent years, ML for EDA is becoming one of the trending topics, and a lot of studies that use ML to improve EDA methods have been proposed, which cover almost all the stages in the chip design flow, including design space reduction and exploration, logic synthesis, placement, routing, testing, verification, manufacturing, etc. These ML-based methods have demonstrated impressive improvement compared with traditional methods.

æœ€è¿‘å‡ å¹´é‡Œï¼ŒML for EDAå˜å¾—è¶Šæ¥è¶Šæµè¡Œï¼Œæå‡ºäº†å¾ˆå¤šä½¿ç”¨MLæ”¹è¿›EDAçš„ç ”ç©¶ï¼Œè¦†ç›–äº†èŠ¯ç‰‡è®¾è®¡æµç¨‹çš„å‡ ä¹æ‰€æœ‰é˜¶æ®µï¼ŒåŒ…æ‹¬è®¾è®¡ç©ºé—´ç¼©å‡å’Œæ¢ç´¢ï¼Œé€»è¾‘ç»¼åˆï¼Œå¸ƒå±€å¸ƒçº¿ï¼Œæµ‹è¯•ï¼ŒéªŒè¯ï¼Œåˆ¶é€ ç­‰ã€‚ä¸ä¼ ç»Ÿæ–¹æ³•ç›¸æ¯”ï¼Œè¿™äº›åŸºäºMLçš„æ–¹æ³•å±•ç°å‡ºäº†ä»¤äººå°è±¡æ·±åˆ»çš„è¿›æ­¥ã€‚

We observe that most work collected in this survey can be grouped into four types: decision making in traditional methods, performance prediction, black-box optimization, and automated design, ordered by decreasing manual efforts and expert experiences in the design procedure, or an increasing degree of automation. The opportunity of ML in EDA starts from decision making in traditional methods, where an ML model is trained to select among available tool chains, algorithms, or hyper-parameters, to replace empirical choice or brute-force search. ML is also used for performance prediction, where a model is trained from a database of previously implemented designs to predict the quality of new designs, helping engineers to evaluate new designs without the time-consuming synthesis procedure. Even more automated, EDA tools utilized the workflow of black-box optimization, where the entire procedure of design space exploration (DSE) is guided by a predictive ML model and a sampling strategy supported by ML theories. Recent advances in Deep Learning (DL), especially Reinforcement Learning (RL) techniques have stimulated several studies that fully automate some complex design tasks with extremely large design space, where predictors and policies are learned, performed, and adjusted in an online form, showing a promising future of Artificial Intelligence (AI)-assisted automated design.

æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œæœ¬ç»¼è¿°ä¸­æ”¶é›†çš„å¤šæ•°å·¥ä½œå¯ä»¥å½’ç±»ä¸ºå››ç±»ï¼šä¼ ç»Ÿæ–¹æ³•ä¸­çš„å†³ç­–ç®—æ³•ï¼Œæ€§èƒ½é¢„æµ‹ï¼Œé»‘ç›’ä¼˜åŒ–ï¼Œå’Œè‡ªåŠ¨è®¾è®¡ï¼Œè¿™ä¸ªæ’åºæ˜¯æŒ‰ç…§æ‰‹å·¥å·¥ä½œå’Œä¸“å®¶ç»éªŒåœ¨è®¾è®¡è¿‡ç¨‹ä¸­çš„é™åºæ’åˆ—çš„ï¼Œæˆ–è‡ªåŠ¨åŒ–ç¨‹åº¦çš„å¢åŠ ã€‚MLåœ¨EDAä¸­çš„æœºä¼šå¼€å§‹äºä¼ ç»Ÿæ–¹æ³•ä¸­çš„å†³ç­–ç®—æ³•ï¼Œè®­ç»ƒä¸€ä¸ªæ¨¡å‹åœ¨å¯ç”¨çš„å·¥å…·é“¾ã€ç®—æ³•ï¼Œæˆ–è¶…å‚æ•°ä¸­é€‰æ‹©ï¼Œä»¥æ›¿æ¢ç»éªŒé€‰æ‹©ï¼Œæˆ–æš´åŠ›æœç´¢ã€‚MLä¹Ÿç”¨äºæ€§èƒ½é¢„æµ‹ï¼Œä»ä¹‹å‰å®ç°çš„è®¾è®¡çš„æ•°æ®åº“ä¸­ï¼Œè®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œæ¥é¢„æµ‹æ–°è®¾è®¡çš„è´¨é‡ï¼Œå¸®åŠ©å·¥ç¨‹å¸ˆæ¥è¯„ä¼°æ–°çš„è®¾è®¡ï¼Œä¸éœ€è¦éå¸¸è€—æ—¶çš„ç»¼åˆè¿‡ç¨‹ã€‚è‡ªåŠ¨åŒ–ç¨‹åº¦æ›´é«˜çš„æ˜¯ï¼ŒEDAå·¥å…·åˆ©ç”¨é»‘ç›’ä¼˜åŒ–çš„å·¥ä½œæµï¼Œå…¶ä¸­è®¾è®¡ç©ºé—´æ¢ç´¢çš„æ•´ä¸ªè¿‡ç¨‹ç”±é¢„æµ‹æ€§MLæ¨¡å‹ï¼Œå’Œé‡‡æ ·ç­–ç•¥æ¥å¼•å¯¼ï¼Œè¿™ä¸ªç­–ç•¥ç”±MLç†è®ºæ”¯æ’‘ã€‚æœ€è¿‘åœ¨DLæ–¹é¢çš„è¿›å±•ï¼Œå°¤å…¶æ˜¯RLï¼Œä»¿çœŸäº†å‡ ä¸ªç ”ç©¶ï¼Œä½¿å‡ ä¸ªå¤æ‚çš„è®¾è®¡ä»»åŠ¡å®Œå…¨è‡ªåŠ¨åŒ–äº†ï¼Œæœ‰ç€éå¸¸å¤§çš„è®¾è®¡ç©ºé—´ï¼Œå…¶ä¸­å­¦ä¹ å¹¶ä½¿ç”¨äº†é¢„æµ‹å™¨å’Œç­–ç•¥ï¼Œå¹¶ä»¥åœ¨çº¿çš„å½¢å¼è¿›è¡Œäº†è°ƒæ•´ï¼Œå±•ç°äº†AIè¾…åŠ©çš„è‡ªåŠ¨è®¾è®¡çš„å…‰æ˜æœªæ¥ã€‚

This survey gives a comprehensive review of some recent important studies applying ML to solve some EDA important problems. The review of these studies is organized according to their corresponding stages in the EDA flow. Although the study on ML for EDA can trace back to the last century, most of the works included in this survey are in recent five years. The rest of this survey is organized as follows. In Section 2, we introduce the background of both EDA and ML. From Section 3 to Section 5, we introduce the studies that focus on different stages of the EDA flow, i.e., high-level synthesis, logic synthesis & physical design (placement and routing), and mask synthesis, respectively. In Section 6, analog design methods with ML are reviewed. ML-powered testing and verification methods are discussed in Section 7. Then, in Section 8, other highly-related studies are discussed, including ML for SAT solver and the acceleration of EDA with deep learning engine. The discussion of various studies from the ML perspective is given in Section 9, which is complementary to the main organization of this paper. Finally, Section 10 concludes the existing ML methods for EDA and highlights future trends in this field.

æœ¬æ–‡å¯¹ä¸€äº›æœ€è¿‘çš„å¤æ‚ç ”ç©¶ï¼Œä½¿ç”¨MLæ¥è§£å†³ä¸€äº›EDAé‡è¦é—®é¢˜ï¼Œç»™å‡ºäº†ä¸€ä¸ªç»¼åˆå›é¡¾ã€‚è¿™äº›ç ”ç©¶çš„å›é¡¾æ ¹æ®å…¶åœ¨EDAæµä¸­çš„é˜¶æ®µè¿›è¡Œç»„ç»‡ã€‚è™½ç„¶MLåœ¨EDAä¸­çš„åº”ç”¨å¯ä»¥è¿½æº¯åˆ°ä¸Šä¸ªä¸–çºªï¼Œæœ¬ç»¼è¿°åŒ…å«çš„å¤šæ•°å·¥ä½œéƒ½æ˜¯æœ€è¿‘5å¹´ä»¥å†…çš„ã€‚æœ¬ç»¼è¿°ç»„ç»‡å¦‚ä¸‹ï¼šç¬¬2éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä»‹ç»äº†EDAå’ŒMLçš„èƒŒæ™¯ã€‚ä»ç¬¬3åˆ°ç¬¬5éƒ¨åˆ†ï¼Œæˆ‘ä»¬ä»‹ç»çš„å·¥ä½œèšç„¦åœ¨EDAçš„ä¸åŒé˜¶æ®µï¼Œå³åˆ†åˆ«æ˜¯ï¼Œé«˜å±‚æ¬¡ç»¼åˆï¼Œé€»è¾‘ç»¼åˆ&ç‰©ç†è®¾è®¡ï¼ˆå¸ƒå±€å¸ƒçº¿ï¼‰ï¼Œæ©è†œç»¼åˆã€‚åœ¨ç¬¬6éƒ¨åˆ†ï¼Œå›é¡¾äº†ç”¨MLçš„æ¨¡æ‹Ÿè®¾è®¡æ–¹æ³•ã€‚ç¬¬7éƒ¨åˆ†è®¨è®ºäº†MLèµ‹èƒ½çš„æµ‹è¯•å’ŒéªŒè¯æ–¹æ³•ã€‚åœ¨ç¬¬8éƒ¨åˆ†ï¼Œè®¨è®ºäº†å…¶ä»–é«˜åº¦ç›¸å…³çš„ç ”ç©¶ï¼ŒåŒ…æ‹¬ç”¨äºSATæ±‚è§£å™¨çš„MLï¼Œå’Œç”¨æ·±åº¦å­¦ä¹ å¼•æ“åŠ é€Ÿçš„EDAã€‚ç¬¬9éƒ¨åˆ†æ˜¯ä»MLæ–¹é¢çš„å„ç§ç ”ç©¶çš„è®¨è®ºï¼Œä¸æœ¬æ–‡çš„ä¸»è¦ç»„ç»‡å½¢å¼æ˜¯äº’è¡¥çš„ã€‚æœ€åï¼Œç¬¬10éƒ¨åˆ†æ€»ç»“äº†ç°æœ‰çš„ç”¨äºEDAçš„MLæ–¹æ³•ï¼Œå¼ºè°ƒäº†è¿™ä¸ªé¢†åŸŸä¸­çš„æœªæ¥è¶‹åŠ¿ã€‚

## 2. Background

### 2.1 Electronic Design Automation

Electronic design automation is one of the most important fields in electronic engineering. In the past few decades, it has been witnessed that the flow of chip design became more and more standardized and complicated. A modern chip design flow is shown in Figure 1.

EDAæ˜¯ç”µå­å·¥ç¨‹æœ€é‡è¦çš„é¢†åŸŸä¹‹ä¸€ã€‚åœ¨è¿‡å»å‡ åå¹´ï¼Œå·²ç»è§‚å¯Ÿåˆ°ï¼ŒèŠ¯ç‰‡è®¾è®¡çš„æµç¨‹å·²ç»å˜å¾—è¶Šæ¥è¶Šæ ‡å‡†åŒ–å’Œå¤æ‚ã€‚å›¾1å±•ç¤ºäº†ä¸€ä¸ªç°ä»£èŠ¯ç‰‡è®¾è®¡çš„æµç¨‹ã€‚

High-level synthesis (HLS) provides automatic conversion from C/C++/SystemC-based specifications to hardware description languages (HDL). HLS makes hardware design much more convenient by allowing the designer to use high-level descriptions for a hardware system. However, when facing a large-scale system, HLS often takes a long time to finish the synthesis. Consequently, efficient design space exploration (DSE) strategy is crucial in HLS [74, 95, 107, 112, 180].

é«˜å±‚æ¬¡ç»¼åˆ(HLS)å¯ä»¥ä»åŸºäºC/C++/SystemCçš„äº§å“è§„æ ¼è‡ªåŠ¨è½¬æ¢æˆHDLã€‚HLSä½¿ç¡¬ä»¶è®¾è®¡æ›´åŠ æ–¹ä¾¿äº†ï¼Œä½¿ç¡¬ä»¶è®¾è®¡è€…å¯ä»¥å¯¹ä¸€ä¸ªç¡¬ä»¶ç³»ç»Ÿä½¿ç”¨é«˜å±‚æ¬¡æè¿°ã€‚ä½†æ˜¯ï¼Œå½“é¢å¯¹ä¸€ä¸ªå¤§è§„æ¨¡ç³»ç»Ÿï¼ŒHLSé€šå¸¸è€—æ—¶å¾ˆé•¿æ‰èƒ½å®Œæˆç»¼åˆã€‚ç»“æœæ˜¯ï¼Œé«˜æ•ˆçš„DSEç­–ç•¥åœ¨HLSä¸­éå¸¸å…³é”®ã€‚

Logic synthesis converts the behavioral level description to the gate level description, which is one of the most important problems in EDA. Logic synthesis implements the specific logic functions by generating a combination of gates selected in a given cell library, and optimizes the design for different optimization goals. Logic synthesis is a complicated process that usually cannot be solved optimally, and hence the heuristic algorithms are widely used in this stage, which include lots of ML methods [48, 56, 115, 167].

é€»è¾‘ç»¼åˆå°†è¡Œä¸ºçº§çš„æè¿°ï¼Œè½¬æ¢æˆé—¨çº§çš„æè¿°ï¼Œè¿™æ˜¯EDAä¸­æœ€é‡è¦çš„ä¸€ä¸ªé—®é¢˜ã€‚é€»è¾‘ç»¼åˆé€šè¿‡ç”Ÿæˆåœ¨ç»™å®šå•å…ƒåº“ä¸­çš„é—¨çš„ç»„åˆï¼Œæ¥å®ç°ç‰¹å®šé€»è¾‘å‡½æ•°ï¼Œå¹¶å¯¹ä¸åŒçš„ä¼˜åŒ–ç›®æ ‡ï¼Œæ¥ä¼˜åŒ–è®¾è®¡ã€‚é€»è¾‘ç»¼åˆæ˜¯ä¸€ä¸ªå¤æ‚çš„è¿‡ç¨‹ï¼Œé€šå¸¸ä¸èƒ½å¾—åˆ°æœ€ä¼˜çš„è§£å†³æ–¹æ³•ï¼Œå› æ­¤å¯å‘å¼ç®—æ³•åœ¨è¿™ä¸ªé˜¶æ®µå¾—åˆ°äº†å¹¿æ³›çš„åº”ç”¨ï¼Œè¿™åŒ…æ‹¬å¾ˆå¤šMLæ–¹æ³•ã€‚

Based on the netlist obtained from synthesis, floorplanning and placement aim to assign the netlist components to specific locations on the chip layout. Better placement assignment implies the potential of better chip area utilization, timing performance, and routability. Routing is one of the essential steps in very large-scale integrated (VLSI) physical design flow based on the placement assignment. Routing assigns the wires to connect the components on the chip. At the same time, routing needs to satisfy the requirements of timing performance and total wire-length without violating the design rules. The placement and routing are strongly coupled. Thus it is crucial to consider the routing performance even in the placement stage, and many ML-based routing-aware methods are proposed to improve the performance of physical design [6, 27, 89, 106, 150, 154].

åŸºäºä»ç»¼åˆä¸­å¾—åˆ°çš„ç½‘è¡¨ï¼Œå¹³é¢è§„åˆ’å’Œå¸ƒå±€çš„ç›®æ ‡æ˜¯ï¼ŒæŒ‡å®šç½‘è¡¨çš„ç»„æˆéƒ¨åˆ†åˆ°èŠ¯ç‰‡å¸ƒå±€ä¸­ç‰¹å®šçš„ä½ç½®ã€‚æ›´å¥½çš„å¸ƒå±€æ„å‘³ç€å¯¹èŠ¯ç‰‡é¢ç§¯å¯èƒ½åˆ©ç”¨çš„æ›´å¥½ï¼Œæ—¶åºæ€§èƒ½å’Œå¸ƒçº¿æ€§æ›´å¥½ã€‚å¸ƒçº¿æ˜¯VLSIç‰©ç†è®¾è®¡æµä¸­ä¸€ä¸ªåŸºç¡€çš„æ­¥éª¤ï¼Œæ˜¯åŸºäºå¸ƒå±€æŒ‡å®šç»“æœçš„ã€‚å¸ƒçº¿æŒ‡å®šä¸€äº›çº¿ï¼Œåœ¨èŠ¯ç‰‡ä¸­è¿æ¥è¿™äº›ç»„æˆéƒ¨åˆ†ã€‚åŒæ—¶ï¼Œå¸ƒçº¿éœ€è¦æ»¡è¶³æ—¶åºæ€§èƒ½å’Œæ€»è®¡çº¿é•¿çš„è¦æ±‚ï¼ŒåŒæ—¶ä¸èƒ½è¿åè®¾è®¡è§„åˆ™ã€‚å¸ƒå±€å’Œå¸ƒçº¿æ˜¯å¼ºè€¦åˆçš„ã€‚å› æ­¤ï¼Œåœ¨å¸ƒå±€é˜¶æ®µå°±è€ƒè™‘å¸ƒçº¿çš„æ€§èƒ½ï¼Œè¿™éå¸¸å…³é”®ï¼Œå› æ­¤æå‡ºäº†å¾ˆå¤šåŸºäºMLçš„æ„è¯†åˆ°å¸ƒçº¿çš„æ–¹æ³•æ¥æ”¹è¿›ç‰©ç†è®¾è®¡çš„æ€§èƒ½ã€‚

Fabrication is a complicated process containing multiple steps, which has a high cost in terms of time and resources. Mask synthesis is one of the main steps in the fabrication process, where lithography simulation is leveraged to reduce the probability of fabrication failure. Mask optimization and lithography simulation are still challenging problems. Recently, various ML-based methods are applied in the lithography simulation and mask synthesis [20, 43, 159, 163, 165].

åˆ¶é€ æ˜¯ä¸€ä¸ªå¤æ‚çš„è¿‡ç¨‹ï¼ŒåŒ…å«å¤šä¸ªæ­¥éª¤ï¼Œæ—¶é—´å’Œèµ„æºçš„æ¶ˆè€—éƒ½å¾ˆå¤§ã€‚æ©è†œç»¼åˆæ˜¯åˆ¶é€ è¿‡ç¨‹ä¸­çš„ä¸€ä¸ªä¸»è¦æ­¥éª¤ï¼Œå…¶ä¸­åˆ©ç”¨å…‰åˆ»ä»¿çœŸæ¥é™ä½åˆ¶é€ å¤±è´¥çš„æ¦‚ç‡ã€‚æ©è†œä¼˜åŒ–å’Œå…‰åˆ»ä»¿çœŸä»ç„¶æ˜¯æœ‰æŒ‘æˆ˜çš„é—®é¢˜ã€‚æœ€è¿‘ï¼Œå„ç§åŸºäºMLçš„æ–¹æ³•åº”ç”¨åˆ°äº†å…‰åˆ»ä»¿çœŸå’Œæ©è†œç»¼åˆä¸­ã€‚

To ensure the correctness of a design, we need to perform design verification before manufacturing. In general, verification is conducted after each stage of the EDA flow, and the test set design is one of the major problems. Traditional random or automated test set generation methods are far away from optimal, therefore, there exist many studies that apply ML methods to optimize test set generation for verification [24, 33, 38, 47, 49, 57, 69, 135, 145, 146].

ä¸ºç¡®ä¿ä¸€ä¸ªè®¾è®¡çš„æ­£ç¡®æ€§ï¼Œæˆ‘ä»¬éœ€è¦åœ¨åˆ¶é€ ä¹‹å‰è¿›è¡Œè®¾è®¡éªŒè¯ã€‚æ€»ä½“ä¸Šï¼Œåœ¨EDAæµçš„æ¯ä¸ªé˜¶æ®µï¼Œéƒ½è¿›è¡ŒéªŒè¯ï¼Œæµ‹è¯•é›†çš„è®¾è®¡æ˜¯ä¸€ä¸ªä¸»è¦çš„é—®é¢˜ã€‚ä¼ ç»Ÿçš„éšæœºæˆ–è‡ªåŠ¨æµ‹è¯•é›†ç”Ÿæˆæ–¹æ³•è¿œä¸æ˜¯æœ€ä¼˜çš„ï¼Œå› æ­¤ï¼Œæœ‰å¾ˆå¤šç ”ç©¶åº”ç”¨MLæ–¹æ³•æ¥ä¼˜åŒ–æµ‹è¯•é›†ç”Ÿæˆï¼Œä»¥ç”¨äºéªŒè¯ã€‚

After the chip design flow is finished, manufacturing testing needs to be carried out. The chips need to go through various tests to verify their functionality and reliability. The coverage and the efficiency are two main optimization goals of the testing stage. Generally speaking, a large test set (i.e., a large number of test points) leads to higher coverage at the cost of high resource consumption. To address the high cost of the testing process, studies have focused on applying ML techniques for test set optimization [100, 120, 140, 141] and test complexity reduction [5, 34, 142].

åœ¨èŠ¯ç‰‡è®¾è®¡æµå®Œæˆåï¼Œéœ€è¦è¿›è¡Œåˆ¶é€ æµ‹è¯•ã€‚èŠ¯ç‰‡éœ€è¦ç»è¿‡å„ç§æµ‹è¯•ï¼ŒéªŒè¯å…¶åŠŸèƒ½å’Œå¯é æ€§ã€‚æµ‹è¯•é˜¶æ®µçš„ä¸¤ä¸ªä¸»è¦ä¼˜åŒ–ç›®æ ‡ï¼Œæ˜¯è¦†ç›–å’Œæ•ˆç‡ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå¤§çš„æµ‹è¯•é›†ï¼ˆå³ï¼Œå¤§é‡æµ‹è¯•ç‚¹ï¼‰ï¼Œä¼šä»¥å¾ˆé«˜çš„èµ„æºæ¶ˆè€—çš„ä»£ä»·ï¼Œå¸¦æ¥æ›´é«˜çš„è¦†ç›–ã€‚ä¸ºè§£å†³æµ‹è¯•è¿‡ç¨‹ä¸­çš„é«˜ä»£ä»·é—®é¢˜ï¼Œæœ‰ä¸€äº›ç ”ç©¶èšç„¦åœ¨å°†MLæŠ€æœ¯åº”ç”¨åˆ°æµ‹è¯•é›†ä¼˜åŒ–ä¸­ï¼Œå’Œæµ‹è¯•å¤æ‚åº¦é™ä½ä¸­ã€‚

Thanks to decades of efforts from both academia and industry, the chip design flow is well-developed. However, with the huge increase in the scale of integrated circuits, more efficient and effective methods need to be incorporated to reduce the design cost. Recent advancements in machine learning have provided a far-reaching data-driven perspective for problem-solving. In this survey, we review recent learning-based approaches for each stage in the EDA flow and also discuss the ML for EDA studies from the machine learning perspective.

å­¦æœ¯ç•Œå’Œå·¥ä¸šç•Œç»è¿‡å‡ åå¹´çš„åŠªåŠ›ï¼Œä½¿èŠ¯ç‰‡è®¾è®¡æµç¨‹å¾—åˆ°äº†å……åˆ†å‘å±•ã€‚ä½†æ˜¯ï¼Œéšç€ICçš„è§„æ¨¡çš„å·¨é‡å¢é•¿ï¼Œéœ€è¦æ›´æœ‰æ•ˆæ›´é«˜æ•ˆçš„æ–¹æ³•æ¥é™ä½è®¾è®¡ä»£ä»·ã€‚åœ¨æœºå™¨å­¦ä¹ ä¸­çš„æœ€æ–°è¿›å±•ï¼Œæä¾›äº†ä¸€ä¸ªå½±å“æ·±è¿œçš„ï¼Œæ•°æ®é©±åŠ¨çš„è§£å†³é—®é¢˜çš„è§’åº¦ã€‚æœ¬ç»¼è¿°ä¸­ï¼Œæˆ‘ä»¬å›é¡¾äº†æœ€è¿‘çš„åŸºäºå­¦ä¹ çš„æ–¹æ³•ï¼Œæ¥åœ¨EDAçš„æ¯ä¸ªé˜¶æ®µæ¥è§£å†³é—®é¢˜ï¼Œè¿˜ä»æœºå™¨å­¦ä¹ çš„è§’åº¦è®¨è®ºäº†ç”¨MLè§£å†³EDAé—®é¢˜çš„ç ”ç©¶ã€‚

### 2.2 Machine Learning

Machine learning is a class of algorithms that automatically extract information from datasets or prior knowledge. Such a data-driven approach is a supplement to analytical models that are widely used in the EDA domain. In general, ML-based solutions can be categorized according to their learning paradigms: supervised learning, unsupervised learning, active learning, and reinforcement learning. The difference between supervised and unsupervised learning is whether or not the input data is labeled. With supervised or unsupervised learning, ML models are trained on static data sets offline and then deployed for online inputs without refinement. With active learning, ML models subjectively choose samples from input space to obtain ground truth and refine themselves during the searching process. With reinforcement learning, ML models interact with the environment by taking actions and getting rewards, with the goal of maximizing the total reward. These paradigms all have been shown to be applied to the EDA problems.

æœºå™¨å­¦ä¹ æ˜¯ä¸€ç±»ç®—æ³•ï¼Œå¯ä»¥ä»æ•°æ®é›†æˆ–å…ˆéªŒçŸ¥è¯†ä¸­è‡ªåŠ¨æå–ä¿¡æ¯ã€‚è¿™æ ·ä¸€ç§æ•°æ®é©±åŠ¨çš„æ–¹æ³•æ˜¯è§£ææ¨¡å‹çš„è¡¥å……ï¼Œåœ¨EDAé¢†åŸŸï¼Œè§£ææ¨¡å‹å¾—åˆ°äº†å¹¿æ³›åº”ç”¨ã€‚æ€»ä½“ä¸Šï¼ŒåŸºäºMLçš„æ–¹æ³•å¯ä»¥æ ¹æ®å…¶å­¦ä¹ èŒƒå¼æ¥å½’ç±»ï¼šç›‘ç£å­¦ä¹ ï¼Œæ— ç›‘ç£å­¦ä¹ ï¼Œä¸»åŠ¨å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚ç›‘ç£å­¦ä¹ å’Œæ— ç›‘ç£å­¦ä¹ çš„å·®å¼‚åœ¨äºï¼Œè¾“å…¥æ•°æ®æ˜¯å¦å¸¦æœ‰æ ‡æ³¨ã€‚ç”¨ç›‘ç£å­¦ä¹ æˆ–æ— ç›‘ç£å­¦ä¹ ï¼ŒMLæ¨¡å‹åœ¨é™æ€æ•°æ®é›†ä¸Šè¿›è¡Œç¦»çº¿è®­ç»ƒï¼Œç„¶åéƒ¨ç½²åˆ°åœ¨çº¿è¾“å…¥ä¸­ï¼Œä¸éœ€è¦æç‚¼ä¼˜åŒ–ã€‚åœ¨ä¸»åŠ¨å­¦ä¹ ä¸­ï¼ŒMLæ¨¡å‹ä»è¾“å…¥ç©ºé—´ä¸­ä¸»è§‚çš„é€‰æ‹©æ ·æœ¬ï¼Œå¾—åˆ°çœŸå€¼ï¼Œåœ¨æœç´¢è¿‡ç¨‹ä¸­è¿›è¡Œæç‚¼ä¼˜åŒ–ã€‚åœ¨å¼ºåŒ–å­¦ä¹ ä¸­ï¼ŒMLæ¨¡å‹ä¸ç¯å¢ƒäº’åŠ¨ï¼Œé‡‡å–è¡ŒåŠ¨ï¼Œå¾—åˆ°å›æŠ¥ï¼Œç›®æ ‡æ˜¯æœ€å¤§åŒ–æ€»è®¡å›æŠ¥ã€‚è¿™äº›èŒƒå¼éƒ½åº”ç”¨åˆ°äº†EDAé—®é¢˜ä¸­ã€‚

As for the model construction, conventional machine learning models have been extensively studied for the EDA problems, especially for physical design [66, 178]. Linear regression, random forest (RF) [91] and artificial neural networks (ANN) [55] are classical regression models. Support vector machine (SVM) [12] is a powerful classification algorithm especially suitable for tasks with a small size of training set. Other common classification models include K-Nearest-Neighbor (KNN) algorithm [39] and RF. These models can be combined with ensemble or boosting techniques to build more expressive models. For example, XGBoost [23] is a gradient boosting framework frequently used in the EDA problems.

è‡³äºæ¨¡å‹æ„å»ºï¼Œä¼ ç»Ÿæœºå™¨å­¦ä¹ æ¨¡å‹è§£å†³EDAé—®é¢˜å·²ç»å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼Œå°¤å…¶æ˜¯å¯¹äºç‰©ç†è®¾è®¡ã€‚çº¿æ€§å›å½’ï¼Œéšæœºæ£®æ—ï¼Œå’Œäººå·¥ç¥ç»ç½‘ç»œæ˜¯ç»å…¸çš„å›å½’æ¨¡å‹ã€‚SVMæ˜¯ä¸€ç§å¾ˆå¼ºçš„åˆ†ç±»ç®—æ³•ï¼Œå°¤å…¶é€‚ç”¨äºè®­ç»ƒé›†è¾ƒå°çš„ä»»åŠ¡ã€‚å…¶ä»–å¸¸è§çš„åˆ†ç±»æ¨¡å‹åŒ…æ‹¬KNNç®—æ³•å’Œéšæœºæ£®æ—ã€‚è¿™äº›æ¨¡å‹å¯ä»¥ç”¨é›†æˆæˆ–boostingæŠ€æœ¯ç»“åˆåˆ°ä¸€èµ·ï¼Œæ„å»ºæ›´æœ‰è¡¨è¾¾åŠ›çš„æ¨¡å‹ã€‚æ¯”å¦‚ï¼ŒXGBoostæ˜¯ä¸€ä¸ªæ¢¯åº¦boostingæ¡†æ¶ï¼Œåœ¨EDAé—®é¢˜ä¸­å¾—åˆ°äº†é¢‘ç¹çš„åº”ç”¨ã€‚

Thanks to large public datasets, algorithm breakthrough, and improvements in computation platforms, there have been efforts of applying deep learning (DL) for EDA. In particular, popular models in recent EDA studies include convolutional neural network (CNN) [37, 111], recurrent neural networks (RNN) [83, 148], generative adversarial network (GAN) [165], deep reinforcement learning (DRL) [113, 147] and graph neural networks (GNN) [147, 168]. CNN models are composed of convolutional layers and other basic blocks such as non-linear activation functions and down-sample pooling functions. While CNN is suitable for feature extraction on grid structure data like 2-D image, RNN is good at processing sequential data such as text or audio. GNN is proposed for data organized as graphs. GAN trains jointly a generative network and a discriminative network which compete against each other to eventually generate high quality fake samples. DRL is a class of algorithms that incorporated deep learning into the reinforcement learning paradigm, where an agent learns a strategy from the rewards acquired with previous actions to determine the next action. DRL has achieved great success in complicated tasks with large decision space (e.g., Go game [138]).

å¤šäºäº†å¤§å‹å…¬å…±æ•°æ®é›†ï¼Œç®—æ³•çªç ´ï¼Œå’Œè®¡ç®—å¹³å°çš„æ”¹è¿›ï¼Œå°†æ·±åº¦å­¦ä¹ ç”¨äºEDAä¹Ÿæœ‰å¾ˆå¤šå·¥ä½œã€‚ç‰¹åˆ«æ˜¯ï¼Œåœ¨æœ€è¿‘çš„EDAç ”ç©¶ä¸­æµè¡Œçš„æ¨¡å‹åŒ…æ‹¬ï¼ŒCNNï¼ŒRNNï¼ŒGANï¼ŒDRLå’ŒGNNã€‚CNNæ¨¡å‹ç”±å·ç§¯å±‚å’Œå…¶ä»–åŸºç¡€æ¨¡å—å¦‚éçº¿æ€§æ¿€æ´»å‡½æ•°å’Œä¸‹é‡‡æ ·æ± åŒ–å‡½æ•°ç»„æˆã€‚CNNé€‚ç”¨äºç½‘æ ¼ç»“æ„æ•°æ®ä¸­çš„ç‰¹å¾æå–ï¼Œå¦‚2Då›¾åƒï¼ŒRNNæ“…é•¿å¤„ç†åºåˆ—æ•°æ®ï¼Œå¦‚æ–‡æœ¬æˆ–è¯­éŸ³ã€‚GNNæ˜¯ç”¨äºç»„ç»‡æˆå›¾çš„æ•°æ®ã€‚GANè”åˆè®­ç»ƒä¸€ä¸ªç”Ÿæˆç½‘ç»œå’ŒåŒºåˆ†ç½‘ç»œï¼Œäº’ç›¸ç«äº‰ï¼Œæœ€ç»ˆç”Ÿæˆé«˜è´¨é‡å‡æ ·æœ¬ã€‚DRLè¿™ç±»ç®—æ³•å°†æ·±åº¦å­¦ä¹ ç»“åˆåˆ°å¼ºåŒ–å­¦ä¹ çš„èŒƒå¼ï¼Œå…¶ä¸­ä¸€ä¸ªagentä»å›æŠ¥ä¸­å­¦ä¹ ä¸€ä¸ªç­–ç•¥ï¼Œå›æŠ¥æ˜¯ä¹‹å‰çš„è¡Œä¸ºè·å¾—çš„ï¼Œç¡®å®šä¸‹ä¸€ä¸ªè¡Œä¸ºã€‚DRLå·²ç»åœ¨æœ‰å¤§é‡å†³ç­–ç©ºé—´çš„å¤æ‚ä»»åŠ¡ä¸­è·å¾—äº†æå¤§çš„æˆåŠŸï¼ˆå¦‚ï¼ŒGo gameï¼‰ã€‚

## 3. High Level Synthesis

High-level synthesis (HLS) tools provide automatic conversion from C/C++/SystemC-based specification to hardware description languages like Verilog or VHDL. HLS tools developed in industry and academia [1, 2, 15] have greatly improved productivity in customized hardware design. High-quality HLS designs require appropriate pragmas in the high-level source code related to parallelism, scheduling and resource usage, and careful choices of synthesis configurations in post-Register-Transfer-Level (RTL) stage. Tuning these pragmas and configurations is a non-trivial task, and the long synthesis time for each design (hours from the source code to the final bitstream) prohibits exhaustive DSE.

é«˜å±‚æ¬¡ç»¼åˆ(HLS)å·¥å…·å¯ä»¥å°†åŸºäºC/C++/SystemCçš„æŒ‡æ ‡è‡ªåŠ¨è½¬æ¢åˆ°ç¡¬ä»¶æè¿°è¯­è¨€ï¼Œå¦‚Verilogæˆ–VHDLã€‚å·¥ä¸šç•Œå’Œå­¦æœ¯ç•Œå¼€å‘çš„HLSå·¥å…·åœ¨å®šåˆ¶çš„ç¡¬ä»¶è®¾è®¡ä¸­æå¤§çš„æ”¹è¿›äº†ç”Ÿäº§åŠ›ã€‚é«˜è´¨é‡HLSè®¾è®¡åœ¨é«˜å±‚æºä»£ç ä¸­éœ€è¦åˆé€‚çš„ç¼–è¯‘æŒ‡ç¤ºï¼Œä¸å¹¶è¡Œæ€§ã€è°ƒåº¦å’Œèµ„æºä½¿ç”¨ï¼Œåœ¨åRTLé˜¶æ®µä»”ç»†çš„é€‰æ‹©ç»¼åˆé…ç½®ã€‚è°ƒèŠ‚è¿™äº›ç¼–è¯‘æŒ‡ç¤ºå’Œé…ç½®ï¼Œéœ€è¦ä¸å°‘å·¥ä½œé‡ï¼Œå¯¹äºæ¯ä¸ªè®¾è®¡éƒ½éœ€è¦å¾ˆé•¿çš„ç»¼åˆæ—¶é—´ï¼ˆä»æºä»£ç åˆ°æœ€åçš„æ¯”ç‰¹æµï¼Œè¦å‡ ä¸ªå°æ—¶ï¼‰ï¼Œè¿™å°±å¦¨ç¢ä½¿ç”¨ç©·ä¸¾å¼DSEã€‚

ML techniques have been applied to improve HLS tools from the following three aspects: fast and accurate result estimation [30, 37, 108, 109, 143, 164, 172], refining conventional DSE algorithms [74, 107, 149], and reforming DSE as an active-learning problem [94, 95, 112, 180]. In addition to achieving good results on individual problems, previous studies have also introduced new generalizable techniques about feature engineering [30, 108, 109, 164, 172], selection and customization of ML models [143], and design space sampling and searching strategies [95, 112, 180].

MLæŠ€æœ¯å·²ç»åº”ç”¨ä¸æ”¹è¿›HLSå·¥å…·ï¼Œä¸»è¦ä»ä¸‹é¢è¿™ä¸‰ä¸ªæ–¹é¢ï¼šå¿«é€Ÿå‡†ç¡®çš„ç»“æœä¼°è®¡ï¼Œæ”¹è¿›ä¼ ç»Ÿçš„DSEç®—æ³•ï¼Œå°†DSEé‡æ–°è¡¨è¿°ä¸ºä¸€ä¸ªä¸»åŠ¨å­¦ä¹ é—®é¢˜ã€‚é™¤äº†è¦åœ¨å•ä¸ªé—®é¢˜ä¸Šè·å¾—å¥½çš„ç»“æœï¼Œä¹‹å‰çš„ç ”ç©¶è¿˜å¼•å…¥äº†æ–°çš„å¯æ³›åŒ–çš„å…³äºç‰¹å¾å·¥ç¨‹çš„æŠ€æœ¯ï¼Œé€‰æ‹©å’Œå®šåˆ¶MLæ¨¡å‹ï¼Œè®¾è®¡ç©ºé—´é‡‡æ ·å’Œæœç´¢æŠ€æœ¯ã€‚

This section is organized as follows. Section 3.1 introduces recent studies on employing ML for result estimation, often in a static way. Section 3.2 introduces recent studies on adopting ML in DSE workflow, either to improve conventional methods or in the form of active learning.

æœ¬èŠ‚ç»„ç»‡å¦‚ä¸‹ã€‚3.1èŠ‚ä»‹ç»äº†æœ€è¿‘åœ¨é‡‡ç”¨MLåœ¨ç»“æœä¼°è®¡ä¸Šçš„ç ”ç©¶ï¼Œé€šå¸¸æ˜¯ä¸€ç§é™æ€çš„æ–¹å¼ã€‚3.2èŠ‚ä»‹ç»äº†é‡‡ç”¨MLè¿›è¡ŒDSEå·¥ä½œæµçš„ç ”ç©¶ï¼Œè¦ä¹ˆæ”¹è¿›ä¼ ç»Ÿæ–¹æ³•ï¼Œæˆ–ä»¥ä¸»åŠ¨å­¦ä¹ çš„æ–¹å¼ã€‚

### 3.1 Machine Learning for Result Estimation

The reports from HLS tools provide important guidance for tuning the high-level directives. However, acquiring accurate result estimation in an early stage is difficult due to complex optimizations in the physical synthesis, imposing a trade-off between accuracy (waiting for post-synthesis results) and efficiency (evaluating in the HLS stage). ML can be used to improve the accuracy of HLS reports through learning from real design benchmarks. In Section 3.1.1, we introduce previous work on predicting the timing, resource usage, and operation delay of an HLS design. In Section 3.1.2 we describe two types of research about cross-platform performance prediction.

HLSå·¥å…·çš„æŠ¥å‘Šä¸ºè°ƒèŠ‚é«˜å±‚æŒ‡ä»¤ç»™å‡ºäº†é‡è¦çš„å¼•å¯¼ã€‚ä½†æ˜¯ï¼Œåœ¨æ—©æœŸé˜¶æ®µè·å¾—ç²¾ç¡®çš„ç»“æœä¼°è®¡æ˜¯å¾ˆå›°éš¾çš„ï¼Œå› ä¸ºç‰©ç†ç»¼åˆè¿‡ç¨‹çš„ä¼˜åŒ–éå¸¸å¤æ‚ï¼Œåœ¨å‡†ç¡®ç‡ï¼ˆç­‰å¾…ç»¼åˆåçš„ç»“æœï¼‰å’Œæ•ˆç‡ï¼ˆåœ¨HLSé˜¶æ®µçš„è¯„ä¼°ï¼‰ä¹‹é—´ç»™å‡ºäº†ä¸€ä¸ªæŠ˜ä¸­ã€‚MLå¯ä»¥ä»çœŸå®çš„è®¾è®¡åŸºå‡†æµ‹è¯•ä¸­å­¦ä¹ ï¼Œä»è€Œç”¨äºæ”¹è¿›HLSæŠ¥å‘Šçš„å‡†ç¡®ç‡ã€‚åœ¨3.1.1èŠ‚ï¼Œæˆ‘ä»¬ä»‹ç»äº†ä¸€ä¸ªHLSè®¾è®¡åœ¨é¢„æµ‹æ—¶åºï¼Œèµ„æºä½¿ç”¨å’Œæ“ä½œå»¶è¿Ÿæ–¹é¢ä»¥å‰çš„å·¥ä½œã€‚åœ¨3.1.2èŠ‚æˆ‘ä»¬æè¿°äº†å…³äºè·¨å¹³å°æ€§èƒ½é¢„æµ‹æ–¹é¢ä¸¤ç§ç±»å‹çš„ç ”ç©¶ã€‚

**3.1.1 Estimation of Timing, Resource Usage, and Operation Delay**. The overall workflow of timing and resource usage prediction is concluded in Figure 2. This workflow is first proposed by Dai et al. [30] and augmented by Makrani et al. [108] and Ferianc et al. [37]. The main methodology is to train an ML model that takes HLS reports as input and outputs a more accurate implementation report without conducting the time-consuming post-implementation. The workflow proposed by Dai et al. [30] can be divided into two steps: data processing and training estimation models.

æ—¶åºï¼Œèµ„æºä½¿ç”¨å’Œæ“ä½œå»¶è¿Ÿçš„ä¼°è®¡ã€‚æ—¶åºå’Œèµ„æºä½¿ç”¨é¢„æµ‹æ–¹é¢çš„æ€»ä½“æµç¨‹å¦‚å›¾2æ‰€ç¤ºã€‚è¿™ç§å·¥ä½œæµç¨‹ç»è¿‡äº†æå‡ºå’Œæ”¹è¿›çš„è¿‡ç¨‹ã€‚ä¸»è¦çš„æ–¹æ³•æ˜¯è®­ç»ƒä¸€ä¸ªMLæ¨¡å‹ï¼Œä»¥HLSæŠ¥å‘Šä¸ºè¾“å…¥ï¼Œè¾“å‡ºä¸€ä¸ªæ›´ç²¾ç¡®çš„å®ç°æŠ¥å‘Šï¼Œè€Œä¸éœ€è¦è¿›è¡Œè€—æ—¶çš„åå®ç°æ“ä½œã€‚Daiç­‰[30]æå‡ºçš„å·¥ä½œæµï¼Œå¯ä»¥åˆ†æˆ2ä¸ªæ­¥éª¤ï¼šæ•°æ®å¤„ç†å’Œè®­ç»ƒä¼°è®¡æ¨¡å‹ã€‚

**Step 1: Data Processing**. To enable ML for HLS estimation, we need a dataset for training and testing. The HLS and implementation reports are usually collected across individual designs by running each design through the complete C-to-bitstream flow, for various clock periods and targeting different FPGA devices. After that, one can extract features from the HLS reports as inputs and features from implementation reports as outputs. Besides, to overcome the effect of colinearity and reduce the dimension of the data, previous studies often apply feature selection techniques to systematically remove unimportant features. The most commonly used features are summarized in Table 1.

æ­¥éª¤1ï¼šæ•°æ®å¤„ç†ã€‚ä¸ºèƒ½ç”¨MLå¯¹HLSè¿›è¡Œä¼°è®¡ï¼Œæˆ‘ä»¬éœ€è¦ä¸€ä¸ªæ•°æ®é›†è¿›è¡Œè®­ç»ƒå’Œæµ‹è¯•ã€‚é€šå¸¸è¦æ”¶é›†å•ä¸ªè®¾è®¡çš„HLSæŠ¥å‘Šå’Œå®ç°æŠ¥å‘Šï¼Œå°†æ¯ä¸ªè®¾è®¡ï¼Œå¯¹ä¸åŒçš„æ—¶é’Ÿå‘¨æœŸï¼Œé¢å‘ä¸åŒçš„FPGAè®¾å¤‡ï¼Œè¿è¡Œå®Œæ•´çš„C-to-bitstreamçš„æµã€‚åœ¨è¿™ä¸ªä¹‹åï¼Œå¯ä»¥ä»HLSæŠ¥å‘Šä¸­æå–ç‰¹å¾ä½œä¸ºè¾“å…¥ï¼Œå®ç°æŠ¥å‘Šä¸­çš„ç‰¹å¾ä½œä¸ºè¾“å‡ºã€‚å¦å¤–ï¼Œä¸ºå…‹æœå…±çº¿çš„æ•ˆæœï¼Œé™ä½æ•°æ®çš„ç»´åº¦ï¼Œä¹‹å‰çš„ç ”ç©¶é€šå¸¸ç”¨ç‰¹å¾é€‰æ‹©æŠ€æœ¯ï¼Œæ¥ç³»ç»Ÿæ€§çš„å»é™¤ä¸é‡è¦çš„ç‰¹å¾ã€‚æœ€å¸¸ç”¨çš„ç‰¹å¾ï¼Œå¦‚è¡¨1æ‰€ç¤ºã€‚

**Step 2: Training Estimation Models**. After constructing the dataset, regression models are trained to estimate post-implementation resource usages and clock periods. Frequently used metrics to report the estimation error include relative absolute error (RAE) and relative root mean squared error (RMSE). For both metrics, lower is better. RAE is defined in Equation (1), where $\hat ğ‘¦$ is a vector of values predicted by the model, ğ‘¦ is a vector of actual ground truth values in the testing set, and $\bar ğ‘¦$ denotes the mean value of ğ‘¦.

æ­¥éª¤2ï¼šè®­ç»ƒä¼°è®¡æ¨¡å‹ã€‚åœ¨æ„å»ºæ•°æ®é›†åï¼Œè®­ç»ƒäº†å›å½’æ¨¡å‹æ¥ä¼°è®¡å®ç°åèµ„æºä½¿ç”¨å’Œæ—¶é’Ÿå‘¨æœŸã€‚æŠ¥å‘Šä¼°è®¡é”™è¯¯æ‰€é¢‘ç¹ä½¿ç”¨çš„åº¦é‡åŒ…æ‹¬ï¼Œç›¸å¯¹ç»å¯¹è¯¯å·®(RAE)ï¼Œå’Œç›¸å¯¹å‡æ–¹è¯¯å·®(RMSE)ã€‚å¯¹ä¸¤ç§åº¦é‡éƒ½æ˜¯è¶Šå°è¶Šå¥½ã€‚RAEç”±å¼(1)å®šä¹‰ï¼Œå…¶ä¸­$\hat ğ‘¦$æ˜¯æ¨¡å‹é¢„æµ‹çš„å€¼çš„çŸ¢é‡ï¼Œğ‘¦æ˜¯æµ‹è¯•é›†ä¸­å®é™…çš„çœŸå€¼çŸ¢é‡ï¼Œ$\bar ğ‘¦$è¡¨ç¤ºğ‘¦çš„å‡å€¼ã€‚

$$RAE = \frac {\hat ğ‘¦ - ğ‘¦}{y - \bar y}$$(1)

Relative RMSE is given by Equation (2), where ğ‘ is the number of samples, and $\hat ğ‘¦_ğ‘–$ and $ğ‘¦_ğ‘–$ are the predicted and actual values of a sample, respectively.

ç›¸å¯¹RMSEæ˜¯ç”±å¼(2)ç»™å‡ºï¼Œå…¶ä¸­Næ˜¯æ ·æœ¬æ•°é‡ï¼Œ$\hat ğ‘¦_ğ‘–$å’Œ$ğ‘¦_ğ‘–$åˆ†åˆ«æ˜¯æ ·æœ¬çš„é¢„æµ‹å€¼å’Œå®é™…å€¼ã€‚

$$Relative RMSE = \sqrt{\sum_{i=1}^N (\frac{\hat ğ‘¦_ğ‘– - ğ‘¦_ğ‘–}{ğ‘¦_ğ‘–})^2/N} Ã— 100%$$(2)

Makrani et al. [108] model timing as a regression problem, and use the Minerva tool [36] to obtain results in terms of maximum clock frequency, throughput, and throughput-to-area ratio for the RTL code generated by the HLS tool. Then an ensemble model combining linear regression, neural network, SVM, and random forest, is proposed to conduct estimation and achieve an accuracy higher than 95%. There are also studies that predict whether a post-implementation is required or not, instead of predicting the implementation results. As a representative study, Liu and SchÃ¤fer [94] train a predictive model to avoid re-synthesizing each new configuration.

Makraniç­‰[108]å°†æ—¶åºå»ºæ¨¡ä¸ºä¸€ä¸ªå›å½’é—®é¢˜ï¼Œä½¿ç”¨Minervaå·¥å…·[36]æ¥å¯¹HLSå·¥å…·ç”Ÿæˆçš„RTLä»£ç å¾—åˆ°æœ€å¤§æ—¶é’Ÿé¢‘ç‡ï¼Œé€šé‡å’Œé€šé‡é¢ç§¯æ¯”ç­‰ç»“æœã€‚ç„¶åï¼Œä¸€ä¸ªé›†æˆæ¨¡å‹å°†çº¿æ€§å›å½’ï¼Œç¥ç»ç½‘ç»œï¼ŒSVMå’Œéšæœºæ£®æ—ï¼Œç»“åˆåˆ°ä¸€èµ·ï¼Œè¿›è¡Œä¼°è®¡ï¼Œå¾—åˆ°çš„å‡†ç¡®ç‡é«˜äº95%ã€‚ä¹Ÿæœ‰ç ”ç©¶é¢„æµ‹å®ç°åæ˜¯å¦éœ€è¦ï¼Œè€Œä¸æ˜¯é¢„æµ‹å®ç°ç»“æœã€‚ä½œä¸ºä¸€ä¸ªä»£è¡¨æ€§ç ”ç©¶ï¼ŒLiu and SchÃ¤fer [94]è®­ç»ƒäº†ä¸€ä¸ªé¢„æµ‹æ¨¡å‹ï¼Œæ¥é¿å…é‡æ–°ç»¼åˆæ¯ä¸ªæ–°é…ç½®ã€‚

ML techniques have been applied recently to reduce the HLS toolâ€™s prediction error of the operation delay [143]. Existing HLS tools perform delay estimations based on the simple addition of pre-characterized delays of individual operations, and can be inaccurate because of the post-implementation optimizations (e.g., mapping to hardened blocks like DSP adder cluster). A customized Graph Neural Network (GNN) model is built to capture the association between operations from the dataflow graph, and train this model to infer the mapping choices about hardened blocks. Their method can reduce the RMSE of the operation delay prediction of Vivado HLS by 72%.

MLæŠ€æœ¯æœ€è¿‘è¿›è¡Œäº†åº”ç”¨ï¼Œä»¥é™ä½HLSå·¥å…·çš„æ“ä½œå»¶è¿Ÿé¢„æµ‹è¯¯å·®[143]ã€‚ç°æœ‰çš„HLSå·¥å…·è¿›è¡Œå»¶è¿Ÿä¼°è®¡ï¼Œæ˜¯åŸºäºå•ä¸ªæ“ä½œçš„é¢„å…ˆç‰¹å¾åŒ–çš„å»¶è¿Ÿçš„ç®€å•ç›¸åŠ ï¼Œå¯èƒ½æ˜¯ä¸å‡†ç¡®çš„ï¼Œå› ä¸ºè¿˜æœ‰åå®ç°ä¼˜åŒ–ï¼ˆå¦‚ï¼Œæ˜ å°„åˆ°ç¡¬åŒ–çš„æ¨¡å—ï¼Œå¦‚DSPåŠ æ³•ç°‡ï¼‰ã€‚æ„å»ºäº†ä¸€ä¸ªå®šåˆ¶çš„GNNæ¨¡å‹ï¼Œæ¥æ•è·æ•°æ®æµå›¾ä¸­çš„æ“ä½œä¹‹é—´çš„å…³è”ï¼Œè®­ç»ƒè¿™ä¸ªæ¨¡å‹æ¥æ¨ç†ç¡¬åŒ–æ¨¡å—çš„æ˜ å°„é€‰é¡¹ã€‚ä»–ä»¬çš„æ–¹æ³•å¯ä»¥é™ä½Vivado HLSçš„æ“ä½œå»¶è¿Ÿé¢„æµ‹çš„RMSEè¾¾åˆ°72%ã€‚

**3.1.2 Cross-Platform Performance Prediction**. Hardware/software co-design enables designers to take advantage of new hybrid platforms such as Zynq. However, dividing an application into two parts makes the platform selection difficult for the developers, since there is a huge variation in the applicationâ€™s performance of the same workload across various platforms. To avoid fully implementing the design on each platform, Makrani et al. [109] propose an ML-based cross-platform performance estimator, XPPE, and its overall workflow is described in Figure 3. The key functionality of XPPE is using the resource utilization of an application on one specific FPGA to estimate its performance on other FPGAs.

3.1.2 è·¨å¹³å°æ€§èƒ½é¢„æµ‹ã€‚ç¡¬ä»¶/è½¯ä»¶ååŒè®¾è®¡ä½¿è®¾è®¡è€…å¯ä»¥åˆ©ç”¨æ–°çš„æ··åˆå¹³å°ï¼Œå¦‚Zynqã€‚ä½†æ˜¯ï¼Œå°†ä¸€ä¸ªåº”ç”¨åˆ†æˆä¸¤éƒ¨åˆ†ï¼Œä½¿å¼€å‘è€…å¾ˆéš¾é€‰æ‹©å¹³å°ï¼Œç”±äºç›¸åŒçš„workloadåœ¨ä¸åŒçš„å¹³å°ä¸Šï¼Œå…¶åº”ç”¨çš„æ€§èƒ½ä¼šæœ‰å¾ˆå¤§çš„å˜åŒ–ã€‚ä¸ºé¿å…åœ¨æ¯ä¸ªå¹³å°å¯¹è®¾è®¡è¿›è¡Œå®Œæ•´çš„å®ç°ï¼ŒMakraniç­‰[109]æå‡ºä¸€ç§åŸºäºMLçš„è·¨å¹³å°æ€§èƒ½ä¼°è®¡å™¨ï¼ŒXPPEï¼Œå…¶æ€»ä½“å·¥ä½œæµå¦‚å›¾3æ‰€ç¤ºã€‚XPPEçš„å…³é”®åŠŸèƒ½æ˜¯ï¼Œä½¿ç”¨ä¸€ä¸ªåº”ç”¨åœ¨ç‰¹å®šFPGAå¹³å°çš„èµ„æºåˆ©ç”¨ï¼Œæ¥ä¼°è®¡å…¶åœ¨å…¶ä»–FPGAä¸Šæ€§èƒ½ã€‚

XPPE uses a Neural Network (NN) model to estimate the speedup of an application for a target FPGA over the ARM processor. The inputs of XPPE are available resources on target FPGA, resource utilization report from HLS Vivado tool (extracted features, similar to the features in Table 1), and applicationâ€™s characteristics. The output is the speedup estimation on the target FPGA over an ARM A-9 processor. This method is similar to Dai et al. [30] and Makrani et al. [108] in that they all take the features in HLS reports as input and aim to avoid the time-consuming post-implementation. The main difference is that the input and output features in XPPE are from different platforms. The relative RMSE between the predictions and the real measurements is used to evaluate the accuracy of the estimator. The proposed architecture can achieve a relative mean square error of 5.1% and the speedup is more than 0.98Ã—.

XPPEä½¿ç”¨ä¸€ä¸ªç¥ç»ç½‘ç»œæ¨¡å‹æ¥ä¼°è®¡ä¸€ä¸ªåº”ç”¨åœ¨ç›®æ ‡FPGAä¸Šå¯¹åœ¨ARMå¤„ç†å™¨ä¸Šçš„åŠ é€Ÿã€‚XPPEçš„è¾“å…¥æ˜¯åœ¨ç›®æ ‡FPGAä¸Šçš„å¯ç”¨èµ„æºï¼ŒHLS Vivadoå·¥å…·çš„èµ„æºåˆ©ç”¨æŠ¥å‘Šï¼ˆæå–å‡ºçš„ç‰¹å¾ï¼Œä¸è¡¨1ä¸­çš„ç‰¹å¾ç±»ä¼¼ï¼‰ï¼Œå’Œåº”ç”¨çš„ç‰¹å¾ã€‚è¾“å‡ºæ˜¯åœ¨ç›®æ ‡FPGAä¸Šï¼Œå¯¹åœ¨ARM A-9å¤„ç†å™¨çš„åŠ é€Ÿä¼°è®¡ã€‚è¿™ä¸ªæ–¹æ³•ä¸Daiç­‰[30]å’ŒMakraniç­‰[108]ç­‰çš„æ–¹æ³•ç±»ä¼¼ï¼Œéƒ½ä»¥HLSæŠ¥å‘Šä¸­çš„ç‰¹å¾ä½œä¸ºè¾“å…¥ï¼Œä»¥é¿å…è€—æ—¶çš„åå®ç°ã€‚ä¸»è¦çš„å·®åˆ«æ˜¯ï¼ŒXPPEçš„è¾“å…¥å’Œè¾“å‡ºç‰¹å¾ï¼Œæ˜¯ä»ä¸åŒçš„å¹³å°æ¥çš„ã€‚åœ¨é¢„æµ‹å’Œå®é™…æµ‹é‡ä¹‹é—´çš„ç›¸å¯¹RMSEï¼Œç”¨äºè¯„ä¼°ä¼°è®¡å™¨çš„å‡†ç¡®ç‡ã€‚æå‡ºçš„æ¶æ„å¯ä»¥å¾—åˆ°5.1%çš„RMSEï¼ŒåŠ é€Ÿè¶…è¿‡äº†0.98xã€‚

Like XPPE, Oâ€™Neal et al. [116] also propose an ML-based cross-platform estimator, named HLSPredict. There are two differences. First, HLSPredict only takes workloads (the applications in XPPE) as inputs instead of the combination of HLS reports, applicationâ€™s characteristics and specification of target FPGA devices. Second, the target platform of HLSPredict must be the same as the platform in the training stage. In general, HLSPredict aims to rapidly estimate performance on a specific FPGA by direct execution of a workload on a commercially available off-the-shelf host CPU, but XPPE aims to accurately predict the speedup of different target platforms. For optimized workloads, HLSPredict achieves a relative absolute percentage error ($ğ´ğ‘ƒğ¸ = |(ğ‘¦âˆ’ \hat ğ‘¦)/ğ‘¦|$) of 9.08% and a 43.78Ã— runtime speedup compared with FPGA synthesis and direct execution.

ä¸XPPEç±»ä¼¼ï¼ŒO'Nealç­‰[116]è¿˜æå‡ºäº†ä¸€ç§åŸºäºMLçš„è·¨å¹³å°ä¼°è®¡å™¨ï¼Œåä¸ºHLSPredictã€‚æœ‰ä¸¤ä¸ªå·®å¼‚ã€‚ç¬¬ä¸€ï¼ŒHLSPredictåªå°†workloadä½œä¸ºè¾“å…¥ï¼ˆXPPEä¸­çš„åº”ç”¨ï¼‰ï¼Œè€Œä¸æ˜¯HLSæŠ¥å‘Šï¼Œåº”ç”¨çš„ç‰¹å¾å’Œç›®æ ‡FPGAè®¾å¤‡çš„æŒ‡æ ‡çš„é›†åˆã€‚ç¬¬äºŒï¼ŒHLSPredictçš„ç›®æ ‡å¹³å°ï¼Œå¿…é¡»ä¸è®­ç»ƒæ—¶çš„å¹³å°ä¸€æ ·ã€‚æ€»ä½“æ¥è¯´ï¼ŒHLSPredictçš„ç›®æ ‡æ˜¯è¿…é€Ÿä¼°è®¡ç‰¹å®šFPGAå¹³å°çš„æ€§èƒ½ï¼Œå°†workloadåœ¨ä¸€ä¸ªå•†ç”¨å¼€ç®±å³ç”¨çš„å®¿ä¸»CPUè¿›è¡Œç›´æ¥æ‰§è¡Œï¼Œè€ŒXPPEçš„ç›®æ ‡æ˜¯å‡†ç¡®çš„é¢„æµ‹ä¸åŒç›®æ ‡å¹³å°çš„åŠ é€Ÿã€‚å¯¹ä¼˜åŒ–çš„workloadsï¼ŒHLSPredictè·å¾—çš„ç›¸å¯¹ç»å¯¹è¯¯å·®ç™¾åˆ†æ¯”($ğ´ğ‘ƒğ¸ = |(ğ‘¦âˆ’ \hat ğ‘¦)/ğ‘¦|$)ä¸º9.08%ï¼Œè¿è¡Œæ—¶åŠ é€Ÿä¸º43.78xï¼ˆFPGAç»¼åˆå’Œç›´æ¥æ‰§è¡Œçš„å¯¹æ¯”ï¼‰ã€‚

### 3.2 Machine Learning for Design Space Exploration in HLS

In the previous subsection, we describe how ML models are used to predict the quality of results. Another application of ML in HLS is to assist DSE. The tunable synthesis options in HLS, provided in the form of pragmas, span a very large design space. Most often, the task of DSE is to find the Pareto Frontier Curve, on which every point is not fully dominated by any other points under all the metrics.

åœ¨å‰ä¸€å°èŠ‚ä¸­ï¼Œæˆ‘ä»¬æè¿°äº†MLæ¨¡å‹æ€æ ·ç”¨äºé¢„æµ‹ç»“æœçš„è´¨é‡ã€‚MLåœ¨HLSä¸­çš„å¦ä¸€ä¸ªåº”ç”¨æ˜¯æ”¯æŒDSEã€‚HLSä¸­çš„å¯è°ƒèŠ‚çš„ç»¼åˆé€‰é¡¹æ˜¯ä»¥pragmaçš„å½¢å¼ç»™å‡ºçš„ï¼Œæ’‘èµ·äº†ä¸€ä¸ªéå¸¸å¤§çš„è®¾è®¡ç©ºé—´ã€‚DSEä»»åŠ¡æœ€ç»å¸¸çš„ç›®æ ‡æ˜¯ï¼Œæ‰¾åˆ°Pareto frontæ›²çº¿ï¼Œå…¶ä¸­çš„æ¯ä¸ªç‚¹ä¸Šï¼Œå…¶ä»–ä»»ä½•ç‚¹éƒ½ä¸ä¼šåœ¨æ‰€æœ‰åº¦é‡ä¸Šdominateè¿™ä¸ªç‚¹ã€‚

Classical search algorithms have been applied in HLS DSE, such as Simulated Annealing (SA) and Genetic Algorithm (GA). But these algorithms are unable to learn from the database of previously explored designs. Many previous studies use an ML predictive model to guide the DSE. The models are trained on the synthesis results of explored design points, and used to predict the quality of new designs (See more discussions on this active learning workflow in Section 9.1). Typical studies are elaborated in Section 3.2.1. There is also a thread of work that involves learning-based methods to improve the inefficient or sensitive part of the classical search algorithms, as elaborated inSection 3.2.2. Some work included in this subsection focuses on system-level DSE rather than HLS design [74], or general active learning theories [180].

ç»å…¸æœç´¢ç®—æ³•æ›¾ç»åº”ç”¨åˆ°HLS DSEä¸Šï¼Œæ¯”å¦‚æ¨¡æ‹Ÿé€€ç«ï¼Œå’Œé—ä¼ ç®—æ³•ã€‚ä½†è¿™äº›ç®—æ³•ä¸èƒ½ä»ä¹‹å‰æ¢ç´¢è¿‡çš„è®¾è®¡ä¸­å­¦ä¹ ã€‚å¾ˆå¤šä¹‹å‰çš„ç ”ç©¶ä½¿ç”¨ä¸€ä¸ªMLé¢„æµ‹æ¨¡å‹æ¥å¼•å¯¼DSEã€‚æ¨¡å‹æ˜¯åœ¨æ¢ç´¢è¿‡çš„è®¾è®¡ç‚¹çš„ç»¼åˆç»“æœä¸Šè¿›è¡Œè®­ç»ƒçš„ï¼Œç”¨äºé¢„æµ‹æ–°è®¾è®¡çš„è´¨é‡ï¼ˆè§9.1èŠ‚è¿™ä¸ªä¸»åŠ¨å­¦ä¹ å·¥ä½œæµçš„æ›´å¤šè®¨è®ºï¼‰ã€‚å…¸å‹çš„ç ”ç©¶åœ¨3.2.1èŠ‚ä¸­è¿›è¡Œè¯¦è¿°ã€‚è¿˜æœ‰ä¸€æ¡å·¥ä½œçº¿æ˜¯ï¼ŒåŸºäºå­¦ä¹ çš„æ–¹æ³•æ¥æ”¹è¿›ç»å…¸æœç´¢æ–¹æ³•çš„ä½æ•ˆæ€§æˆ–æ•æ„Ÿéƒ¨åˆ†ï¼Œè¿™éƒ¨åˆ†è§3.2.2èŠ‚ã€‚æœ¬èŠ‚ä¸­åŒ…å«çš„ä¸€äº›å·¥ä½œï¼Œå…³æ³¨çš„æ˜¯ç³»ç»Ÿçº§çš„DSEï¼Œè€Œä¸æ˜¯HLS è®¾è®¡ï¼Œæˆ–ä¸€èˆ¬çš„ä¸»åŠ¨å­¦ä¹ ç†è®ºã€‚

**3.2.1 Active Learning**. The four papers visited in this part utilize the active learning approach to perform DSE for HLS, and use predictive ML models to surrogate actual synthesis when evaluating a design. Liu and SchÃ¤fer [94] propose a design space explorer that selects new designs to implement through an active learning approach. Transductive experimental design (TED) [95] focuses on seeking the samples that describe the design space accurately. Pareto active learning (PAL) in [180] is proposed to sample designs which the learner cannot clearly classify. Instead of focusing on how accurately the model describes the design space, adaptive threshold non-pareto elimination (ATNE) [112] estimates the inaccuracy of the learner and achieves better performance than TED and PAL.

ä¸»åŠ¨å­¦ä¹ ã€‚æœ¬å°èŠ‚å›é¡¾çš„å››ç¯‡æ–‡ç« ï¼Œåˆ©ç”¨äº†ä¸»åŠ¨å­¦ä¹ çš„æ–¹æ³•ï¼Œæ¥è¿›è¡ŒHLSçš„DSEï¼Œåœ¨è¯„ä¼°ä¸€ä¸ªè®¾è®¡æ—¶ï¼Œä½¿ç”¨é¢„æµ‹æ€§çš„MLæ¨¡å‹ï¼Œæ¥ä»£ç†å®é™…çš„ç»¼åˆã€‚[94]æå‡ºäº†ä¸€ä¸ªè®¾è®¡ç©ºé—´æ¢ç´¢æ–¹æ³•ï¼Œé€šè¿‡ä¸€ç§ä¸»åŠ¨å­¦ä¹ æ–¹æ³•ï¼Œæ¥é€‰æ‹©æ–°çš„è®¾è®¡æ¥å®ç°ã€‚è½¬æ¢è¯•éªŒè®¾è®¡(TED)[95]èšç„¦çš„æ˜¯å¯»æ‰¾å¯ä»¥ç²¾ç¡®çš„æè¿°è®¾è®¡ç©ºé—´çš„æ ·æœ¬ã€‚Paretoä¸»åŠ¨å­¦ä¹ (PAL)[180]çš„æå‡ºï¼Œæ˜¯ä¸ºäº†é‡‡æ ·å­¦ä¹ è€…ä¸èƒ½æ¸…æ™°çš„åˆ†ç±»çš„è®¾è®¡ã€‚è‡ªé€‚åº”é˜ˆå€¼non-paretoæ¶ˆé™¤(ANTE)[112]ä¼°è®¡çš„æ˜¯å­¦ä¹ è€…çš„ä¸ç²¾ç¡®æ€§ï¼Œè€Œä¸æ˜¯èšç„¦åœ¨æ¨¡å‹æ€æ ·ç²¾ç¡®çš„æè¿°è®¾è®¡ç©ºé—´ï¼Œè·å¾—äº†æ¯”TEDå’ŒPALæ›´å¥½çš„æ€§èƒ½ã€‚

Liu and SchÃ¤fer [94] propose a dedicated explorer to search for Pareto-optimal HLS designs for FPGAs. The explorer iteratively selects potential Pareto-optimal designs to synthesize and verify. The selection is based on a set of important features, which are adjusted during the exploration. The proposed method runs 6.5Ã— faster than an exhaustive search, and runs 3.0Ã— faster than a restricted search method but finds results with higher quality.

[94]æå‡ºäº†ä¸€ä¸ªä¸“ç”¨explorerï¼Œæœç´¢FPGAç”¨çš„Paretoæœ€ä¼˜HLSè®¾è®¡ã€‚Explorerè¿­ä»£é€‰æ‹©å¯èƒ½çš„Paretoæœ€ä¼˜è®¾è®¡æ¥è¿›è¡Œç»¼åˆå’ŒéªŒè¯ã€‚è¿™ä¸ªé€‰æ‹©æ˜¯åŸºäºä¸€ç³»åˆ—é‡è¦çš„ç‰¹å¾ï¼Œåœ¨æ¢ç´¢çš„è¿‡ç¨‹ä¸­è¿›è¡Œè°ƒæ•´ã€‚æå‡ºçš„æ–¹æ³•æ¯”ç©·ä¸¾å¼æœç´¢è¦å¿«6.5xï¼Œæ¯”ä¸€ç§å—é™çš„æœç´¢æ–¹æ³•å¿«3.0xï¼Œä½†æ‰¾åˆ°çš„ç»“æœæœ‰æ›´é«˜çš„è´¨é‡ã€‚

The basic idea of TED [95] is to select representative as well as the hard-to-predict samples from the design space, instead of the random sample used in previous work. The target is to maximize the accuracy of the predictive model with the fewest training samples. The authors formulate the problem of finding the best sampling strategy as follows: TED assumes that the overall number of knob settings is ğ‘›(|K| = ğ‘›), from which we want to select a training set $\tilde K$ such that $|\tilde K| = ğ‘š$. Minimizing the prediction error $ğ»(ğ‘˜) âˆ’ \tilde ğ»(ğ‘˜)$ for all ğ‘˜ âˆˆ K is equivalent to the following problem:

TED[95]çš„åŸºæœ¬æ€æƒ³æ˜¯ä»è®¾è®¡ç©ºé—´ä¸­é€‰æ‹©æœ‰ä»£è¡¨æ€§çš„ï¼ŒåŒæ—¶ä¹Ÿæ˜¯éš¾ä»¥é¢„æµ‹çš„æ ·æœ¬ï¼Œè€Œä¸æ˜¯ä¹‹å‰çš„å·¥ä½œä¸­ä½¿ç”¨çš„éšæœºæ ·æœ¬ã€‚ç›®æ ‡æ˜¯ç”¨æœ€å°çš„è®­ç»ƒæ ·æœ¬æœ€å¤§åŒ–é¢„æµ‹æ¨¡å‹çš„å‡†ç¡®ç‡ã€‚ä½œè€…å°†æ‰¾åˆ°æœ€å¥½çš„é‡‡æ ·ç­–ç•¥çš„é—®é¢˜è¡¨è¿°å¦‚ä¸‹ï¼šTEDå‡è®¾ï¼Œæ€»ä½“knobè®¾ç½®çš„æ•°é‡ä¸ºğ‘›(|K| = ğ‘›)ï¼Œä»ä¸­æˆ‘ä»¬å¸Œæœ›é€‰æ‹©ä¸€ä¸ªè®­ç»ƒé›†$\tilde K$ï¼Œä½¿å¾—$|\tilde K| = ğ‘š$ã€‚å¯¹æ‰€æœ‰çš„ğ‘˜ âˆˆ Kæœ€å°åŒ–é¢„æµ‹è¯¯å·®$ğ»(ğ‘˜) âˆ’ \tilde ğ»(ğ‘˜)$ï¼Œç­‰ä»·äºå¦‚ä¸‹é—®é¢˜ï¼š

$$max_{\tilde K} T[K\tilde K^T(\tilde K\tilde K^T + ğœ‡I)^{-1}\tilde KK^T], s.t. \tilde K âŠ‚ K, |\tilde K| = m$$

where ğ‘‡[Â·] is the matrix trace operator and ğœ‡ > 0. The authors interpret their solution as sampling from a set $\tilde K$ that span a linear space, to retain most of the information of K [95].

å…¶ä¸­ğ‘‡[Â·]æ˜¯çŸ©é˜µè¿¹è¿ç®—ï¼Œğœ‡ > 0ã€‚ä½œè€…è§£é‡Šå…¶è§£å†³æ–¹æ¡ˆï¼Œé‡‡æ ·å¾—åˆ°çš„é›†åˆ$\tilde K$è¦å¼ æˆä¸€ä¸ªçº¿æ€§ç©ºé—´ï¼Œä¿ç•™æœ€å¤šçš„Kä¸­çš„ä¿¡æ¯ã€‚

PAL [180] is proposed for general active learning scenarios and is demonstrated by a sorting network synthesis DSE problem in the paper. It uses Gaussian Process (GP) to predict Pareto-optimal points in design space. The models predict the objective functions to identify points that are Pareto-optimal with high probabilities. A point ğ‘¥ that has not been sampled is predicted as $\hat ğ‘“(ğ‘¥) = ğœ‡(ğ‘¥)$ and ğœ(ğ‘¥) is interpreted as the uncertainty of the prediction which can be captured by the hyperrectangle

PAL[180]æ˜¯ä¸ºäº†é€šç”¨ä¸»åŠ¨å­¦ä¹ åœºæ™¯è€Œæå‡ºï¼Œè®ºæ–‡ä¸­ç”¨ä¸€ä¸ªæ’åºç½‘ç»œç»¼åˆDSEé—®é¢˜æ¥è¯æ˜ã€‚å…¶åˆ©ç”¨é«˜æ–¯è¿‡ç¨‹(GP)æ¥é¢„æµ‹è®¾è®¡ç©ºé—´ä¸­çš„Pareto-optimalç‚¹ã€‚æ¨¡å‹é¢„æµ‹ç›®æ ‡å‡½æ•°æ¥è¯†åˆ«é«˜æ¦‚ç‡Pareto-optimalç‚¹ã€‚æ²¡æœ‰è¢«é‡‡æ ·çš„ç‚¹ğ‘¥é¢„æµ‹ä¸º$\hat ğ‘“(ğ‘¥) = ğœ‡(ğ‘¥)$ï¼Œğœ(ğ‘¥)è§£é‡Šä¸ºé¢„æµ‹çš„ä¸ç¡®å®šæ€§ï¼Œå¯ä»¥é€šè¿‡ä¸‹é¢çš„è¶…çŸ©å½¢æ¥æ•è·

$$ğ‘„_{ğœ‡,ğœ,ğ›½} (ğ‘¥) = {ğ‘¦: ğœ‡(ğ‘¥) âˆ’ ğ›½^{1/2}ğœ(ğ‘¥) âª¯ ğ‘¦ âª¯ ğœ‡(ğ‘¥) + ğ›½^{1/2}ğœ(ğ‘¥)}$$

where ğ›½ is a scaling parameter to be chosen. PAL focuses on accurately predicting points near the Pareto frontier, instead of the whole design space. In every iteration, the algorithm classifies samples into three groups: Pareto-optimal, Non-Pareto-optimal, and uncertain ones. The next design point to evaluate is the one with the largest uncertainty, which intuitively has more information to improve the model. The training process is terminated when there are no uncertain points. The points classified as Pareto-optimal are then returned.

å…¶ä¸­ğ›½æ˜¯ä¸€ä¸ªè¦é€‰æ‹©çš„ç¼©æ”¾å‚æ•°ã€‚PALèšç„¦åœ¨å‡†ç¡®çš„é¢„æµ‹Pareto frontieré™„è¿‘çš„ç‚¹ä¸Šï¼Œè€Œä¸æ˜¯æ•´ä¸ªè®¾è®¡ç©ºé—´ã€‚åœ¨æ¯æ¬¡è¿­ä»£ä¸­ï¼Œç®—æ³•å°†æ ·æœ¬åˆ†æˆä¸‰ç»„ï¼šPareto-optimal, Non-Pareto-optimal, å’Œä¸ç¡®å®šçš„æ ·æœ¬ã€‚è¦è¯„ä¼°çš„ä¸‹ä¸€ä¸ªç‚¹ï¼Œæ˜¯æœ‰æœ€å¤§ä¸ç¡®å®šæ€§çš„é‚£ä¸ªç‚¹ï¼Œä»ç›´è§‰ä¸Šæ¥è¯´ï¼Œæœ‰æ›´å¤šçš„ä¿¡æ¯æ¥æ”¹è¿›æ¨¡å‹ã€‚åœ¨æ²¡æœ‰ä¸ç¡®å®šç‚¹çš„æ—¶å€™ï¼Œè®­ç»ƒè¿‡ç¨‹åœæ­¢ã€‚åˆ†ç±»ä¸ºPareto-optimalçš„ç‚¹ï¼Œç„¶åè¿›è¡Œè¿”å›ã€‚

ATNE [112] utilizes Random Forest (RF) to aid the DSE process. This work uses a Pareto identification threshold that adapts to the estimated inaccuracy of the RF regressor and eliminates the non-Pareto-optimal designs incrementally. Instead of focusing on improving the accuracy of the learner, ATNE focuses on estimating and minimizing the risk of losing â€œgoodâ€ designs due to learning inaccuracy.

ATNEåˆ©ç”¨éšæœºæ£®æ—æ¥å¸®åŠ©DSEè¿‡ç¨‹ã€‚è¿™ä¸ªå·¥ä½œä½¿ç”¨ä¸€ä¸ªParetoè¯†åˆ«é˜ˆå€¼ï¼Œéšç€RFå›å½’å™¨çš„ä¼°è®¡çš„ä¸å‡†ç¡®ç‡æ¥æ”¹å˜ï¼Œé€æ¸çš„æ¶ˆé™¤non-Pareto-optimalè®¾è®¡ã€‚ATNEèšç„¦åœ¨ä¼°è®¡å’Œæœ€å°åŒ–æŸå¤±å¥½çš„è®¾è®¡çš„é£é™©ï¼ˆç”±äºå­¦ä¹ åˆ°äº†ä¸å‡†ç¡®æ€§ï¼‰ï¼Œè€Œæ²¡æœ‰èšç„¦åœ¨æå‡å­¦ä¹ è€…çš„å‡†ç¡®ç‡ã€‚

**3.2.2 Machine Learning for Improving Other Optimization Algorithms**. In this part, we summarize three studies that use ML techniques to improve classical optimization algorithms.

æœºå™¨å­¦ä¹ ç”¨äºæ”¹è¿›å…¶ä»–ä¼˜åŒ–ç®—æ³•ã€‚è¿™ä¸ªéƒ¨åˆ†ä¸­ï¼Œæˆ‘ä»¬æ€»ç»“äº†ä¸‰ä¸ªç ”ç©¶ï¼Œä½¿ç”¨MLæŠ€æœ¯æ¥æ”¹è¿›ç»å…¸ä¼˜åŒ–ç®—æ³•ã€‚

STAGE [74] is proposed for DSE of many-core systems. The motivating observation of STAGE is that the performance of simulated annealing is highly sensitive to the starting point of the search process. The authors build an ML model to learn which parts of the design space should be focused on, eliminating the times of futile exploration [13]. The proposed strategy is divided into two stages. The first stage (local search) performs a normal local search, guided by a cost function based on the designerâ€™s goals. The second stage (meta search) tries to use the search trajectories from previous local search runs to learn to predict the outcome of local search given a certain starting point [74].

STAGEçš„æå‡ºæ˜¯ä¸ºäº†ä¼—æ ¸ç³»ç»Ÿçš„DSEã€‚STAGEçš„æ¨åŠ¨æ€§è§‚å¯Ÿæ˜¯ï¼Œæ¨¡æ‹Ÿé€€ç«çš„æ€§èƒ½ï¼Œå¯¹æœç´¢è¿‡ç¨‹çš„èµ·å§‹ç‚¹é«˜åº¦æ•æ„Ÿã€‚ä½œè€…æ„å»ºäº†ä¸€ä¸ªMLæ¨¡å‹ï¼Œæ¥å­¦ä¹ åº”å½“èšç„¦åœ¨è®¾è®¡ç©ºé—´çš„å“ªä¸ªéƒ¨åˆ†ï¼Œæ¶ˆé™¤æ— ç”¨æ¢ç´¢çš„æ¬¡æ•°ã€‚æå‡ºçš„ç­–ç•¥åˆ†æˆä¸¤ä¸ªé˜¶æ®µã€‚ç¬¬ä¸€é˜¶æ®µï¼ˆå±€éƒ¨æœç´¢ï¼‰è¿›è¡Œæ­£å¸¸çš„å±€éƒ¨æœç´¢ï¼Œç”±åŸºäºè®¾è®¡è€…ç›®æ ‡çš„ä»£ä»·å‡½æ•°æ¥å¼•å¯¼ã€‚ç¬¬äºŒä¸ªé˜¶æ®µï¼ˆå…ƒæœç´¢ï¼‰è¯•å›¾ä½¿ç”¨ä¹‹å‰çš„å±€éƒ¨æœç´¢çš„æœç´¢è½¨è¿¹ï¼Œåœ¨ç»™å®šç‰¹å®šèµ·å§‹ç‚¹çš„æƒ…å†µä¸‹ï¼Œæ¥å­¦ä¹ é¢„æµ‹å±€éƒ¨æœç´¢çš„è¾“å‡ºã€‚

Fast Simulated Annealing (FSA) [107] utilizes the decision tree to improve the performance of SA. Decision tree learning is a widely used method for inductive inference. The HLS pragmas are taken as input features. FSA first performs standard SA to generate enough training sets to build the decision tree. Then it generates new design configurations with the decision tree and keeps the dominating designs [107].

å¿«é€Ÿæ¨¡æ‹Ÿé€€ç«(FPA)[107]åˆ©ç”¨å†³ç­–æ ‘æ¥æ”¹è¿›SAçš„æ€§èƒ½ã€‚å†³ç­–æ ‘å­¦ä¹ æ˜¯ä¸€ç§å¹¿æ³›ä½¿ç”¨çš„æ–¹æ³•ï¼Œå¯ä»¥è¿›è¡Œå½’çº³æ¨ç†ã€‚HLS pragmaä½œä¸ºè¾“å…¥ç‰¹å¾ã€‚FSAé¦–å…ˆè¿›è¡Œæ ‡å‡†çš„SAæ¥ç”Ÿæˆè¶³å¤Ÿçš„è®­ç»ƒé›†ï¼Œæ¥æ„å»ºå†³ç­–æ ‘ã€‚ç„¶åç”¨å†³ç­–æ ‘æ¥ç”Ÿæˆæ–°çš„è®¾è®¡é…ç½®ï¼Œä¿ç•™ä¸»å¯¼çš„è®¾è®¡ã€‚

In a recent study, Wang and SchÃ¤fer [149] propose several ML techniques to help decide the hyper-parameter settings of three meta-heuristic algorithms: SA, GA and Ant Colony Optimizations (ACO). For each algorithm, the authors build an ML model that predicts the resultant design quality (measured by Average Distance to the Reference Set, ADRS) and runtime from hyper-parameter settings. Compared with the default hyper-parameters, their models can improve the ADRS by more than 1.92Ã— within similar runtime. The authors also combine SA, GA and ACO to build a new design space explorer, which further improves the search efficiency.

åœ¨æœ€è¿‘çš„ç ”ç©¶ä¸­ï¼Œ[149]æå‡ºå‡ ç§MLæŠ€æœ¯æ¥å¸®åŠ©å†³å®šä¸‰ç§å…ƒå¯å‘å¼ç®—æ³•çš„è¶…å‚æ•°é…ç½®ï¼šSAï¼ŒGAå’ŒACOã€‚å¯¹äºæ¯ç§ç®—æ³•ï¼Œä½œè€…æ„å»ºäº†ä¸€ä¸ªMLæ¨¡å‹ï¼Œä»è¶…å‚æ•°è®¾ç½®ä¸­é¢„æµ‹å¾—åˆ°çš„è®¾è®¡è´¨é‡ï¼ˆç”±åˆ°å‚è€ƒé›†çš„å¹³å‡è·ç¦»ADRSæ¥åº¦é‡ï¼‰å’Œè¿è¡Œæ—¶ã€‚ä¸é»˜è®¤çš„è¶…å‚æ•°æ¯”è¾ƒï¼Œä»–ä»¬çš„æ¨¡å‹å¯ä»¥æ”¹è¿›ADRSè¶…è¿‡1.92xï¼Œè¿è¡Œæ—¶é—´ç±»ä¼¼ã€‚ä½œè€…è¿˜ç»“åˆSAï¼ŒGAå’ŒACOï¼Œæ„å»ºäº†ä¸€ä¸ªæ–°çš„è®¾è®¡ç©ºé—´æ¢ç´¢è€…ï¼Œè¿™æ›´è¿›ä¸€æ­¥æ”¹è¿›äº†æœç´¢æ•ˆç‡ã€‚

### 3.3 Summary of Machine Learning for HLS

This section reviews recent work on ML techniques in HLS, as listed in Table 2. Using ML-based timing/resource/latency predictors and data-driven searching strategies, the engineering productivity of HLS tools can be further improved and higher-quality designs can be generated by efficiently exploring a large design space.

æœ¬èŠ‚å›é¡¾äº†æœ€è¿‘MLæŠ€æœ¯åœ¨HLSä¸­çš„å·¥ä½œï¼Œå¦‚è¡¨2æ‰€ç¤ºã€‚ä½¿ç”¨åŸºäºMLçš„æ—¶åº/èµ„æº/å»¶è¿Ÿé¢„æµ‹å™¨ï¼Œå’Œæ•°æ®é©±åŠ¨çš„æœç´¢ç­–ç•¥ï¼ŒHLSå·¥å…·çš„å·¥ç¨‹ç”Ÿäº§åŠ›å¯ä»¥è¿›ä¸€æ­¥æ”¹è¿›ï¼Œé€šè¿‡é«˜æ•ˆçš„æ¢ç´¢å¤§å‹è®¾è®¡ç©ºé—´ï¼Œå¯ä»¥ç”Ÿæˆæ›´é«˜è´¨é‡çš„è®¾è®¡ã€‚

We believe the following practice can help promote future research of ML in HLS: æˆ‘ä»¬ç›¸ä¿¡ï¼Œä¸‹é¢çš„å®è·µå¯ä»¥å¸®åŠ©æ¨åŠ¨MLåœ¨HLSä¸­çš„æœªæ¥ç ”ç©¶ï¼š

- Public benchmark for DSE problems. The researches about result estimation are all evaluated on public benchmarks of HLS applications, such as Rosetta [174], MachSuite [127], etc. However, DSE researches are often evaluated on a few applications because the cost of synthesizing a large design space for each application is heavy. Building a benchmark that collects different implementations of each application can help fairly evaluate DSE algorithms.

DSEé—®é¢˜çš„å…¬å¼€åŸºå‡†æµ‹è¯•ã€‚ç»“æœä¼°è®¡çš„ç ”ç©¶è€…éƒ½åœ¨HLSåº”ç”¨çš„å…¬å¼€åŸºå‡†æµ‹è¯•ä¸Šè¿›è¡Œè¯„ä¼°ï¼Œæ¯”å¦‚Rosettaï¼ŒMachSuiteç­‰ã€‚ä½†æ˜¯ï¼ŒDSEç ”ç©¶è€…é€šå¸¸åœ¨å‡ ä¸ªåº”ç”¨ä¸­è¿›è¡Œè¯„ä¼°ï¼Œå› ä¸ºä¸ºæ¯ä¸ªåº”ç”¨ç»¼åˆä¸€ä¸ªå¤§å‹è®¾è®¡ç©ºé—´çš„ä»£ä»·æ˜¯å¾ˆé‡çš„ã€‚æ„å»ºä¸€ä¸ªåŸºå‡†æµ‹è¯•ï¼Œæ”¶é›†æ¯ä¸ªåº”ç”¨çš„ä¸åŒå®ç°ï¼Œå¯ä»¥å¸®åŠ©å…¬å¹³çš„è¯„ä¼°DSEç®—æ³•ã€‚

- Customized ML models. Most of the previous studies use off-the-shelf ML models. Combining universal ML algorithms with domain knowledge can potentially improve the performance of the model. For example, Ustun et al. [143] customize a standard GNN model to handle the specific delay prediction problem, which brings extra benefit in model accuracy.

å®šåˆ¶MLæ¨¡å‹ã€‚å¤šæ•°ä¹‹å‰çš„ç ”ç©¶ä½¿ç”¨å¼€ç®±å³ç”¨çš„MLæ¨¡å‹ã€‚å°†é€šç”¨MLç®—æ³•ä¸é¢†åŸŸçŸ¥è¯†ç»“åˆèµ·æ¥ï¼Œå¯èƒ½ä¼šæ”¹è¿›æ¨¡å‹æ€§èƒ½ã€‚æ¯”å¦‚ï¼ŒUstunç­‰[143]å®šåˆ¶äº†ä¸€ä¸ªæ ‡å‡†GNNæ¨¡å‹ï¼Œæ¥å¤„ç†ç‰¹å®šå»¶è¿Ÿé¢„æµ‹é—®é¢˜ï¼Œå¯¹æ¨¡å‹å‡†ç¡®ç‡å¸¦æ¥äº†é¢å¤–çš„å¥½å¤„ã€‚

## 4. Logic Synthesis and Physical Design

In the logic synthesis and physical design stage, there are many key sub-problems that can benefit from the power of ML models, including lithography hotspot detection, path classification, congestion prediction, placement guide, fast timing analysis, logic synthesis scheduling, and so on. In this section, we organize the review of studies by their targeting problems.

### 4.1 Logic Synthesis

Logic synthesis is an optimization problem with complicated constraints, which requires accurate solutions. Consequently, using ML algorithms to directly generate logic synthesis solutions is difficult. However, there are some studies using ML algorithms to schedule existing traditional optimization strategies. For logic synthesis, LSOracle [115] relies on DNN to dynamically decide which optimizer should be applied to different parts of the circuit. The framework exploits two optimizers, and-inverter graph (AIG) and majority-inverter graph (MIG), and applies k-way partitioning on circuit directed acyclic graph (DAG).

There are many logic transformations in current synthesis tools such as ABC [14]. To select an appropriate synthesis flow, Yu et al. [167] formulate a multi-class classification problem and design a CNN to map a synthesis flow to quality of results (QoR) levels. The prediction on unlabeled flows are then used to select the optimal synthesis flow. The CNN takes the one-hot encoding of synthesis flows as inputs and outputs the possibilities of the input flow belonging to different QoR metric levels.

Reinforcement learning is also employed for logic synthesis in [48, 56]. A transformation between two DAGs with the same I/O behaviors is modeled as an action. In [48], GCN is utilized as a policy function to obtain the probabilities for every action. [56] employs advantage actor critic agent (A2C) to search the optimal solution.

### 4.2 Placement and Routing Prediction

**4.2.1 Traditional Placers Enhancement**. While previous fast placers can conduct random logic placement efficiently with good performances, researchers find that their placement of data path logic is suboptimal. PADE [150] proposes a placement process with automatic data path extraction and evaluation, in which the placement of data path logic is conducted separately from random logic. PADE is a force-directed global placer, which applies SVM and NN to extract and evaluate the data path patterns with high dimensional data such as netlist symmetrical structures, initial placement hints, and relative area. The extracted data path is mapped to bit stack structure and uses SAPT [151] (a placer placed on SimPL [73]) to optimize separately from random logic.

**4.2.2 Routing Information Prediction**. The basic requirements of routing design rules must be considered in the placement stage. However, it is difficult to predict routing information in the placement stage accurately and fast, and researchers recently employ machine learning to solve this. RouteNet [154] is the first work to employ CNN for design rule checking (DRC) hotspot detection. The input features of a customized fully convolutional network (FCN) include the outputs of rectangular uniform wire density (RUDY), a pre-routing congestion estimator. An 18-layer ResNet is also employed to predict design rule violation (DRV) count. A recent work [89] abstracts the pins and macros density in placement results into image data, and utilizes a pixel-wise loss function to optimize an encoder-decoder model (an extension of U-Net architecture). The network output is a heat-map, which represents the location where detailed routing congestion may occur. PROS [21] takes advantages of fully convolution networks to predict routing congestion from global placement results. The framework is demonstrated efficient on industrial netlists. Pui et al. [124] explore the possibilities using ML methods to predict routing congestion in UltraScale FPGAs. Alawieh et al. [6] transfer the routing congestion problem in large-scale FPGAs to an image-to-image problem, and then uses conditional GAN to solve it. In addition, there are some studies that only predict the number of congestions instead of the location of congestion [27, 106]. Maarouf et al. [106] use models like linear regression, RF and MLP to learn how to use features from earlier stages to produce more accurate congestion prediction, so that the placement strategy can be adjusted. Qi et al. [125] predict the detailed routing congestion using nonparametric regression algorithm, multivariate adaptive regression splines (MARS) with the global information as inputs. Another study [18] takes the netlist, clock period, utilization, aspect ratio and BEOL stack as inputs and utilizes MARS and SVM to predict the routability of a placement. This study also predicts Pareto frontiers of utilization, number of metal layers, and aspect ratio. Study in [99] demonstrates the potential of embedding ML-based routing congestion estimator into global placement stage. Recently, Liang et al. [90] build a routing-free crosstalk prediction model by adopting several ML algorithms such as regression, NN, GraphSAGE and GraphAttention. The proposed framework can identify nets with large crosstalk noise before the routing step, which allows us to modify the placement results to reduce crosstalk in advance.

There is also a need to estimate the final wirelength, timing performance, circuit area, power consumption, clock and other parameters in the early stage. Such prediction task can be modeled as a regression task and commonly-used ML models include SVM, Boosting, RF, MARS, etc. Jeong et al. [63] learn a model with MARS to predict performance from a given set of circuit configurations, with NoC router, a specific functional circuit and a specific business tool. In [60], the researchers introduce linear discriminant analysis (LDA) algorithm to find seven combined features for the best representation, and then a KNN-like approach is adopted to combine the prediction results of ANN, SVM, LASSO, and other machine learning models. In this way, Hyun et al. [60] improve the wirelength prediction given by the virtual placement and routing in the synthesis. Cheng et al. [27] predict the final circuit performance in the macro placement stage, and Li and Franzon [82] predict the circuit performance in the global routing stage, including congestion number, hold slack, area and power.

For sign-off timing analysis, Barboza et al. [8] use random forest to give the sign-off timing slack from hand-crafted features. Another research [67] works on sign-off timing analysis and use linear regression to fit the static timing analysis (STA) model, thus reduce the frequency that the incremental static timing analysis (iSTA) tool need to be called. Han et al. [52] propose SI for Free, a regression method to predict expensive signal integrity (SI) mode sign-off timing results by using cheap non-SI mode sign-off timing analysis. [68] propose golden timer extension (GTX), a framework to reduce mismatches between different sign-off timing analysis tools to obtain neither optimistic nor pessimistic results.

Lu et al. [102] employ GAN and RL for clock tree prediction. Flip flop distribution, clock net distribution, and trial routing results serve as input images. For feature extraction, GAN-CTS adopts transfer learning from a pre-trained ResNet-50 on the ImageNet dataset by adding fully-connected (FC) layers. A conditional GAN is utilized to optimize the clock tree synthesis, of which the generator is supervised by the regression model. An RL-based policy gradient algorithm is leveraged for the clock tree synthesis optimization.

**4.2.3 Placement Decision Making**. As the preliminary step of the placement, floorplanning aims to roughly determine the geometric relationship among circuit modules and to estimate the cost of the design. He et al. [53] explore the possibility of acquiring local search heuristics through a learning mechanism. More specifically, an agent has been trained using a novel deep Q-learning algorithm to perform a walk in the search space by selecting a candidate neighbor solution at each step, while avoiding introducing too much prior human knowledge during the search. Google [113] recently models chip placement as a sequential decision making problem and trains an RL policy to make placement decisions. During each episode, the RL agent lays the macro in order. After arranging macros, it utilizes the force-directed method for standard cell placement. GCN is adopted in this work to embed information related to macro features and the adjacency matrix of the netlist. Besides, FC layers are used to embed metadata. After the embedding of the macros, the graph and the metadata, another FC layer is applied for reward prediction. Such embedding is also fed into a deconvolution CNN model, called PolicyNet, to output the mask representing the current macro placement. The policy is optimized with RL to maximize the reward, which is the weighted average of wirelength and congestion.

### 4.3 Power Deliver Network Synthesis and IR Drop Predictions

Power delivery network (PDN) design is a complex iterative optimization task, which strongly influences the performance, area and cost of a chip. To reduce the design time, recent studies have paid attention to ML-based IR drop estimation, a time-consuming sub-task. Previous work usually adopts simulator-based IR analysis, which is challenged by the increasing complexity of chip design. IR drop can be divided into two categories: static and dynamic. Static IR drop is mainly caused by voltage deviation of the metal wires in the power grid, while dynamic IR drop is led by the switching behaviors and localized fluctuating currents. In IncPIRD [54], the authors employ XGBoost to conduct incremental prediction of static IR drop problem, which is to predict IR value changes caused by the modification of the floorplan. For dynamic IR drop estimation, Xie et al. [155] aim to predict the IR values of different locations and models IR drop estimation problem as a regression task. This work introduces a â€œmaximum CNNâ€ algorithm to solve the problem. Besides, PowerNet is designed to be transferable to new designs, while most previous studies train models for specific designs. A recent work [173] proposes an electromigration-induced IR drop analysis framework based on conditional GAN. The framework regards the time and selected electrical features as input images and outputs the voltage map. Another recent work [28] focuses on PDN synthesis in floorplan and placement stages. This paper designs a library of stitchable templates to represent the power grid in different layers. In the training phase, SA is adopted to choose a template. In the inference phase, MLP and CNN are used to choose the template for floorplan and placement stages, respectively. Cao et al. [16] use hybrid surrogate modeling (HSM) that combines SVM, ANN and MARS to predict the bump inductance that represents the quality of the power delivery network.

### 4.4 Design Challenges for 3D Integration

3D integration is gaining more attention as a promising approach to further improve the integration density. It has been widely applied in memory fabrication by stacking memory over logic.

Different from the 2D design, 3D integration introduces die-to-die variation, which does not exist in 2D modeling. The data or clock path may cross different dies in through-silicon via (TSV)-based 3D IC. Therefore, the conventional variation modeling methods, such as on-chip variation (OCV), advanced OCV (AOCV), parametric OCV (POCV), are not able to accurately capture the path delay [131]. Samal et al. [131] use MARS to model the path delay variation in 3D ICs.

3D integration also brings challenges to the design optimization due to the expanded design space and the overhead of design evaluation. To tackle these challenges, several studies [31, 122, 131] have utilized design space exploration methods based on machine learning to facilitate 3D integration optimization.

The state-of-the-art 3D placement methods [75, 121] perform bin-based tier partitioning on 2D placement and routing design. However, the bin-based partitioning can cause significant quality degradation to the 3D design because of the unawareness of the design hierarchy and technology. Considering the graph-like nature of the VLSI circuits, Lu et al. [103] proposed a GNN-based unsupervised framework (TP-GNN) for tier partitioning. TP-GNN first performs the hierarchy-aware edge contraction to acquire the clique-based graph where nodes within the same hierarchy can be contracted into supernodes. Moreover, the hierarchy and the timing information is included in the initial feature of each node before GNN training. Then the unsupervised GNN learning can be applied to general 3D design. After the GNN training, the weighted k-means clustering is performed on the clique-based graph for the tier assignment based on the learned representation. The proposed TP-GNN framework is validated on experiments of RISC-V based multi-core system and NETCARD from ISPD 2012 benchmark. The experiment results indicate 7.7% better wirelength, 27.4% higher effective frequency and 20.3% performance improvement.

### 4.5 Other Predictions

For other parameters, Chan et al. [17] adopt HSM to predict the embedded memory timing failure during initial floorplan design. Bian et al. [11] work on aging effect prediction for high-dimensional correlated on-chip variations using random forest.

### 4.6 Summary of Machine Learning for Logic Synthesis and Physical Design

We summarize recent studies on ML for logic synthesis and physical design in Table 3. For logic synthesis, researchers focus on predicting and evaluating the optimal synthesis flows. Currently, these studies optimize the synthesis flow based on the primitives of existing tools. In the future, we expect to see more advanced algorithms for logic synthesis be explored, and more metrics can be formulated to evaluate the results of logic synthesis. Besides, applying machine learning to logic synthesis for emerging technologies is also an interesting direction.

In the physical design stage, recent studies mainly aim to improve the efficiency and accuracy by predicting the related information that traditionally needs further simulation. A popular practice is to formulate the EDA task as a computer vision (CV) task. In the future, we expect to see more studies that incorporate advanced techniques (e.g., neural architecture search, automatic feature generation, unsupervised learning) to achieve better routing and placement results.

## 5. Lithography and Mask Synthesis

Lithography is a key step in semiconductor manufacturing, which turns the designed circuit and layout into real objects. Two popular research directions are lithography hotspot detection and mask optimization. To improve yield, lithography hotspot detection is introduced after the physical implementation flow to identify process-sensitive patterns prior to the manufacturing. The complete optical simulation is always time-consuming, so it is necessary to analyze the routed layout by machine learning to reduce lithography hotspots in the early stages. Mask optimization tries to compensate diffraction information loss of design patterns such that the remaining pattern after lithography is as close to the design patterns as possible. Mask optimization plays an important role in VLSI design and fabrication flow, which is a very complicated optimization problem with high verification costs caused by expensive lithography simulation. Unlike the hotspot detection studies in Section 5.1 that take placement & routing stages into consideration, mask optimization focuses only on the lithography process, ensuring that the fabricated chip matches the designed layout. Optical proximity correction (OPC) and sub-resolution assist feature (SRAF) insertion are two main methods to optimize the mask and improve the printability of the target pattern.

### 5.1 Lithography Hotspot Detection

For lithography hotspot detection, Ding et al. [32] uses SVM for hotspot detection and small neural network for routing path prediction on each grid. To achieve better feature representation, Yang et al. [162] introduces feature tensor extraction, which is aware of the spatial relations of layout patterns. This work develops a batch-biased learning algorithm, which provides better trade-offs between accuracy and false alarms. Besides, there are also attempts to check inter-layer failures with deep learning solutions. A representative solution is proposed by Yang et al. [161]. They employ an adaptive squish layout representation for efficient metal-to-via failure check. Different layout-friendly neural network architectures are also investigated these include vanilla VGG [160], shallow CNN [162] and binary ResNet [65].

With the increased chip complexity, traditional deep learning/machine learning-based solutions are facing challenges from both runtime and detection accuracy. Chen et al. [22] recently propose an end-to-end trainable object detection model for large scale hotspot detection. The framework takes the input of a full/large-scale layout design and localizes the area that hotspots might occur (see Figure 6). In [44], an attention-based CNN with inception-based backbone is developed for better feature embeddings.

### 5.2 Machine Learning for Optical Proximity Correction

For OPC, inverse lithography technique (ILT) and model-based OPC are two representative mask optimization methodologies, and each of which has its own advantages and disadvantages. Yang et al. [163] propose a heterogeneous OPC framework that assists mask layout optimization, where a deterministic ML model is built to choose the appropriate one from multiple OPC solutions for a given design, as shown in Figure 7.

With the improvement of semiconductor technology and the scaling down of ICs, traditional OPC methodologies are becoming more and more complicated and time-consuming. Yang et al.[159] propose a new OPC method based on generative adversarial network (GAN). A Generator(G) is used to generate the mask pattern from the target pattern, and a discriminator (D) is used to estimate the quality of the generated mask. GAN-OPC can avoid complicated computation in ILT-based OPC, but it faces the problem that the algorithm is hard to converge. To deal with this problem, ILT-guided pre-training is proposed. In the pre-training stage, the D network is replaced with the ILT convolution model, and only the G network is trained. After pre-training, the ILT model that has huge cost is removed, and the whole GAN is trained. The training flow of GAN-OPC and ILT-guided pre-training is shown in Figure 8. The experimental results show that the GAN-based methodology can accelerate ILT based OPC significantly and generate more accurate mask patterns.

Traditional ILT-based OPC methods are costly and result in highly complex masks where many rectangular variable-shaped-beam (VSB) shots exist. To solve this problem, Jiang et al. [64] propose an ML-based OPC algorithm named neural-ILT, which uses a neural network to replace the costly ILT process. The loss function is specially designed to reduce the mask complexity, which gives punishment to complicated output mask patterns. In addition, for fast litho-simulation, a CUDA-based accelerator is proposed as well, which can save 96% simulation time. The experimental results show that neural-ILT achieves a 70Ã— speedup and 0.43Ã— mask complexity compared with traditional ILT methods.

Recently, Chen et al. [20] propose DAMO, an end-to-end OPC framework to tackle the full-chip scale. The lithography simulator and mask generator share the same deep conditional GAN (DCGAN), which is dedicatedly designed and can provide a competitively high resolution. The proposed DCGAN adopts UNet++ [176] backbone and adds residual blocks at the bottleneck of UNet++. To further apply DAMO on full-chip layouts, a coarse-to-fine window splitting algorithm is proposed. First, it locates the regions of high via density and then runs KMeans++ algorithm on each cluster containing the via pattern to find the best splitting window. Results on ISPD 2019 full-chip layout show that DAMO outperforms state-of-the-art OPC solutions in both academia [43] and an industrial toolkit.

### 5.3 Machine Learning for SRAF Insertion

Several studies have investigated ML-aided SRAF insertion techniques. Xu et al. [158] propose an SRAF insertion framework based on ML techniques. Geng et al. [43] propose a framework with a better feature extraction strategy. Figure 9 shows the feature extraction stage. After their concentric circle area sampling (CCAS) method, high-dimension features ğ‘¥ğ‘¡ are mapped into a discriminative low-dimension features ğ‘¦ğ‘¡ through dictionary training by multiplication of an atom matrix ğ·. The atom matrix is the dictionary consists of representative atoms of the original features. Then, the sparse codes ğ‘¦ğ‘¡ are used as the input of a machine learning model, more specifically, a logistic regression model that outputs a probability map indicating whether SRAF should be inserted at each grid. Then, the authors formulate and solve the SRAF insertion problem as an integer linear programming based on the probability grid and various SRAF design rules.

### 5.4 Machine Learning for Lithography Simulation

There are also studies that focus on fast simulation of the tedious lithography process. Traditional lithography simulation contains multiple steps, such as optical model building, resist model building, and resist pattern generation. LithoGAN [165] proposes an end-to-end lithography modeling method by using GAN, of which the framework is shown in Figure 10. Specifically, a conditional GAN is trained to map the mask pattern to a resist pattern. However, due to the characteristic of GAN, the generated shape pattern is good, while the position of the pattern is not precise. To tackle this problem, LithoGAN adopts a conditional GAN for shape modeling and a CNN for center prediction. The experimental results show that LithoGAN can predict the resist pattern with high accuracy, and this algorithm can reduce the lithography simulation time for several orders of magnitude. [20] is also equipped with a machine learning-based lithography simulator that can output via contours accurately to assist via-oriented OPC.

### 5.5 Summary

This section reviews ML techniques used in the design for manufacturability stage that include lithography hotspot detection, mask optimization and lithography modeling. Related studies are summarized in Table 4.

## 6. Analog Design

Despite the promotion of digital circuits, the analog counterpart is still irreplaceable in applications like nature signal processing, high speed I/O and drive electronics [126]. Unlike digital circuit design, analog design demands lots of manual work and expert knowledge, which often makes it the bottleneck of the job. For example, the analog/digital converter and Radio Frequency (RF) transceiver only occupy a small fraction of area but cost the majority of design efforts in a typical mixed-signal System-on-Chip (SoC), compared to other digital processors [129].

The reason for the discrepancy can be summarized as follows: 1) Analog circuits have a larger design space in terms of device size and topology than digital circuits. Sophisticated efforts are required to achieve satisfactory results. 2) The specifications of analog design are variable for different applications. It is difficult to construct a uniform framework to evaluate and optimize different analog designs. 3) Analog signals are more susceptible to noise and process-voltage-temperature variations, which cost additional efforts in validation and verification.

### 6.1 The Design Flow of Analog Circuits

Gielen and Rutenbar [45] provide the design flow followed by most analog designers. As shown in Figure 11, it includes both top-down design steps from system level to device-level optimizations and bottom-up layout synthesis and verification. In the top-down flow, designers choose proper topology, which satisfies system specifications in the circuit level. Then device sizes are optimized in the device level. The topology design and device sizing constitute the pre-layout design. After the schematic is well-designed, designers draw the layout of the circuit. Then they extract parasitics from the layout and simulate the circuit with parasitics. This is known as post-layout simulations. If the post-layout simulation fails to satisfy the specifications, designers need to resize the parameters and repeat the process again. This process can go for many iterations before the layout is done [136].

Although analog design automation has improved significantly over the past few decades, automatic tools cannot replace manual work in the design flow [10] yet. Recently, researchers are trying to introduce machine learning techniques to solve analog design problems. Their attempts range from topology selection at the circuit level to device sizing at the device level as well as the analog layout in the physical level.

### 6.2 Machine Learning for Circuit Topology Design Automation

6.2.1 Topology Selection.

6.2.2 Topological Feature Extraction.

6.2.3 Topology Generation.

### 6.3 Machine Learning for Device Sizing Automation

6.3.1 Reinforcement Learning Based Device Sizing.

6.3.2 Artificial Neural Network Based Device Sizing.

6.3.3 Machine Learning Based Prediction Methods.

6.3.4 Comparison and Discussion on Device Sizing.

### 6.4 Machine Learning for Analog Layout

### 6.5 Conclusion of Analog Design

## 7. Verification and Testing

### 7.1 Machine Learning for Test Set Redundancy Reduction

7.1.1 Test Set Redundancy Reduction for Digital Design Verification.

7.1.2 Test Set Redundancy Reduction for Analog/RF Design Testing.

7.1.3 Test Set Redundancy Reduction for Semiconductor Technology Testing.

### 7.2 Machine Learning for Test & Diagnosis Complexity Reduction

7.2.1 Test Complexity Reduction for Digital Design.

7.2.2 Verification Diagnosis Complexity Reduction for Digital Design.

7.2.3 Verification & Test Complexity Reduction for Analog/RF Design.

### 7.3 Summary of ML for Verification and Testing

## 8. Other Related Studies

### 8.1 Power Prediction

### 8.2 Machine Learning for SAT Solver

### 8.3 Acceleration with Deep Learning Engine

### 8.4 Auto-tuning design flow

## 9. Discussion From The Machine Learning Perspective

In this section, we revisit some aforementioned research studies from an ML-application perspective.

æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä»MLåº”ç”¨çš„è§’åº¦ï¼Œé‡æ–°å›é¡¾äº†ä¸€äº›ä¹‹å‰æåˆ°çš„ç ”ç©¶ã€‚

### 9.1 The Functionality of ML

Section 2.2 introduces the major ML models and algorithms used in EDA problems. Based on the functionality of ML in the EDA workflow, we can group most researches into four categories: decision making in traditional methods, performance prediction, black-box optimization, and automated design.

2.2èŠ‚ä»‹ç»äº†åœ¨EDAé—®é¢˜ä¸­ä½¿ç”¨çš„ä¸»è¦çš„MLæ¨¡å‹å’Œç®—æ³•ã€‚åŸºäºMLåœ¨EDAå·¥ä½œæµä¸­çš„ä¸»è¦åŠŸèƒ½ï¼Œæˆ‘ä»¬å°†ä¸»è¦ç ”ç©¶åˆ†æˆå››ç§ç±»åˆ«ï¼šä¼ ç»Ÿæ–¹æ³•ä¸­çš„å†³ç­–ç®—æ³•ï¼Œæ€§èƒ½é¢„æµ‹ï¼Œé»‘ç›’ä¼˜åŒ–ï¼Œå’Œè‡ªåŠ¨è®¾è®¡ã€‚

**Decision making in traditional methods**. The configurations of EDA tools, including the choice of algorithm or hyper-parameters, have a strong impact on the efficiency of the procedure and quality of the outcome. This class of researches utilizes ML models to replace brute-force or empirical methods when deciding configurations. ML has been used to select among available tool-chains for logic synthesis [115, 167] , mask synthesis [163], and topology selection in analog design [111, 117, 137]. ML has also been exploited to select hyper-parameters for non-ML algorithms such as Simulated Annealing, Genetic Algorithm, etc. (refer to Section 3.2.2).

**ä¼ ç»Ÿæ–¹æ³•ä¸­çš„å†³ç­–ç®—æ³•**ã€‚EDAå·¥å…·çš„é…ç½®ï¼ŒåŒ…æ‹¬ç®—æ³•æˆ–è¶…å‚æ•°çš„é€‰æ‹©ï¼Œå¯¹è¾“å‡ºçš„è¿‡ç¨‹å’Œè´¨é‡çš„æ•ˆç‡æœ‰å¾ˆå¼ºçš„å½±å“ã€‚å½“å†³å®šé…ç½®æ—¶ï¼Œç»å…¸çš„ç ”ç©¶åˆ©ç”¨MLæ¨¡å‹æ¥æ›¿ä»£æš´åŠ›æˆ–ç»éªŒæ–¹æ³•ã€‚MLå·²ç»è¢«ç”¨äºé€‰æ‹©å¯ç”¨çš„å·¥å…·é“¾è¿›è¡Œé€»è¾‘ç»¼åˆï¼Œæ©è†œç»¼åˆï¼Œæ¨¡æ‹Ÿè®¾è®¡ä¸­çš„æ‹“æ‰‘é€‰æ‹©ã€‚MLä¹Ÿè¢«åˆ©ç”¨æ¥é€‰æ‹©éMLç®—æ³•çš„è¶…å‚æ•°ï¼Œæ¯”å¦‚æ¨¡æ‹Ÿé€€ç«ï¼Œé—ä¼ ç®—æ³•ï¼Œç­‰ã€‚

**Performance prediction**. This type of tasks mainly use supervised or unsupervised learning algorithms. Classification, regression and generative models are trained by former cases in real production to estimate QoR rapidly, to assist engineers to drop unqualified designs without time-consuming simulation or synthesis.

**æ€§èƒ½é¢„æµ‹**ã€‚è¿™ç§ç±»å‹çš„ä»»åŠ¡ä¸»è¦ä½¿ç”¨ç›‘ç£å­¦ä¹ æˆ–æ— ç›‘ç£å­¦ä¹ ç®—æ³•ã€‚ç”¨ä¹‹å‰çš„æ¡ˆä¾‹æ¥åœ¨å®é™…ç”Ÿäº§ä¸­è®­ç»ƒåˆ†ç±»ï¼Œå›å½’å’Œç”Ÿæˆå¼æ¨¡å‹ï¼Œæ¥è¿…é€Ÿä¼°è®¡QoRï¼Œæ¥å¸®åŠ©å·¥ç¨‹å¸ˆä¸¢å¼ƒä¸åˆæ ¼çš„è®¾è®¡ï¼Œè€Œä¸ç”¨è€—æ—¶çš„ä»¿çœŸæˆ–ç»¼åˆã€‚

ML-based performance prediction is a very common type of ML application. Typical applications of this type include congestion prediction in placement & routing and hotspot detection in manufacturability estimation (Table 8). The most commonly-used models are Linear Regression, Random Forests, XGBoost, and prevailing CNNs.

åŸºäºMLçš„æ€§èƒ½é¢„æµ‹æ˜¯éå¸¸å¸¸è§ç±»å‹çš„MLåº”ç”¨ã€‚è¿™ç§ç±»å‹çš„å…¸å‹åº”ç”¨åŒ…æ‹¬å¸ƒå±€å¸ƒçº¿ä¸­çš„æ‹¥å µé¢„æµ‹ï¼Œå’Œå¯åˆ¶é€ æ€§ä¼°è®¡ä¸­çš„çƒ­ç‚¹æ£€æµ‹ï¼ˆè¡¨8ï¼‰ã€‚æœ€å¸¸ç”¨çš„æ¨¡å‹åŒ…æ‹¬ï¼Œçº¿æ€§å›å½’ï¼Œéšæœºæ£®æ—ï¼ŒXGBoostï¼Œå’Œæµè¡Œçš„CNNsã€‚

**Black-box optimization**. This type of tasks mainly use active learning. Many tasks in EDA are DSE, i.e., searching for an optimal (single- or multi-objective) design point in a design space. Leveraging ML in these problems usually yields black-box optimization, which means that the search for optimum is guided by a surrogate ML model, not an explicit analytical model or hill-climbing techniques. The ML model learns from previously-explored design points and guides the search direction by making predictions on new design points. Different from the first category, the ML model is trained in an active-learning process rather than on a static dataset, and the inputs are usually a set of configurable parameters rather than results from other design stages.

é»‘ç›’ä¼˜åŒ–ã€‚è¿™ç§ç±»å‹çš„ä»»åŠ¡ä¸»è¦ä½¿ç”¨ä¸»åŠ¨å­¦ä¹ ã€‚EDAä¸­çš„å¾ˆå¤šä»»åŠ¡æ˜¯DSEï¼Œå³ï¼Œåœ¨è®¾è®¡ç©ºé—´ä¸­æœç´¢ä¸€ä¸ªæœ€ä¼˜çš„è®¾è®¡ç‚¹ï¼ˆå•ç›®æ ‡æˆ–å¤šç›®æ ‡çš„ï¼‰ã€‚åœ¨è¿™äº›é—®é¢˜ä¸­åˆ©ç”¨MLé€šå¸¸ä¼šå¾—åˆ°é»‘ç›’ä¼˜åŒ–é—®é¢˜ï¼Œè¿™æ„å‘³ç€æœç´¢æœ€ä¼˜æ˜¯ç”±ä»£ç†MLæ¨¡å‹å¼•å¯¼çš„ï¼Œå¹¶ä¸æ˜¯ä¸€ä¸ªæ˜¾å¼çš„è§£ææ¨¡å‹ï¼Œæˆ–çˆ¬å¡æŠ€æœ¯ã€‚MLæ¨¡å‹ä»ä¹‹å‰æ¢ç´¢è¿‡çš„è®¾è®¡ç‚¹ä¸­å­¦ä¹ ï¼Œé€šè¿‡åœ¨æ–°è®¾è®¡ç‚¹ä¸Šè¿›è¡Œé¢„æµ‹ï¼Œæ¥å¼•å¯¼æœç´¢æ–¹å‘ã€‚ä¸ç¬¬ä¸€ç§ç±»åˆ«ä¸åŒï¼ŒMLæ¨¡å‹çš„è®­ç»ƒï¼Œæ˜¯ä¸€ä¸ªä¸»åŠ¨å­¦ä¹ è¿‡ç¨‹ï¼Œè€Œä¸æ˜¯åœ¨ä¸€ä¸ªé™æ€æ•°æ®é›†ä¸Šï¼Œè¾“å…¥é€šå¸¸æ˜¯å¯é…ç½®çš„å‚æ•°é›†ï¼Œè€Œä¸æ˜¯ä»å…¶ä»–è®¾è®¡é˜¶æ®µçš„ç»“æœã€‚

Black-box optimization is widely used for DSE in many EDA problems. Related ML theories and how to combine with the EDA domain knowledge are extensively studied in literature. Typical applications of this type include tuning HLS-level parameters and physical parameters of 3D integration (see Table 8). The key techniques are to find an underlying surrogate model and a search strategy to sample new design points. Options of the surrogate model include GP, along with all the models used in performance prediction [105, 112]. Search strategies are usually heuristics from domain knowledge, including uniformly random exploration [95], exploring the most uncertain designs [180], exploring and eliminating the worst designs [112], etc.

åœ¨å¾ˆå¤šEDAé—®é¢˜ä¸­ï¼Œé»‘ç›’ä¼˜åŒ–å¹¿æ³›çš„ç”¨äºDSEã€‚ç›¸å…³çš„MLç†è®ºå’Œæ€æ ·ä¸EDAé¢†åŸŸçŸ¥è¯†ç»“åˆï¼Œåœ¨æ–‡çŒ®ä¸­è¿›è¡Œäº†å¹¿æ³›çš„ç ”ç©¶ã€‚è¿™ç§ç±»å‹çš„å…¸å‹åº”ç”¨åŒ…æ‹¬ï¼Œè°ƒèŠ‚HLSçº§çš„å‚æ•°ï¼Œå’Œ3Dé›†æˆçš„ç‰©ç†å‚æ•°ï¼ˆè§è¡¨8ï¼‰ã€‚å…³é”®æŠ€æœ¯æ˜¯æ‰¾åˆ°æ½œåœ¨çš„ä»£ç†æ¨¡å‹å’Œæœç´¢ç­–ç•¥ï¼Œæ¥é‡‡æ ·æ–°çš„è®¾è®¡ç‚¹ã€‚ä»£ç†æ¨¡å‹çš„é€‰æ‹©åŒ…æ‹¬GPï¼Œå’Œåœ¨æ€§èƒ½é¢„æµ‹ä¸­ç”¨åˆ°çš„æ‰€æœ‰æ¨¡å‹ã€‚æœç´¢ç­–ç•¥é€šå¸¸æ˜¯é¢†åŸŸçŸ¥è¯†ä¸­çš„å¯å‘å¼ï¼ŒåŒ…æ‹¬å‡åŒ€éšæœºæœç´¢ï¼Œæ¢ç´¢æœ€ä¸ç¡®å®šçš„è®¾è®¡ï¼Œæ¢ç´¢å’Œæ¶ˆé™¤æœ€åçš„è®¾è®¡ï¼Œç­‰ã€‚

**Automated design**. Some studies leverage AI to automate design tasks that rely heavily on human efforts. Typical applications are placement [113] and analog device sizing [134, 147, 148]. At first look it is similar to black-box optimization, but we highlight the differences as:

è‡ªåŠ¨åŒ–è®¾è®¡ï¼šä¸€äº›ç ”ç©¶åˆ©ç”¨AIæ¥è‡ªåŠ¨åŒ–è®¾è®¡ä»»åŠ¡ï¼Œè¿™äº›ä»»åŠ¡ä¸¥é‡ä¾èµ–äºäººçš„åŠªåŠ›ã€‚å…¸å‹çš„åº”ç”¨æ˜¯å¸ƒå±€ï¼Œå’Œæ¨¡æ‹Ÿdevice sizingã€‚ç¬¬ä¸€çœ¼çœ‹ä¸Šå»ï¼Œä¸é»‘ç›’ä¼˜åŒ–å¾ˆåƒï¼Œä½†å…¶å·®å¼‚æ˜¯ï¼š

- The design space can be larger and more complex, for example in placement, the locations of all the cells. è®¾è®¡ç©ºé—´ä¼šæ›´å¤§æ›´å¤æ‚ï¼Œæ¯”å¦‚åœ¨å¸ƒå±€ä¸­ï¼Œæ˜¯æ‰€æœ‰å•å…ƒçš„ä½ç½®ã€‚

- Instead of searching in the decision space, there exists a trainable decision-making policy that outputs the decisions, which is usually learned with RL techniques. å¹¶ä¸æ˜¯åœ¨å†³ç­–ç©ºé—´ä¸­æœç´¢ï¼Œè€Œæ˜¯æœ‰ä¸€ç§å¯è®­ç»ƒçš„å†³ç­–ç­–ç•¥ï¼Œè¾“å‡ºå†³ç­–ï¼Œé€šå¸¸æ˜¯é€šè¿‡RLæŠ€æœ¯æ¥å­¦ä¹ çš„ã€‚

More complicated algorithms with large volumes of parameters, such as deep reinforcement learning, are used in these problems. This stream of researches show the potential to fully automate IC design.

å¸¦æœ‰æ›´å¤§ä½“é‡å‚æ•°çš„æ›´å¤æ‚ç®—æ³•ï¼Œæ¯”å¦‚æ·±åº¦å¼ºåŒ–å­¦ä¹ ï¼Œä¹Ÿç”¨åœ¨äº†è¿™äº›é—®é¢˜ä¸­ã€‚è¿™æ¡çº¿çš„ç ”ç©¶å±•ç¤ºäº†å®Œå…¨è‡ªåŠ¨åŒ–ICè®¾è®¡çš„æ½œåŠ›ã€‚

Table 8 summarizes representative work of each category and typical model settings in terms of algorithm, input and output.

è¡¨8æ€»ç»“äº†æ¯ä¸ªç±»åˆ«çš„ä»£è¡¨æ€§å·¥ä½œï¼Œå’Œå…¸å‹çš„æ¨¡å‹è®¾ç½®ï¼ŒåŒ…æ‹¬ç®—æ³•ï¼Œè¾“å…¥å’Œè¾“å‡ºã€‚

### 9.2 Data Preparation

The volume and quality of the dataset are essential to model performance. Almost all studies we review make some discussions on leveraging EDA domain knowledge to engineer a large, fair and clean dataset.

æ•°æ®é›†çš„ä½“é‡å’Œè´¨é‡ï¼Œå¯¹æ¨¡å‹æ€§èƒ½æ˜¯éå¸¸å…³é”®çš„ã€‚æˆ‘ä»¬å›é¡¾çš„å‡ ä¹æ‰€æœ‰ç ”ç©¶ï¼Œéƒ½ä¼šå¯¹åˆ©ç”¨EDAé¢†åŸŸçŸ¥è¯†æ¥ç”Ÿæˆä¸€ä¸ªå¤§å‹çš„åˆç†çš„å¹²å‡€çš„æ•°æ®é›†æ¥è¿›è¡Œä¸€äº›è®¨è®ºã€‚

**Raw data collection**. Raw features and ground truth / labels are two types of data needed by ML models. Raw feature extraction is often a problem-specific design, but there are some shared heuristics. Some studies treat the layout as images and leverage image processing algorithms [32, 89, 154]. Some choose geometric or graph-based features from the netlist [150]. Some use traditional algorithms to generate features [6, 67, 106, 154]. Quite a lot studies choose features manually [6, 11, 16, 17, 27, 82, 115]. To some extend, manual feature selection lacks a theoretical guarantee or practical guidance for other problems. The labels or ground truth are acquired through time-consuming simulation or synthesis. This also drives researchers to improve data efficiency by carefully architect their models and preprocess input features, or use semi-supervised techniques [25] to expand the dataset.

åŸå§‹æ•°æ®æ”¶é›†ã€‚åŸå§‹ç‰¹å¾å’ŒçœŸå€¼/æ ‡ç­¾æ˜¯MLæ¨¡å‹éœ€è¦çš„ä¸¤ç§ç±»å‹çš„æ•°æ®ã€‚åŸå§‹ç‰¹å¾æå–é€šå¸¸æ˜¯ä¸€ä¸ªé—®é¢˜ç‰¹å®šçš„è®¾è®¡ï¼Œä½†æ˜¯æœ‰ä¸€äº›å…±äº«çš„å¯å‘å¼ã€‚ä¸€äº›ç ”ç©¶å°†layoutè§†ä¸ºå›¾åƒï¼Œåˆ©ç”¨å›¾åƒå¤„ç†ç®—æ³•ã€‚ä¸€äº›ä»ç½‘è¡¨ä¸­é€‰æ‹©å‡ ä½•ç‰¹å¾æˆ–åŸºäºå›¾çš„ç‰¹å¾ã€‚ä¸€äº›ä½¿ç”¨ä¼ ç»Ÿç®—æ³•æ¥ç”Ÿæˆç‰¹å¾ã€‚ç›¸å½“å¤šçš„ç ”ç©¶æ˜¯æ‰‹å·¥é€‰æ‹©ç‰¹å¾çš„ã€‚åœ¨ä¸€å®šç¨‹åº¦ä¸Šï¼Œæ‰‹å·¥ç‰¹å¾é€‰æ‹©ç¼ºå°‘ç†è®ºä¿è¯æˆ–å¯¹å…¶ä»–é—®é¢˜çš„å®è·µå¼•å¯¼ã€‚æ ‡ç­¾æˆ–çœŸå€¼æ˜¯é€šè¿‡è€—æ—¶çš„ä»¿çœŸæˆ–ç»¼åˆè·å¾—çš„ã€‚è¿™ä¹Ÿæ¨åŠ¨ç ”ç©¶è€…é€šè¿‡ä»”ç»†çš„è®¾è®¡æ¨¡å‹æ¶æ„ï¼Œæˆ–é¢„å¤„ç†è¾“å…¥ç‰¹å¾ï¼Œå»æ”¹è¿›æ•°æ®æ•ˆç‡ï¼Œæˆ–ä½¿ç”¨åŠç›‘ç£çš„æŠ€æœ¯ï¼Œæ¥æ‰©å±•æ•°æ®é›†ã€‚

**Feature preprocessing**. Standard practices like feature normalization and edge data removal are commonly used in the preprocessing stage. Some studies also use dimension reduction techniques like PCA and LDA to further adjust input features [60].

ç‰¹å¾é¢„å¤„ç†ã€‚æ ‡å‡†çš„å®è·µå¦‚ç‰¹å¾å½’ä¸€åŒ–ï¼Œå’Œè¾¹ç¼˜æ•°æ®ç§»é™¤ï¼Œåœ¨é¢„å¤„ç†é˜¶æ®µæ˜¯å¸¸ç”¨çš„ã€‚ä¸€äº›ç ”ç©¶è¿˜ä½¿ç”¨é™ç»´æŠ€æœ¯ï¼Œå¦‚PCAå’ŒLDAï¼Œæ¥è¿›ä¸€æ­¥è°ƒæ•´è¾“å…¥ç‰¹å¾ã€‚

### 9.3 Domain Transfer

There have been consistent efforts to make ML-based solutions more adaptive to domain shift, so as to save training from scratch for every new task. Some researches propose ML models that take specifications of the new application domain and predict results in new domain based on results acquired in original domain. This idea is used in cross-platform performance estimation of FPGA design instances [109, 116]. It would be more exciting to train AI agents to adapt to new task without preliminary information of the new domain, and recent studies show that Reinforcement Learning (RL) might be a promising approach. RL models pre-trained on one task is able to perform nicely on new tasks after a fine-tune training on the new domain [113, 134, 147], which costs much less time than training from scratch and sometimes lead to even better results.

ä½¿åŸºäºMLçš„è§£å†³æ–¹æ¡ˆå¯¹é¢†åŸŸçš„å˜åŒ–æ›´åŠ é€‚åº”ï¼Œä¸€ç›´éƒ½æœ‰è¿™æ–¹é¢çš„åŠªåŠ›ï¼Œè¿™æ ·å°±å¯ä»¥ä¸ç”¨å¯¹æ¯ä¸ªæ–°ä»»åŠ¡ï¼Œéƒ½ä»å¤´è¿›è¡Œè®­ç»ƒã€‚ä¸€äº›ç ”ç©¶è€…æå‡ºMLæ¨¡å‹ï¼Œä»¥æ–°åº”ç”¨é¢†åŸŸçš„æŒ‡æ ‡ä¸ºè¾“å…¥ï¼ŒåŸºäºåŸå§‹é¢†åŸŸè·å¾—çš„ç»“æœï¼Œé¢„æµ‹æ–°é¢†åŸŸçš„ç»“æœã€‚è¿™ä¸ªè§‚ç‚¹åœ¨FPGAè®¾è®¡å®ä¾‹çš„è·¨å¹³å°æ€§èƒ½ä¼°è®¡ä¸­å¾—åˆ°äº†åº”ç”¨ã€‚è®­ç»ƒAI agentsï¼Œä¸éœ€è¦æ–°é¢†åŸŸçš„åˆæ­¥çŸ¥è¯†ï¼Œå°±å¯ä»¥é€‚åº”åˆ°æ–°ä»»åŠ¡ï¼Œè¿™ä¼šæ›´åŠ ä»¤äººæ¿€åŠ¨ï¼Œæœ€è¿‘çš„ç ”ç©¶è¡¨æ˜ï¼Œå¼ºåŒ–å­¦ä¹ å¯èƒ½æ˜¯ä¸€ä¸ªæœ‰å¸Œæœ›çš„æ–¹æ³•ã€‚åœ¨ä¸€ä¸ªä»»åŠ¡ä¸Šé¢„è®­ç»ƒçš„RLæ¨¡å‹å¯ä»¥åœ¨æ–°ä»»åŠ¡ä¸Šç»è¿‡åœ¨æ–°é¢†åŸŸçš„ç²¾è°ƒè®­ç»ƒï¼Œå°±å¯ä»¥è¡¨ç°çš„å¾ˆå¥½ï¼Œè¿™æ¯”ä»å¤´è®­ç»ƒçš„è€—æ—¶è¦å°‘å¾ˆå¤šï¼Œæœ‰æ—¶ç”šè‡³ä¼šå¸¦æ¥æ›´å¥½çš„ç»“æœã€‚

## 10. Conclusion and Future Work

It is promising to apply machine learning techniques in accelerating EDA tasks. In this way, the EDA tools can learn from previous experiences and solve the problem at hand more efficiently. So far machine learning techniques have found their applications in almost all stages of the EDA hierarchy. In this paper, we have provided a comprehensive review of the literature from both the EDA and the ML perspectives.

åº”ç”¨æœºå™¨å­¦ä¹ æŠ€æœ¯åŠ é€ŸEDAä»»åŠ¡æ˜¯å¾ˆæœ‰å¸Œæœ›çš„ã€‚åœ¨è¿™æ–¹é¢ï¼ŒEDAå·¥å…·å¯ä»¥ä»ä¹‹å‰çš„ç»éªŒä¸­è¿›è¡Œå­¦ä¹ ï¼Œæ›´é«˜æ•ˆçš„æ±‚è§£æ‰‹å¤´çš„é—®é¢˜ã€‚è¿„ä»Šä¸ºæ­¢ï¼Œæœºå™¨å­¦ä¹ æŠ€æœ¯åœ¨EDAçš„å‡ ä¹æ‰€æœ‰é˜¶æ®µéƒ½æœ‰å…¶åº”ç”¨ã€‚æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä»EDAçš„è§’åº¦å’ŒMLçš„è§’åº¦ç»™å‡ºäº†æ–‡çŒ®çš„ç»¼åˆå›é¡¾ã€‚

Although remarkable progress has been made in the field, we are looking forward to more studies on applying ML for EDA tasks from the following aspects. è™½ç„¶åœ¨è¿™ä¸ªé¢†åŸŸå·²ç»æœ‰äº†å¾ˆå¤šè¿›å±•ï¼Œæˆ‘ä»¬æœŸæœ›åœ¨ä¸‹é¢çš„æ–¹é¢æœ‰æ›´å¤šçš„ç ”ç©¶å°†MLåº”ç”¨åˆ°EDAä»»åŠ¡ä¸­ã€‚

- **Towards full-fledged ML-powered EDA tools**. In many tasks (e.g., analog/RF testing, physical design), the performance of purely using machine learning models is still difficult to meet the industrial needs. Therefore, smart combination of machine learning and the traditional method is of great importance. Current machine learning aided EDA methods may be still restricted to less flexible design spaces, or aim at solving a simplified problem. New models and algorithms are desired to be developed to make the ML models more useful in real applications.

å®Œå…¨æˆç†Ÿçš„MLèµ‹èƒ½çš„EDAå·¥å…·ã€‚åœ¨å¾ˆå¤šä»»åŠ¡ä¸­ï¼ˆå¦‚ï¼Œæ¨¡æ‹Ÿ/RFæµ‹è¯•ï¼Œç‰©ç†è®¾è®¡ï¼‰ï¼Œçº¯ä½¿ç”¨æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ï¼Œä»ç„¶å¾ˆéš¾æ»¡è¶³å·¥ä¸šåº”ç”¨ã€‚å› æ­¤ï¼Œæœºå™¨å­¦ä¹ å’Œä¼ ç»Ÿæ–¹æ³•çš„ç»“åˆæ˜¯éå¸¸é‡è¦çš„ã€‚ç›®å‰çš„æœºå™¨å­¦ä¹ è¾…åŠ©çš„EDAæ–¹æ³•ä»ç„¶å—é™äºæ²¡é‚£ä¹ˆçµæ´»çš„è®¾è®¡ç©ºé—´ï¼Œæˆ–ç›®æ ‡åœ¨äºæ±‚è§£ä¸€ä¸ªç®€åŒ–çš„é—®é¢˜ã€‚è¦å¼€å‘æ–°çš„æ¨¡å‹å’Œç®—æ³•ï¼Œä½¿MLæ¨¡å‹åœ¨å®é™…åº”ç”¨ä¸­æ›´åŠ æœ‰ç”¨ã€‚

- **Application of new ML techniques**. Very recently, some new machine learning models and methodologies (e.g., point cloud and GCN) and machine learning techniques (e.g., domain adaptation and reinforcement learning) begin to find their application in the EDA field. We expect to see a broader application of these techniques in the near future.

æ–°MLæŠ€æœ¯çš„åº”ç”¨ã€‚æœ€è¿‘ï¼Œä¸€äº›æ–°çš„æœºå™¨å­¦ä¹ æ¨¡å‹å’Œæ–¹æ³•å­¦ï¼ˆå¦‚ï¼Œç‚¹äº‘å’ŒGCNï¼‰å’Œæœºå™¨å­¦ä¹ æŠ€æœ¯ï¼ˆå¦‚ï¼Œé¢†åŸŸè‡ªé€‚åº”å’Œå¼ºåŒ–å­¦ä¹ ï¼‰ï¼Œå¼€å§‹åœ¨EDAé¢†åŸŸä¸­æ‰¾åˆ°äº†å…¶åº”ç”¨ã€‚æˆ‘ä»¬æœŸæœ›çœ‹åˆ°è¿™äº›æŠ€æœ¯åœ¨æœªæ¥æœ‰æ›´å¹¿æ³›çš„åº”ç”¨ã€‚

- **Trusted Machine Learning**. While ML holds the promise of delivering valuable insights and knowledge into the EDA flow, broad adoption of ML will rely heavily on the ability to trust their predictions/outputs. For instance, our trust in technology is based on our understanding of how it works and our assessment of its safety and reliability. To trust a decision made by an algorithm or a machine learning model, circuit designers or EDA tool users need to know that it is reliable and fair, and that it will cause no harm. We expect to see more research along this line making our automatic tool trusted.

å—ä¿¡çš„æœºå™¨å­¦ä¹ ã€‚MLæœ‰å¸Œæœ›ç»™å‡ºEDAæµçš„å®è´µçš„æ´è§å’ŒçŸ¥è¯†ï¼Œä½†æ˜¯å¹¿æ³›çš„é‡‡ç”¨MLï¼Œè‚¯å®šä¾èµ–äºä¿¡ä»»å…¶é¢„æµ‹/è¾“å‡ºçš„èƒ½åŠ›ã€‚æ¯”å¦‚ï¼Œæˆ‘ä»¬å¯¹ç§‘æŠ€çš„ä¿¡ä»»ï¼Œæ˜¯åŸºäºæˆ‘ä»¬å¯¹å…¶æ€æ ·å·¥ä½œçš„ç†è§£ï¼Œå’Œæˆ‘ä»¬å¯¹å…¶å®‰å…¨æ€§å’Œå¯é æ€§çš„è¯„ä¼°ã€‚ä¸ºä¿¡ä»»ç®—æ³•æˆ–æœºå™¨å­¦ä¹ æ¨¡å‹åšå‡ºçš„å†³ç­–ï¼Œç”µè·¯è®¾è®¡è€…æˆ–EDAå·¥å…·ä½¿ç”¨è€…éœ€è¦çŸ¥é“ï¼Œè¿™æ˜¯å¯é çš„ï¼Œåˆç†çš„ï¼Œå¹¶ä¸”ä¸ä¼šå¯¼è‡´ä»»ä½•å±å®³ã€‚æˆ‘ä»¬æœŸæœ›çœ‹åˆ°æ›´å¤šçš„è¿™æ¡çº¿çš„ç ”ç©¶ï¼Œä½¿æˆ‘ä»¬çš„è‡ªåŠ¨å·¥å…·è¢«ä¿¡ä»»ã€‚