# Simulation Points for SPEC CPU 2006

Arun A. Nair, Lizy K. John @ University of Texas at Austin

## 0. Abstract

Increasing sizes of benchmarks make detailed simulation an extremely time consuming process. Statistical techniques such as the SimPoint methodology have been proposed in order to address this problem during the initial design phase. The SimPoint methodology attempts to identify repetitive, long, large-grain phases in programs and predict the performance of the architecture based on its aggregate performance on the individual phases. This study attempts to compare accuracy of the SimPoint methodology for the SPEC CPU 2006 benchmark suite with that of SPEC CPU 2000 and to study the large-grain phases in the two benchmark suites using the SimPoint methodology. We find that there has not been a significant increase in the number of simulation points required to accurately predict the behavior of the programs in SPEC CPU 2006, despite its significantly larger data footprint and dynamic instruction count. We also find that the programs in both benchmark suites have similar characteristics in terms of the number of phases that contribute significantly towards overall behavior, further emphasizing the similarity between the two benchmark suites with respect to the number of simulation points required for similar accuracies.

基准测试规模的增加，使得其详细仿真过程是一个极度耗时的过程。已经提出了一些统计技术，比如SimPoint，来在初始设计阶段来解决这个问题。SimPoint方法试图去识别程序中重复的，较长的，大粒度的阶段，基于在单个阶段总计的性能来预测架构的性能。本文研究试图比较SimPoint对SPEC CPU 2006基准测试包和SPEC CPU 2000的准确率，使用SimPoint来研究两个基准测试包的大粒度阶段。我们发现，要准确的预测SPEC CPU 2006中的程序的行为，所需要的仿真点的数量并没有显著增加，尽管其数据占用空间和动态指令数明显更多了。我们还发现，两个基准测试包的程序，在对整体行为有明显贡献的阶段数量上，有很相似的特征，进一步强调了，在相似的准确率下需要的仿真点的数量上，两个基准测试包的相似性。

## 1. Introduction

The SPEC CPU 2006 suite, the latest SPEC CPU benchmark suite provided by the Standards Performance Evaluation Corportation [7], consists of 12 integer and 17 floating point programs. The programs have significantly larger dynamic instruction counts and data footprint than the earlier SPEC 2000 benchmark suite [2]. The increased runtime of the programs due to the combined effect of the increased data footprint and dynamic instruction count make detailed simulation on a software-based micro-architectural simulator impractical. The SimPoint technique was proposed by Sherwood et al. [3] as a means of reducing effective run time of programs without inducing significant errors. The technique takes advantage of the fact that programs in the SPEC suite exhibit long repetitive phases of execution. Therefore, simulation of phases that are representative of these repeating phases should suffice towards providing a reasonably accurate prediction of performance of the architecture. The methodology actually uses a proxy for the unique phases, called simulation points, the details of which shall be presented in Section II.

SPEC CPU 2006包是SPEC提供的最新的SPEC CPU基准测试，由12个整数和17个浮点程序组成。该程序中的动态指令数量和数据占用空间，比之前的SPEC 2000基准测试包明显更大。由于数据占用空间和动态指令数量增加，导致程序的运行时间变长，使基于软件的微架构仿真器进行的详细仿真变得不切实际。Sherwood等[3]提出了SimPoint技术，是一种可以显著降低程序运行时间的方法，而且不会带来显著的误差。这种技术利用的事实是，SPEC包中的程序有很长的重复阶段执行。因此，对这些重复的阶段的有代表性的仿真阶段，应当可以满足提供一个合理准确的架构性能预测。这种方法实际上使用了这些独特阶段的代理，称为仿真点，其细节在第2部分给出。

The accuracy of the SimPoint methodology depends on the number of unique phases that are specified to be uncovered. Too many simulation points could result in longer simulation time and/or resources, and too few would result in large errors. Since SPEC CPU 2006 has at least an order of magnitude larger number of dynamic instructions than SPEC CPU 2000, one might expect that it would have a larger number of unique phases and hence require larger number of simulation points. Unfortunately, it is hard to know how many simulation points would suffice for low error in prediction without doing a validation of the methodology. This validation process would involve running the program in its entirety and comparing the results thus obtained with those obtained through the use of the SimPoint methodology.

SimPoint方法的准确率依赖于独特阶段的数量。太多仿真点会导致更长的仿真时间和/或资源，太少会导致更大的误差。由于SPEC CPU 2006的动态指令数比SPEC CPU 2000要多至少一个数量级，所以可能会认为，其独特阶段的数量也会更大，因此需要更多数量的仿真点。不幸的是，要知道多少仿真点能满足预测结果的低误差，必须要对该方法进行实际核实。这个核实过程需要完整运行整个程序，比较其结果，得到使用SimPoint的结果。

Since the process of identifying SimPoints is ISA independent, validation on an x86 machine would validate the process across any other ISA. We compare the number of simulation points required for producing accurate results with SPEC CPU 2006 with those obtained using SPEC CPU 2000 and compare the results. The major contributions of this work are:

由于识别SimPoint的过程是与ISA无关的，在x86机器上进行核实，也就核实了其他ISA的过程。我们比较了SPEC CPU 2006产生精确结果所需的仿真点数量，与使用SPEC CPU 2000得到的数量。本文的主要贡献如下：

1) Validation of SimPoint for SPEC CPU 2006. This will provide justification for the use of the technique to accelerate simulation using this benchmark suite. 在SPEC CPU 2006上核实SimPoint。这会提供使用这种技术来加速仿真的理由。

2) Contrasting the number of unique phases required for accurate simulation of programs in SPEC CPU 2006 with that of SPEC CPU 2000. It is impossible to know how many simulation points are required for low error without such a study. Also, we study the similarity in the distribution of phases in terms of their contribution towards overall execution. 比较对SPEC CPU 2006进行程序精确仿真所需的独特阶段的数量，和SPEC CPU 2000所需的。只有进行研究，才能知道需要多少仿真点。同时，我们研究阶段的分布的相似性，研究其对整体执行的贡献。

We expect that our work will provide a reference for architects to make an informed decision about the number of simulation points that would be needed to have low error while using the SimPoints methodology with SPEC CPU 2006.

我们期望，我们的工作会为架构师提供了一个参考，在对SPEC CPU 2006使用SimPoint时，究竟需要多少仿真点才能得到较低的误差。

## 2. Background

We use the PinPoints tool provided by Intel Corporation in order to identify the simulation points in the program. PinPoints uses the PIN tool to extract program characteristics from x86 binaries, which can then be used by the SimPoint tool. Thus, it eliminates the need for a functional simulator by allowing code to run directly on real hardware. The details of the SimPoint methodology and the PinPoints tool are presented below.

我们使用Intel提供的PinPoints工具来识别程序中的仿真点。PinPoints使用PIN工具来从x86 binaries中提出程序特征，然后可以送入SimPoint工具中。因此，这就不需要功能仿真器了，使代码可以直接在真实的硬件中运行。SimPoint和PinPoints工具的细节在下面给出。

### 2.1. SimPoint

The SimPoint methodology provides a means of identifying and isolating unique phase behavior that exist in many programs. A phase may be thought of as a region of execution when the program execution is stable - the program exhibits a relatively constant CPI, cache misses etc. The SimPoint methodology involves uncovering all phases in the dynamic execution stream, grouping similar phases together, and picking a representative phase from each group.

SimPoint提供了一种方法，可以将很多程序中存在的独特阶段行为识别并孤立出来。一个阶段可以认为是程序执行稳定时的一个执行区域，程序表现出相对稳定的CPI，cache misses等。SimPoint方法在动态执行流中找到所有阶段，将类似的阶段聚类到一起，从每个组中选择一个代表性的阶段。

The first step in this process involves slicing the dynamic execution trace into chunks of a fixed size. Large slices may fail to capture certain short-lived phase behavior but can generate reasonably accurate results without the need to warm up the caches. Reduction in the size of slices makes each simulation point susceptible to errors due to cold cache misses, but will identify phases of shorter duration, and may therefore lead to better simulation accuracy.

这个过程中的第一个步骤是，将动态执行痕迹切片成固定大小的块。大的切片可能无法捕获到存活很短的阶段行为，但可以在不需要warmup caches的情况下生成相对精确的结果。降低切片的大小，使每个仿真点因为cold cache misses更容易出现错误，但会识别更短持续时间的阶段，因此会带来更好的仿真准确性。

The next step is to identify characteristics that can be used to measure similarity between slices. Towards this end, the SimPoints procedure creates Basic Block Vectors (BBV) for each slice. Each BBV contains the product of the number of times the basic block was executed and the number of instructions in that basic block. This is used to group similar slices together. A representative slice is then chosen from amongst them, which is called a simulation point or SimPoint. Simulation of these slices followed by a weighted addition of results obtained through this process may be used to predict the behavior of the entire program on the architecture. Clustering algorithms may be used to achieve this goal; however, highly dimensional data causes these algorithms to run very slowly. The basic block vectors are therefore normalized and random linear projection is used to reduce the dimensionality of the basic block vectors down to 15. K-means clustering is now used to form clusters of similar slices. K-means algorithm involves assigning a set of points randomly as cluster centers in the multi-dimensional space, assigning each slice to the closest center, and then iteratively assigning cluster centers to the centroid of each such cluster. The algorithm terminates once there is no change in cluster center position or a maximum count is reached. Since K-means algorithm requires the number of centers to be known a priori, the algorithm is run multiple times with multiple centers, and Bayesian Information Criterion (BIC) is used to pick the smallest value of K that produces the best fit for the clustering.

下一个步骤是，找出可用于度量切片间相似性的特征。为此，SimPoint过程为每个切片创建BBV。每个BBV包含的是基础块执行的次数，和基础块中包含的指令数量的乘积。这用于将类似的切片分组到一起。然后从中选择一个代表性的切片，称为一个仿真点，或SimPoint。这些切片的仿真，然后对这个过程得到的结果进行加权求和，可以用于预测整个程序在这个架构上的行为。聚类算法可以用于得到这个目标，但是，高维数据导致这些算法运行很慢。BBV因此要归一化，然后进行随机线性投影以让BBV降维到15。然后使用k-means聚类算法以形成类似切片的聚类。k-means算法的过程是，在高维空间中随机设一些点作为聚类核心，将每个切片归属到最近的中心，然后迭代的将聚类中心设为每个这样的聚类的重心。如果聚类中心位置停止变化，或达到最大迭代次数，那么算法就停止。由于k-means算法需要已知中心数量，算法用多个中心运行多次，使用BIC来选择最佳适配聚类的最小的K值。

At the end of the process, a representative slice is picked from each cluster, which is called a simulation point. Each SimPoint is weighted based upon the size of the cluster it represents. Now detailed simulation needs to be done starting at the SimPoints, over the length of the slice. A weighted addition of the parameter of interest (such as CPI) is used to predict the parameter of a complete simulation. Since the simulation points are independent of one another, they can be run in parallel, resulting in a significant reduction of simulation time.

在这个过程的最后，从每个聚类中选择出了一个代表性的切片，称为一个仿真点。每个SimPoint的加权是基于其代表的聚类的大小。现在在每个SimPoints的开始进行详细的仿真，运行的长度就是切片的长度。感兴趣参数（如CPI）的加权求和，用于预测完整仿真的参数。由于仿真点一个一个是互相独立的，所以可以并行运行，仿真时间会得到显著的降低。

Sherwood et al. use slices of 100 million instructions, with up to 10 simulation points in order to validate the procedure using the SimpleScalar simulator running SPEC CPU 2000 Alpha binaries, which resulted in an average IPC error of 3% [3]. The process was also verified for L1 and L2 cache miss rate and branch prediction accuracy.

Sherwood等使用1亿条指令的切片大小，最多10个仿真点，使用SimpleScalar仿真器运行SPEC CPU 2000的Alpha binaries核实这个过程，得到的平均IPC误差为3%。这个过程也验证了L1和L2 cache miss率和分支预测准确率。

### 2.2. PinPoints

PIN [6], [9] is a dynamic instrumentation tool targeted at Intel Xscale, x86 and IA64 platforms. It provides a rich set of APIs that can be used to study various characteristics of program behavior at the level of the ISA. Since the program is instrumented dynamically and runs natively on hardware, using PIN provides orders of magnitude of speedup over a functional simulator. PinPoints [8] is a tool built on top of PIN that applies the SimPoint technique to programs compiled for the aforementioned ISAs. Since the programs that constitute SPEC CPU 2006 has dynamic instruction counts of over one trillion, a detailed microarchitectural simulator would take several months, if not more than a year to simulate a complete run of each program. In order to check the applicability of SimPoints for SPEC CPU 2006, we would require a detailed run of each program in the benchmark suite, which is quite infeasible. We therefore use the PinPoint technique to validate SimPoints as a technique for evaluation of new architectural features through detailed simulation.

PIN是一种动态工具，用于Intel XScale，x86和IA64平台。它提供了丰富的APIs，可以用于研究ISA级别的各种程序行为特征。由于程序是动态进行的，在硬件上本地执行，使用PIN比一个功能仿真器要快好几个数量级。PinPoints是在PIN上构建的一个枸橘，将SimPoint技术应用到为之前提到ISAs编译的程序。由于构成SPEC CPU 2006的程序的动态指令数量超过了一万亿，一个详细的微架构仿真器要运行几个月甚至一年来运行完整的程序。为检查SimPoint对SPEC CPU 2006的可应用性，我们会需要基准测试包中每个程序的细节运行，这是很不可行的。我们因此使用PinPoint技术，来核实SimPoint是一种通过详细的仿真，来评估新的架构特征的技术。

PinPoints provides tools to analyze the dynamic instruction trace of the program and produce a basic block vector, which can be used to identify simulation points using the SimPoint methodology. The identified simulation points can then be used to simulate various microarchitectural features such as cache hierarchies and branch predictors, using PIN. These simulation points can also be used in a detailed x86 simulator to predict parameters such as CPI, cache miss rates and branch predictor accuracies. Using a maximum of 10 simulation points with each slice of size 250 million instructions, they reported CPI errors of less than 10%, on multiple processor configurations. The procedure was also verified by comparing the results obtained by executing identified simulation points on a detailed microarchitectural simulator with a complete run on real hardware. In this case too, the procedure compared favorably, with low errors on most programs.

PinPoints中的工具可以用来分析程序的动态指令迹，产生BBV，可以用于使用SimPoint技术来识别仿真点。识别出的仿真点可以用于使用PIN来仿真多个微架构特征，比如缓存层次结构和分支预测器。这些仿真点也可以用在详细的x86仿真器中，来预测CPI、cache miss rates和分支预测器准确率这样的参数。使用最多10个仿真点，每个切片的大小2.5亿条指令，他们对多个处理器配置给出的CPI错误率小于10%。这个过程也进行了验证，在一个详细微架构仿真器上执行了识别的仿真点，得到的结果与在真实硬件上的完整执行结果进行了比较。在这个case中，这个过程比较结果还不错，在多数程序中的错误率都很低。

## 3. Evaluation Setup and Methodology

The programs in the benchmark suite are first profiled using the PinPoints tool in order to generate the BBVs, which are then processed using the SimPoint methodology to obtain the simulation points and their corresponding weights. We compare the errors obtained from predicting CPI, L1 and L2 cache misses using the SimPoints methodology for SPEC CPU 2006 and SPEC CPU 2000, for the same maximum number of simulation points. In order to do this, we would need data on CPI, L1 and L2 cache misses per kilo-instruction (MPKI) for the complete run of each program in the benchmark suites as compared to the weighted aggregate of these respective metrics measured at each simulation point, over the length of a SimPoint slice. We also need to select a maximum possible number of simulation points for the K-means clustering algorithm. The smaller the maximum, the greater is the potential error due to the shortage of representative simulation points. The program could have had many repetitive phases, but by selecting a low maxK, we may force it to compromise in its selection of representative points. In order to minimize this effect, we fix the maximum number of simulation points (maxK value) to 30. As can be seen in Table I and Table II, most programs are well short of the maximum number of simulation points.

基准测试包中的程序，首先使用PinPoints工具进行性能评估，以生成BBVs，然后使用SimPoint处理，以得到仿真点及其对应的权重。我们将SimPoints应用于SPEC CPU 2006和SPEC CPU 2000，使用相同数量的仿真点最大数量，比较其预测CPI，L1和L2 cache misses得到的错误。为此，我们需要基准测试包中每个程序完整运行在CPI，L1和L2 cache misses per kilo-instruction(MPKI)上的数据，和在每个仿真点上（长度为一个SimPoint切片）这些度量的加权和。我们还需要为K-means聚类算法选择最大数量的仿真点。最大值越小，可能的误差就会越大，因为缺少代表性的仿真点。程序可能有很多重复的阶段，选择较低的maxK，我们迫使其选择代表性的点以折中。为最小化这个效果，我们固定仿真点的最大数量(maxK)为30。如表I和表II所示，多数程序都远没有达到仿真点的最大数量。

We use a SimPoint slice of 100 million instructions, which is the same size as that used by Sherwood et al.[3] while evaluating the SimPoints methodology. The rationale behind the use of large slices is that they minimize the effect of cold misses in the cache for each simulation point and therefore, warming up of the caches prior to execution of each simulation point is unnecessary [12]. We use pfmon [10] to measure both the overall metrics as well as the metrics at each simulation point. Pfmon reads the performance monitoring counters of the processor. We run each program one at a time on an Intel Pentium4 machine with hyper-threading, running at a clock frequency of 2.8 GHz. It is equipped with a 16kB, 8-way set associative L1-D cache and 1MB, 8-way set-associative L2 cache, both having 64 byte lines. We use Linux kernel 2.6.23.1, patched and recompiled to support pfmon. The SPEC benchmark binaries are compiled using gcc 4.2 in base configuration. Since simulation of all 55 program-input combination of SPEC CPU 2006 programs is impractical, we use the results of clustering techniques applied to SPEC CPU 2006 by Phansalkar et al. [14] to pick one representative input for the program. A complete run of each benchmark program is first performed, and the aggregate information of instructions retired and the number of clock cycles consumed in the process is directly reported using pfmon. Then a second run is performed in which the corresponding counter is sampled using pfmon is sampled every 100 million instructions. The difference between the value at the start of the simulation point and the subsequent sample is used to compute the number of clock cycles elapsed during that simulation slice, taking counter overflows into account. This process effectively provides a warmedup cache for each simulation point. However, since a large simulation point is not significantly affected by the state of the cache, we do not expect our results to be far from what would have been obtained using a cold cache. A similar approach was used by Patil et al. [8] in their work to validate PinPoints. We run each of the above simulations three times and pick the lowest value obtained, in order to minimize errors due to interference from other processes that may have been running at the time. The process is repeated to obtain the L1 and L2 Misses Per Kilo Instructions (MPKI).

我们使用的一个SimPoint切片大小为1亿条指令，这与Sherwood等[3]所用的相同。使用大型切片的原因是，这样可以对每个仿真点最小化cache的cold misses的效果，因此，在每个仿真点的执行前，对cache进行warmup是不必要的。我们使用pfmon来测量整体的度量，以及在每个仿真点上的度量。Pfmon读取处理器的性能监控计数。我们在一台Intel Pentium4机器上，每次运行一个程序，机器带有超线程，运行时钟频率为2.8GHz。机器有一个16kB，8路组相连L1-D cache和1MB，8路组相连的L2 cache，都有64 byte的cachelines。我们使用Linux Kernel 2.6.23.1，打了补丁并重新编译以支持pfmon。使用gcc 4.2在基础配置下编译了SPEC基准测试包的binaries。由于对SPEC CPU 2006中所有55对程序-输入的仿真是不切实际的，我们使用Phansalkar等[14]应用到SPEC CPU 2006的聚类结果，对每个程序选择一个代表。每个基准测试程序的一次完整运行首先进行，使用pfmon来直接报告这个过程中retired指令的聚集信息和消耗的时钟数量。然后运行第二次，使用pfmon每1亿条指令采样对应的Counter。在仿真点开始和后续的样本的值的差异，用于计算在仿真切片过程中消耗的时钟周期数量。这个过程为每个仿真点有效的提供了预热过的cache。但是，由于一个大型仿真点并没有受cache的状态显著影响，我们并不期望我们的结果会与使用cold cache得到的结果有很大不同。Patil等[8]在其工作中使用了类似的方法来核实PinPoints。我们运行上面每个仿真三次，选择得到的最低值，以最小化同时运行的其他过程干扰导致的误差。这个过程重复进行，以得到L1和L2 MPKI。

## 4. Results

We begin by analyzing the differences between the two benchmark suites in terms of the number of simulation points that were produced. This data is useful in order to estimate the amount of additional simulation time, if any, required to run a program from benchmark suite as compared to the other. We then contrast the number of simulation points that have significant weights associated with them, and the number of such simulation points that contribute 90% of the total weights. This analysis will provide useful information on the nature of phase behavior in programs of both benchmark suites, and their differences, if any. We finally compare the error for the two benchmark suites in order to determine the need for additional simulation points for SPEC CPU 2006, considering its vastly increased size over SPEC CPU 2000.

我们分析两个基准测试包之间，在产生的仿真点数量之间的差异。这个数据是有用的，可以估计如果要运行基准测试包中的一个程序额外的仿真时间，与其他的进行比较。我们然后比较有显著权重的仿真点数量，以及贡献了90%的总计权重的这样的仿真点的数量。这个分析会就两个基准测试包程序的阶段行为的本质给出有用的信息，以及其差别。我们最后比较两个基准测试包的误差，以确定对SPEC CPU 2006的额外仿真点的需求，因为对SPEC CPU 2000大小增加了很多。

### 4.1. Comparison of Simulation Points for SPEC CPU 2006 and SPEC CPU 2000

The number of simulation points and their weights, generated for each program in SPEC CPU 2006 and SPEC CPU 2000 can be used to compare their overall behavior. Table I lists the total number of simulation points and the total number of dynamic instructions for the SPEC CPU 2006. Most of the programs had less than 21 simulation points, with only four exceeding this value. The table also contains the dynamic instruction count as obtained using pfmon. The number of simulations points and number of instructions may vary depending on which input file is used for the programs.

We analyze the weights of the simulation points to evaluate the phase behavior of programs. A large weight on a simulation point would imply that the program spends a longer fraction of time in that phase of execution. Calculix has a dominant simulation point that accounts for 42% of the execution, and four simulation points together account for almost 80% of the execution. This indicates that there is very low diversity in the behavior of this program. CactusADM has an even more dominant simulation point, accounting for 73% of the total weight, and three simulation points accounting for over 90% of the execution. For such programs, a smaller maxK value may yield acceptable results. Programs such as soplex, namd, milc, zeusmp and leslie3d have a fairly regular distribution of weights and hence need more number of simulation points for better accuracy. Many programs, such as perlbmk, lbm and cactusADM have a substantial number of simulation points with very low weights. While the existence of these phases indicate diversity in program behavior, they do not contribute significantly to the overall CPI. Such programs can be simulated accurately using a fewer number of simulation points. For instance, cactusADM has 21 simulation points, but only 10 of them individually account for more than 1% of the execution, while the rest combined contribute approximately 0.5% to the total. This analysis indicates that we can reduce the number of simulation points used by the programs and in many cases, achieve high accuracies. Only programs that have a regular distribution of phases, so as to not have a dominant phase would need additional phases. This information can be used to adaptively fix the number of simulation points that would yield acceptable results during simulation, saving on resources and/or total simulation time.

While programs may have a large number of phases, many have only a few dominant phases. These phases contribute most significantly towards the overall behavior of the program and hence are assigned the highest weights in the SimPoint methodology. It may be possible that most SPEC CPU 2000 programs have very few such dominant simulation points, and hence there is no significant loss in accuracy even if the more infrequent phases are ignored, whereas CPU 2006 may not. We would therefore need to address the question whether a reduction in the number of simulation points (maxK) would result in different behavior as far as errors are concerned. A significant discrepency in the distribution of weights in one benchmark suite over another would suggest that one suite requires more simulation points than the other.

We therefore analyze the weights of programs in CPU 2000 suite, much in the same way as we did earlier for SPEC CPU 2006. We find that the overall trends are very similar in both. There is a similar mix of some programs having few, dominant simulation points, and others having simulation points with similar weights. For instance, apsi has a very dominant simulation point which contributes 50% of the total weight. Six simulation points in sixtrack contribute over 98% of the execution. However, the other programs have weights that are close to one another. Programs such as bzip-graphic and equake have a number of simulation points with relatively similar weights. Hence these programs would have higher errors if the number of simulation points were reduced.

In order to gauge the number of phases that contribute significantly to the overall program behavior, we identify the 10 most dominant simulation points of the benchmarks in both suites, based on their weights. The higher the cumulative weight for these simulation points, the greater the influence of the 10 simulation points on program behavior, and potentially, the better the accuracy when maxK is reduced to 10. We find that 10 simulation points cover at least 70% of the total weight for 17 out of a total of 25 SPEC CPU 2000 programs evaluated. 8 of these programs have over 90% of their total weights covered by 10 simulation points. A similar analysis for CPU 2006 reveals that 22 out of a total of 28 programs cover at least 70% of the total weights, and that 10 of these have over 90% of their total weight represented by 10 simulation points. For both benchmark suites, majority of the programs ranged between 75% and 85% of the maximum, for 10 simulation points. This suggests that both programs have similar distribution of weights for simulation points. This result is important because it suggests that reducing the number of simulation points will not decrease accuracy on one benchmark suite versus the other. It also provides us with a first-order approximation of the effect of reducing the number of simulation points without actually having to simulate for multiple max-K values.

We also try to identify the number of simulation points that combined, constitute 90% of the overall program execution. The program spends a significant amount of time in these phases. A fewer number of simulation points required to achieve this number would indicate fewer significant phases of program execution. We find that the average number of simulation points required to achieve 90% of the total weight in CPU 2006 suite is 13.07, with a standard deviation of 4.3, whereas that for CPU 2000 is 13.4, with a standard deviation of 4.4. The values for each benchmark can be found in Table I and II under the 90 percentile points column. Our analysis suggests that not only are the number of simulation points similar, but the weights of these phases are also similar in distribution. Hence, a reduced number of simulation points would result in inaccuracies not dissimilar to those observed for CPU 2000.

### 4.2 Comparison of Errors in CPI and Cache Misses

Figures 1(a) and 1(b) plot the CPI obtained from a complete run as compared to the results obtained from the weighted addition of the CPI of each simulation point. The highest error for SPEC CPU 2006 is obtained for GemsFDTD, at 10.24%, and the average error is 2.45%. The highest error for SPEC CPU 2000 is 10.7%, obtained for gcc-166. The average error obtained is 2.15%. This validates the efficacy of the SimPoints procedure for SPEC CPU 2006. We may conclude that the errors for both benchmark suites are very similar, for the same maximum number of simulation points. Despite the large increase in the dynamic instruction count of SPEC CPU 2006 programs, an increase in the number of simulation points over SPEC CPU 2000 is unnecessary.

Figure 1(c) plots the error in misses per kilo-instructions (MPKI) for L1 cache, using the SimPoints methodology, for CPU 2006. The average error obtained for L1 MPKI on SPEC CPU 2006 is 3.55%. This compares favorably with the average error of 6.51% for CPU 2000, as shown in Figure 1(d). The large average error is predominantly due to a relatively higher error of 31.57% on gzip-graphic. In absolute terms, this is a result of an error of 9 misses per kilo-instructions. The graphic input set is also responsible for the third-highest error value of 18% in bzip2. The error in MPKI here is 2.95. Barring these exceptions, the overall L1 MPKI results are very similar.

In the average error between the predicted and actual MPKI for SPEC CPU 2006 is 12.62%, as shown in Figure 1(e). The highest error in L2 cache MPKI is for gobmk (115%). When compared to SPEC CPU 2000 (Figure 1(f)), we obtain an average error of 23.77% and a maximum error of 271% for wupwise. The percentage values appear to be high, but this is largely due to the fact that L2 MPKI for all programs tends to be a very small value, and in many cases, zero. Hence, depending on the actual MPKI, a small deviation may show up as a large percentage, or vice versa. This is indeed true for both gobmk and wupwise. In absolute terms, the highest error is for xalancbmk (0.41) and art-2 (2.9) for SPEC CPU 2006 and SPEC CPU 2000 respectively. Since misses do not necessarily cause stalls in superscalar processors, simulation points with higher inaccuracy in predicting L2 misses may not necessarily also have higher error in CPI prediction . Clearly, the reverse of this is also true – not all programs with high cache accuracy will provide high accuracy with CPI. Since the simulation point cannot possibly be identical to all other phases it is designated to represent, differences in instruction mix and data access contribute to errors.

These results are interesting, as they indicate that even though there has been orders of magnitude increase in the dynamic instruction counts and data footprints of programs in SPEC CPU 2006 over CPU 2000, for the same number of maximum simulation points, the error rates are very similar. It suggests that the number of phases in the programs that constitute the benchmark suite have not changed, in spite of an increase in the data footprint and dynamic instruction count. Even for programs with dynamic count exceeding 3000 billion instructions, the total number of simulation points required for generating accurate results remains small. Calculix represents a good example for the repetitive phase behavior for which the SimPoints technique was devised: it has the highest dynamic instruction count of approximately 8500 billion instructions, and yet can be represented with just 10 simulation points with an error of only 3% for CPI. The fact that the number of simulation points required for accurate simulation remains practically unchanged is important for performance evaluation studies using the SimPoint methodology. Researchers may use these results to make an educated choice on the number of simulation points required for the SPEC CPU 2006 program that is of interest.

It is also interesting to note that overall, the CPI in both benchmark suites is similar. This is, in part, due to the fact that the L2 miss rates in both benchmark suites are very similar as well. A detailed analysis of this is beyond the scope of this paper. This observation seems to agree with the findings of Gove [1], who performed a comparative study of the working set sizes of SPEC CPU 2006 and CPU 2000. Gove notes that the increase in memory footprint for integer programs had not resulted in a proportionate increase in the working set size (WSS) of all programs in the SPEC CPU 2006.

We may therefore conclude that SPEC CPU 2006 produces similar accuracies with similar number of simulation points, as compared to SPEC CPU 2000. Despite the significantly larger run-time for programs in SPEC CPU 2006 suite, our study indicates that there is no significant increase in simulation time, if the SimPoint methodology is adopted.

## 5. Related Work

Sherwood et al [3], [11] presented the SimPoints technique and validated it for SimpleScalar using Alpha binaries. Patil et al [8] use Pin on IA64 to generate BBVs that may be used to identify simulation points using the SimPoint methodology. Both these have been covered in detail in Section II. Therefore, for the sake of brevity, we shall not repeat it here. Some comparative studies between the two benchmark suites have been done. Henning presented studies on the memory footprint of SPEC CPU 2000 [13] and SPEC CPU 2006 [2], in which it was noted that SPEC CPU 2000 was designed to target systems with 256MB of memory, whereas SPEC CPU 2006 targets a memory footprint of 900MB. Gove [1] analyzes the working set size (WSS) of SPEC 2006 programs and concludes that overall, the WSS of SPEC CPU 2000 benchmarks is not significantly different from that of SPEC CPU 2006. He notes that with the exception of mcf in CPU 2006, all integer programs in both suites have a WSS of less than 256MB. However, for floating point programs, there is significant increase in the WSS, with around 25% exceeding 256MB. He also notes that some floating point programs have very low working set sizes. We are unaware of any other comparative studies involving SPEC CPU 2000 and 2006.

## 6. Summary

In this work, we apply the SimPoints procedure to SPEC CPU 2006, using PinPoints. Our results indicate that, for a maxK of 30, with slice size of 100 million instructions, we observe a low average CPI error of 2.45% and a maximum error of 10.24%. For most programs, the CPI error is well below 3%, indicating that SimPoints can be used to accelerate simulation of single threaded benchmarks on detailed simulators. For a maxK of 30, no program produced more than 26 simulation points, and all but one produced less than 23 simulation points. This fact, along with the the low CPI error indicates that CPU 2006 exhibits long repetitive phases, much like its predecessor, the SPEC CPU 2000 benchmark suite. We compare the results of the SimPoints methodology applied to SPEC CPU 2000 with the results obtained from SPEC CPU 2006, and find that the CPI error percentage is similar, for the same maxK and slice size. Furthermore, we note that the number of simulation points produced are also similar. This indicates that there is no significant increase in the number of unique phases in the new benchmark suite and hence additional simulation points are not required to adequately capture phase program behavior. From a simulation standpoint, this means that the overall time taken to predict performance using either benchmark remains the same, since we have to execute a similar number of fixed-size slices.

We may therefore conlude that while there are some differences in the working set size and memory footprint for many programs in the two benchmark suites, the fundamental behavior of the programs insofar as repetitive phase behavior is concerned, remains largely unchanged. This work provides the scientific community with sufficient information to decide on the number of simulation points that would be necessary for accurate simulation of a program in the SPEC CPU 2006 suite.