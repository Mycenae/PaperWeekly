# Panoptic Studio: A Massively Multiview System for Social Interaction Capture

Hanbyul Joo et. al.  Carnegie Mellon University

## 0. Abstract

We present an approach to capture the 3D motion of a group of people engaged in a social interaction. The core challenges in capturing social interactions are: (1) occlusion is functional and frequent; (2) subtle motion needs to be measured over a space large enough to host a social group; (3) human appearance and configuration variation is immense; and (4) attaching markers to the body may prime the nature of interactions. The Panoptic Studio is a system organized around the thesis that social interactions should be measured through the integration of perceptual analyses over a large variety of view points. We present a modularized system designed around this principle, consisting of integrated structural, hardware, and software innovations. The system takes, as input, 480 synchronized video streams of multiple people engaged in social activities, and produces, as output, the labeled time-varying 3D structure of anatomical landmarks on individuals in the space. Our algorithm is designed to fuse the “weak” perceptual processes in the large number of views by progressively generating skeletal proposals from low-level appearance cues, and a framework for temporal refinement is also presented by associating body parts to reconstructed dense 3D trajectory stream. Our system and method are the first in reconstructing full body motion of more than five people engaged in social interactions without using markers. We also empirically demonstrate the impact of the number of views in achieving this goal.

我们提出了一种方法，来捕获在社交互动的一群人的3D运动。捕获社交互动的核心挑战是：(1) 遮挡是实用的，经常的；(2) 要在足够进行群组社交的大型空间中，测量微小的运动；(3) 人们的外观和配置变化是海量的；(4) 在身体上附着标记会激发互动的本质。全景录像室是围绕这个主题组织的系统，即社交互动应当通过大量视角的感知分析的整合来进行测量。我们提出了一种按照这个原则设计的模块化的系统，包含集成的结构、硬件和软件创新。系统以正在社交活动的多人的480个同步的视频流为输入，输出为标注的随着时间变化在空间中单个人的解剖结构特征点的3D结构。我们的算法设计用于，通过从低层外观线索逐渐生成骨骼建议，在多视角中融合很弱的感知过程，通过将身体部位与重建的密集3D轨迹流，给出了时域提炼框架。我们的系统和方法是第一个重建超过5个社交互动的人群的完全的身体运动的，不使用标记点。我们还通过经验证明了，视角的数量对得到这个目标的影响。

## 1. Introduction

Despite the fundamental role nonverbal cues play in enabling social function [1], [2], the protocol underlying this communication is poorly understood — Sapir [3] called it “an elaborate code that is written nowhere, known to no one, and understood by all”. Some structures of this code have been identified through observational study, such as reciprocity [4] or synchrony [5]. However, systematic studies of such phenomena have remained almost entirely focused on the analysis of facial expressions, despite emerging evidence [6], [7] that facial expressions provide a fundamentally incomplete characterization of nonverbal communication. One proximal cause for this singular focus on the face is that capturing natural social interaction presents challenges that current state-of-the-art motion capture systems simply cannot address. This paper describes an approach to capture social signals in natural human interactions, presenting fundamental innovations that span capture design architecture, motion reconstruction algorithms, and a large scale dataset capturing more than 3 hours of group interaction scenes using 521 heterogeneous sensors.

尽管非语言的线索在实现社交功能中是基础角色，在这种沟通之下的规则理解的非常差 - Sapir[3]称之为“一种复杂的代码，没有在任何地方写出来，所有人都不知道，但是所有人都理解”。这种代码的结构通过观察研究得以识别，比如互助或同步。但是，这种现象的系统性研究几乎全都在关注面部表情的分析，但是越来越多的证据显示，面部表情所提供的非语言交流的特征只是很小一部分。这种对面部的单一的聚焦的一个主要原因是，捕获自然的社交互动会给出下面的挑战，即目前最好的运动捕获系统仍然不能处理。本文描述了一种方法，来捕获自然人类互动的社交信号，提出了基础的创新，可以扩展捕获设计架构，运动重建算法，和利用521个异类传感器捕获的多于3个消失的群组互动场景的大规模数据集。

There are four principal challenges in capturing social signaling between individuals in a group: (1) social interactions have to be measured over a volume sufficient to house a dynamic social group, yet subtle details of the motion where important social signals are embedded must be captured; (2) strong occlusions emerge functionally in natural social interactions (e.g., people systematically face each other while interacting, bodies are occluded by gesticulating limbs); (3) human appearance and configuration variation is immense; and (4) social signaling is sensitive to interference — for instance, attaching markers to the face or body, a pre-capture model building stage, or even instructing each individual to assume a canonical body pose during an interaction, primes the nature of subsequent interactions.

在群组中捕获个人之间的社交信号，有4个主要挑战：(1)社交互动的测量，要在挺大的一个空间中，足以容纳一群动态的社交群体，而运动的微妙细节必须被捕获到，因为包含重要的社交信号；(2)在自然社交互动中，很强的遮挡是很有用的（如，人们会系统性的互相面对同时互动，打手势的时候，四肢会遮挡身体），(3)人们的外观和配置变化是非常多的，(4)社交信号对干扰非常敏感，比如，将标记贴到面部或身体上，这是预捕获模型的构建阶段，或甚至告诉每个人在互动中要使用标准身体姿态，都会使后续的互动的本质有所改变。

In this paper, we present a system designed to address these issues, with integrated innovations in hardware design, motion representation, and motion reconstruction. The organizing principle is that social motion capture should be performed by the consolidation of a large number of “weak” perceptual processes rather than the analysis of a few sophisticated sensors. The large number of views provide robustness to occlusions, provide precision over the capture space, and facilitate the boosting of weak 2D human pose detectors into a strong 3D skeletal tracker without any prior about the scenes and subjects. In particular, our contributions include:

本文中，我们提出了一个系统，设计用于解决这些问题，在硬件设计、运动表示和运动重建上都有创新。组织原则是，社交运动捕获要由大量弱感知过程的联合进行，而不是分析少数几个复杂的传感器。大量视角会对遮挡很稳健，在捕获空间中精度会较高，促进2D人体弱姿态检测器提升为3D强骨骼跟踪器，不需要对场景和目标的任何先验知识。特别的，我们的贡献包括：

1) **Modularized Hardware**: We present the modular design of a massively multiview capture consisting of 480 VGA cameras, 31 HD Cameras, and 10 Kinect v2 RGB+D sensors, distributed over the surface of geodesic sphere with a 5.49m diameter (sufficient to house social groups).

模块化的硬件：我们给出了多视角捕获系统的模块化设计，包括480个VGA相机，31个高清相机，10个Kinect v2 RGBD传感器，在一个半径5.49米的网格球顶的表面上（足以容纳社交群体）。

2) **3D Motion Reconstruction Algorithm for Interaction Capture**: We present a method to automatically reconstruct full body motion of interacting multiple people. Our method does not rely on a 3D template model or any subject-specific assumption such as body shape, color, height, and body topology. Yet, our method works robustly in various challenging social interaction scenes of arbitrary number of people, producing temporally coherent time-varying body structures. Furthermore, our method is free from error accumulation and, thus, enables capture of long term group interactions (e.g., more than 10 minutes).

捕获互动的3D运动重建算法：我们提出一种方法，自动重建互动的多个人的全身体运动。我们的方法不依赖于3D模板模型，或任何目标特定的假设，比如身体形状，色彩，高度和身体拓扑结构。但是，我们的方法在各种有挑战性的任意数量人的社交互动场景中工作的都很稳定，可以给出时间上连续的随时间变化的身体结构。而且，我们的方法不会积累误差，因此，可以捕获长期的群体互动（如，超过10分钟）。

3) **Social Interaction Dataset**: We publicly share a novel dataset which is the largest in terms of the number of views (521 views), duration (3+ hours in total), and the number of subjects in the scenes (up to 8 subjects) for full body motion capture. Our dataset is distinctive from the previously presented datasets in that ours captures natural interactions of groups without controlling their behavior and appearance, and contains motions with rich social signals as shown in Figure 1 (right). The system described in this paper provides empirical data of unprecedented resolution with the promise of facilitating data-driven exploration of scientific conjectures about the communication code of social behavior. All the data and output are publicly shared on our website.

社交互动数据集：我们公开了一个新的数据集，在视角上是最多的（521个），时长上是最长的（总计超过3小时），场景中的目标数量也是最多的（最多8个），而且是进行全身体运动捕获。我们的数据集与之前发布的数据集是有明显区别的，因为我们捕获的自然群体交互，不需要控制他们的行为和外观，包含丰富的社交信号运动，如图1所示（右）。本文描述的系统给出的经验数据，分辨率前所未有，有希望促进数据驱动的社交行为通信规则的科学推测的探索。所有数据和输出都在我们的网站上公开可用。

## 2. Related Work

### 2.1 Automated Group Behavior Analysis

Over the last decade, there has been increasing interest in automatically analyzing multiple people’s social interaction using multiple camera sensors. Several datasets recording unstructured social scenes are presented, where multiple people (from 5 to 14 subjects) naturally communicate without restriction in their behavior [8], [9], [10]. In contrast to the scenes captured in structured environment such as round-table meetings [11], the subjects in such unstructured environments show richer social signals in their body motion, locations, and orientations. However, due to the unconstrained nature, it is challenging to measure their body motion because of severe occlusions among people. Thus, the previous work in this area usually aims to get coarse measurements (e.g., quantized body/head orientation), and they rather focus on higher level social understanding from the coarse measurements, such as F-formation detection [11] and personality predictions [8], [10]. None of the previous work in this domain addresses reconstructing full body skeletal motion of individuals in such challenging scenarios, although rich social signals are embedded in those subtle motions.

在过去十年中，使用多个相机传感器自动分析多人社交互动的兴趣有很大增长。提出了几个录制无结构的社交场景的数据集，其中多个人（5-14）很自然的沟通，没有对其行为进行限制。比较起来，在结构化环境中捕获的场景，比如圆桌会议，在这种无结构的环境中，目标在身体运动，位置和方向中展现出更丰富的社交信号。但是，由于无结构的本质，测量其身体运动是很有挑战性的，因为人们之间的遮挡很严重。因此，在这个领域中之前的工作通常目标是得到粗糙的测量（比如，量化的身体/头部方向），而且聚焦在从这些粗糙的测量中的更高层次的社交理解，比如F-formation检测[11]和性格预测[8,10]。在这个领域中，之前的所有工作都没有处理在这样有挑战的场景中，进行单人全身体谷歌运动重建的问题，虽然在这些微妙的运动中包含着丰富的社交信号。

### 2.2 Markerless Motion Capturing Using Multiple View Systems

In computer vision, there has been a large number of approaches to measure 3D structure and motion of dynamically moving people using multiple camera sensors. Kanade et al. [12] pioneered the use of multi-view sensing systems to “virtualize” reality, using 51 cameras mounted on a geodesic dome of 5 meters in diameter. A number of systems were subsequently proposed to produce realtime virtualizations [13], [14], [15], [16]. Vlasic et al. [17] recovered detail by applying multi-view photometric stereo constraints using a system with 1200 lights on a dome and eight cameras. More recently, a multimodal multi-view stereo system fusing 53 RGB cameras and 53 infrared cameras has been proposed to reconstruct high quality 3D virtual characters [18].

在计算机视觉中，有大量方法使用多个相机传感器测量动态移动的人的3D结构和运动。Kanade等[12]率先使用多视角感知系统对现实虚拟化，使用了51个相机安装在半径为5米的网格球顶。后续也提出了几个系统，以产生实时的虚拟化。Vlasic等通过应用多视角光度立体视觉约束，使用了有1200个光源和8个相机的网格球顶，恢复了细节。最近，提出了一个多模态多视角立体视觉系统，融合了53个RGB相机和53个红外相机，以重建高质量3D虚拟角色[18]。

Other methods explicitly tackle the markerless motion capture by producing 3D skeletal structures over time similar to the marker-based counterparts [19], [20], [21], [22], [23], [24], [25], [26], [27], [28], [29], [30], [31]. The methods deform pre-defined articulated templates of fixed topology to recover the details that were subsampled or occluded in the set of views at a time instant. These methods require an offline method to generate a rigged 3D model for each individual, and the quality of the template is important to achieve high accuracy. The template models need to be aligned at the initial frame to be tracked, and usually a predefined pose (such as a T-pose) are assumed and performed by all individuals. The methods in this area fundamentally suffer from topological changes restricted by the template model, and, similar to other tracking methods, error accumulation is a critical issue in tracking for long durations. Although the 3D template-based method shows good performance — and has become a standard in markerless motion capture approaches — the requirement of a high quality 3D template for each individual limits the practicality of the method, especially in our scenario where dozens of individuals are involved, as the method does not scale well to multiple people. Previous work is demonstrated on a single actor with few exceptions [32], [33]. For example, it is required to segment image cues per subject to track them independently as in [33], which becomes more complicated if a large number of people are involved, as in our scenes. It should be noted that none of the previous markerless motion capture approaches focus on capturing non-verbal social behaviors of naturally interacting multiple people.

其他方法通过生成随着时间的3D骨骼结构，来显式的处理无标记运动捕获，与基于标记的工作类似。这些方法对预定义的固定拓扑的铰接模板进行形变，以恢复在某一时刻下多个视角中采样或遮挡的细节。这些方法需要一个离线方法来生成每个人的装配的3D模型，模板的质量对于获得高准确率非常重要。模板模型必须在初始帧进行对齐，然后进行跟踪，而且对所有个体需要预先指定一个姿态（比如T-pose）并进行处理。这个领域中的方法都受到模板模型限制的拓扑变化困扰，而且，与其他跟踪方法类似，误差类似是长时间跟踪的一个关键问题。虽然3D的基于模板的方法展现了很好的性能，已经称为无标记运动捕获方法的标准，但对每个个体都需要一个高质量的3D模板，这限制了该方法的实际应用，尤其是在涉及到十几个人的场景中，因为该方法对于很多人效果并不好。之前的工作在单个人物中进行了展示。比如，[33]需要分割每个目标的图像线索，以独立的跟踪他们，如果涉及到很多人，就会变得非常麻烦，就像我们的场景一样。应当指出，之前的无标记运动捕获算法，没有一个聚焦在捕获多人自然互动的非语言社交行为上。

### 2.3 Pose Detection Based Approach

Over the last few years, single view 2D pose estimation method shows great advances based on Convolutional Neural Network framework with large scale human pose datasets [34], [35]. The state-of-the-art method [35] shows an excellent performance in various environments with varying subject’s shape, appearance, and scales. Recently, a few methods facilitate body pose detectors in multiple views to reconstruct 3D body poses [36], [37], [38], [39], [40]. To infer 3D skeletal parameters from 2D pose detection cues, unary and pairwise terms are defined based on the pre-training data of joint length, relative joint angles, and body colors. The methods are performed at each time independently in fewer camera settings, and thus they typically suffer from motion jitter. Although the results show potential in general environment settings (e.g., outdoors), the methods in this category do not yet reach similar quality compared to the 3D template-based approaches.

在过去几年中，单视角2D姿态估计方法有了很大进展，主要是在大规模人体姿态数据集上基于CNN框架的工作。目前最好的方法[35]在各种环境中，在目标的形状、外观和尺度上的变化中，展现了很好的性能。最近，几种方法推动了多视角的人体姿态检测器，以重建3D人体姿态。为从2D姿态检测线索中推断3D骨骼参数，在基于关节长度，相对关节角度和身体色彩的预训练数据上，定义了一元和二元项。这些方法在很少的相机的设置中，在每个时间独立的进行运算，因此他们通常都受到运动颤抖困扰。虽然结果在通用环境中设置中展现出了潜力（如，室外），这种类别的方法还没有达到基于3D模板的方法的性能。

## 3. Modularized Hardware Design

For social motion capture, we design a massively multiview system with heterogeneous sensors including 480 VGA cameras, 31 HD cameras, 10 Kinects. The large number of cameras at unique viewpoints provide a large volume with robustness against occlusions, and allow no restriction for view direction of the subjects. The HD views provide details for the scene. Multiple Kinects provide initial point clouds to a generate dense trajectory stream.

对于社交运动捕获，我们涉及了一个大规模多视角多样传感器系统，包含480个VGA相机，31个高清相机，10个Kinects。大量在独特视角的相机，提供了大量数据，对遮挡非常稳健，对目标的视角方向没有限制。HD视角为场景提供了很多细节。多个Kinects提供了初始的点云，以生成密集的轨迹流。

### 3.1 Structural Design

The physical frame of the studio is a variant of a face-transitive solid called a truncated pentagonal hexecontahedron. This particular structure was selected because it has among the largest number of transitive faces of any geodesic dome [41]. The transitivity of the faces enables the modular architecture, and ensures that the structure remains easy to upgrade and customize with different panels of the same configuration. The structure has a diameter of 5.49m and a total height of 4.15m. The floor of the dome is 1.40m below the center to increase access to the edges, as shown in Figure 2. In all, the structure consists of 6 pentagonal panels, 40 hexagonal panels, and 10 trimmed base panels.

工作室的物理框架是可传递面的固体的变体，称为截断五边形面体。选择了这个特殊的结构是因为，在任意网格球顶中，有最大数量的可传递面。面的可传递性使这种模块化的结构成为可能，并确保了这个结构很容易升级和定制，不同的面板有相同的配置。这个结构的半径为5.49米，高度为4.15米。球的地板比中央要低1.40m，以增加对边缘的访问，如图2所示。总体上，这个结构包含6个五边形面板，40个六边形面板，和10个修剪过的基础面板。

Our design was modularized so that each hexagonal panel houses a set of 24 VGA cameras. To determine the placement of the VGA cameras, we initialized their positions by tessellating the hexagon face into 24 triangles and using this initialization to define a 3-neighborhood structure shown in the bottom right illustration of Figure 2. Using this neighborhood structure and the initialization we determine the placement of the cameras over the geodesic dome by minimizing the difference in angles between all neighbors of every camera,

我们的设计是模块化的，这样每个六边形面板都可以安装24个VGA相机。为确定VGA相机的摆放，我们将其位置初始化为，将六边形面镶嵌到24个三角形中，使用这种初始化来定义一个3邻域结构，如图2的右下所示。使用这种邻接结构和初始化，我们确定了相机在网格球顶的摆放，最小化了每个相机与所有邻接的角度间的差异

$$\{ θ_{ij} \} = argmin_{\{ θ_{ij} \}} \sum_{p=1}^P \sum_{i=1}^N \sum_{j∈N(i)} \sum_{k∈N(i)\neq j} (r(θ_{ij}|p)-r(θ_{ik}|p))^2$$

where P = 20 is the number of panels, N = 24 is the number of cameras in each panel, N(·) is the neighborhood of a camera, r(·|p) is a function transforming the angle on a reference panel to the p-th panel. The cameras sample the span of the vertical axis of the space and sample 48.71◦ of the horizontal axis. With this distribution, the minimum baseline between any VGA camera and its nearest three neighbors is 21.05cm.

其中P=20是面板的数量，N=24是每个面板上相机的数量，N(·)是一个相机的邻接，r(·|p)是一个函数，将一个参考面板上的角度变换到第p个面板上的角度。相机对空间的竖直轴的跨度进行采样，并对水平轴的48.71◦进行采样。有了这个分布，任意VGA相机与其邻接的3个相机之间的最小基准是21.05cm。

The 31 HD cameras are installed at the center of each hexagonal panel, and 5 projectors are installed at the center of each pentagonal panel. Additionally, a total of 10 Kinect v2 RGB+D sensors are mounted at heights of 1 and 2.6 meters, forming two rings with 5 evenly spaced sensors each. The interior and exterior of our system are shown in Figure 2.

31个HD相机是安装在每个六边形面板的中间的，5个投影仪安装在每个五边形的中央。另外，总计10个Kinect v2 RGB+D传感器安装在高度1米和2.6米，形成了两个环形，每个都有5个均匀间隔的传感器。系统的内部和外部如图2所示。

### 3.2 System Architecture

Figure 3 shows the architecture of our system. The 480 cameras are arranged modularly with 24 cameras in each of 20 standard hexagonal panels on the dome. Each module in each panel is managed by a Distributed Module Controller (DMC) that triggers all cameras in the module, receives data from them, and consolidates the video for transmission to the local machine. Each individual camera is a global shutter CMOS sensor, with a fixed focal length of 4.5mm, that captures VGA (640 × 480) resolution images at 25Hz.

图3给出了系统的架构。480个相机是模块化放置的，在球顶20个标准六边形面板上，每个有24个相机。在每个模板上的每个模块都有一个分布式模块控制器(DMC)管理，触发模块中的所有相机，从中获得数据，并将视频传输到本地机器上。每个单个相机都是一个全局卷帘CMOS传感器，固定焦距4.5mm，以25Hz拍摄VGA分辨率(640 × 480)的图像。

Each panel produces an uncompressed video stream at 1.47 Gbps, and thus, for the entire set of 480 cameras the data-rate is approximately 29.4 Gbps. To handle this stream, the system pipeline has been designed with a modularized communication and control structure. For each subsystem, the clock generator sends a frame counter, trigger signal, and the pixel clock signal to each DMC associated with a panel. The DMC uses this timing information to initiate and synchronize capture of all cameras within the module. Upon trigger and exposure, each of the 24 camera heads transfers back image data via the camera interconnect to the DMC, which consolidates the image data and timing from all cameras. This composite data is then transferred via optical interconnect to the module node, where it is stored locally. Each module node has dual purpose: it serves as a distributed RAID storage unit and participates as a multicore computational node in a cluster. All the local nodes of our system are on a local network on a gigabit switch. The acquisition is controlled via a master node that the system operator can use to control all functions of the studio.

每个面板都产生一个不压缩的1.47Gbps的视频流，因此，完整的480个相机的数据速度是大约29.4Gbps。为处理这个流，系统流程被设计成模块化的通信和控制结构。对每个子系统，时钟生成器发送帧计数器，触发信号，和像素时钟信号到每个与面板关联的DMC。DMC使用计时信息来初始化和同步模块中所有相机的捕获。在触发和曝光时，24个相机的每个都通过相机与DMC的连接传回图像数据，确保了所有相机的图像数据和计时信息。这种混合数据然后通过光学连接传输到模块节点，并进行本地存储。每个模块节点都有双重目标：分布式RAID存储单元，以及一个集群中的多核计算节点。我们系统的所有局部节点都在一个gigabit的本地网络中。数据获取是由主节点控制的，系统操作员可以用之控制整个工作室的所有功能。

Similar to the VGA cameras, HD cameras are modularized and each pair of cameras are connected to a local node machine via SDI cables. Each local node saves the data from two cameras to two RAID storage units respectively.

与VGA相机类似，HD相机也是模块化的，每对相机都通过SDI线连接到一个局部节点。每个局部节点从两个相机中存储数据，分别到两个RAID存储单元。

Each RGB+D sensor is connected to a dedicated capture node that is mounted on the dome exterior. To capture at rates of approximately 30 Hz, the nodes are equipped with two SSD drives each and store color, depth, and infrared frames as well as body and face detections from the Kinect SDK. A separate master node controls and coordinates the 10 capture nodes via the local network.

每个RGB+D传感器都连接到一个专用捕获节点，安装在球顶的外部。为以大约30Hz的频率捕获，每个节点需要安装2个SSD驱动器，来存储色彩、深度和红外帧，以及Kinect SDK检测到的人体和人脸。另外的一个主节点通过局部网络控制和协调10个捕获节点。

### 3.3 Temporal Calibration for Heterogeneous Sensors

Synchronizing the cameras is necessary to use geometric constraints (such as triangulation) across multiple views. In our system, we use hardware clocks to trigger cameras at the same time. Because the frame rates of the VGA and HD cameras are different (25 fps and 29.97 fps respectively) we use two separate hardware clocks to achieve shutter-level synchronization among all VGA cameras, and independently among all HD cameras. To precisely align the two time references, we record the timecode signals generated from the two clocks as a single stereo audio signal, which we then decode to obtain a precise alignment at sub-millisecond accuracy.

同步相机是必须的，以跨越多个视角使用几何约束（比如三角定位）。在我们的系统中，我们使用硬件时钟来同时触发相机。因为VGA和HD的帧率是不同的（分别是25和29.97fps），我们使用两个不同的硬件时钟来在所有VGA相机之间获得快门级的同步，并与所有HD相机独立。为精确的对齐两个时间参考，我们记录了由两个时钟生成的时间代码信号，作为单个立体视觉声音信号，我们然后可以进行解码，得到精确的亚毫秒级准确率的对齐。

Time alignment with the Kinect v2 streams (RGB and depth) is achieved with a small hardware modification: each Kinect’s microphone array is rewired to instead record an LTC timecode signal. This timecode signal is the same that is produced by the genlock and timecode generator used to synchronize the HD cameras, and is distributed to each Kinect via a distribution amplifier. We process the Kinect audio to decode the LTC timecode, yielding temporal alignment between the recorded Kinect data—which is timestamped by the capture API for accurate relative timing between color, depth, and audio frames—and the HD video frames. Empirically, we have confirmed the temporal alignment obtained by this method to be of at least millisecond accuracy.

与Kinect v2流(RGB and depth)的时间对齐，是通过硬件的一个小改动获得的：每个Kinect的麦克风阵列进行了重新接线，以记录一个LTC时间代码信号。这个时间代码信号和genlock生成的，和用于同步HD相机的时间代码生成器的一样，通过一个分布放大器与每个Kinect进行分布连接。我们处理Kinect的声音，以解码LTC时间代码，得到录音的Kinect数据和HD视频帧的时间对齐，录音的Kinect数据由捕获的API打上了时间标签，以在色彩，深度和音频帧之间进行准确的相对计时。

### 3.4 Spatial Calibration

We use Structure from Motion (SfM) to calibrate all of the 521 cameras. To easily generate feature points for SfM, five projectors are also installed on the geodesic dome. For calibration, they project a random pattern on a white structure (we use a portable white tent), and multiple scenes (typically three) are captured by moving the structure within the dome. We perform SfM for each scene separately and perform a bundle adjustment by merging all the matches from each scene. We use the VisualSfM software [42] with 1 distortion parameter to produce an initial estimate and a set of candidate correspondences, and subsequently run our own bundle adjustment implementation with 5 distortion parameters for the final refinement. The computation time is about 12 hours with 6 scenes (521 images for each) using a 6 core machine. In this calibration process, we only use the color cameras of Kinects. We additionally calibrate the transformation between the color and depth sensor for each Kinect with a standard checkerboard pattern, placing all cameras in alignment within a global coordinate frame.

我们使用SfM来校准所有521个相机。为给SfM生成特征点，在网格球顶上还安装了5个投影仪。为校准，它们在一个白色结构上投影了一个随机模式（我们使用了一个可移动白色帐篷），通过在球顶中移动这个结构，来捕获多个场景（一般是3个）。我们对每个场景分别进行SfM，通过合并每个场景的所有匹配，进行了一个bundle调整。我们使用VisualSfM软件[42]（有1个形变参数），来产生初始估计和一个候选对应性集，然后运行我们自己的bundle调整实现，带有5个形变参数，进行最后的精调。使用6核机器，对6个场景（每个521幅图像），计算时间大约是12小时。在这个校准过程中，我们只使用了Kinects的色彩相机。我们还对每个Kinect用一个标准的棋盘格校准了色彩和深度传感器，将所有相机都在一个全局坐标帧上进行对齐。

## 4. Method Overview and Notation

Our algorithm is composed of two major stages. The first stage takes, as input, images from multiple views at a time instance (calibrated and synchronized), and produces 3D body skeletal proposals for multiple human subjects. The second stage further refines the output of the first stage by using a dense 3D patch trajectory stream [43], and produces temporally stable 3D skeletons and an associated set of labeled 3D patch trajectories for each body part, describing subtle surface motions.

我们的算法由两个主要阶段组成。第一阶段的输入为，在同一时刻多视角的图像（经过校准和同步），对多个人类目标生成3D人体骨架建议。第二阶段使用密集3D块轨迹流提炼第一阶段的输出，生成时间上稳定的3D骨骼，和相关的每个身体部位的标注3D块轨迹，描述微妙的表面运动。

In the first stage, a 2D pose detector [35] is computed independently on all 480 VGA views at each time instant t, generating detection score maps for each body joint (see Fig. 4b). The 2D score maps for each body joint j ∈ {1, · · · , J} are combined into a 3D score map $H_j(Z)$ by projecting a grid of voxels $Z ∈ R^3$ onto the 2D score maps and computing an average 3D score at each voxel (subsection 5.1).

在第一阶段，在每个时刻t，对所有480个VGA的视角，独立计算2D姿态检测器，对每个人体关节生成检测分数图（见图4b）。对每个人体关节点j ∈ {1, · · · , J}的2D分数图，将体素网格$Z ∈ R^3$投影到2D分数图上，在每个体素上计算一个平均3D分数，结合成一个3D分数图$H_j(Z)$。

Our approach then generates several levels of proposals, as shown in Figure 4. A set of node proposals $N_j$ for each joint j is generated by non-maxima suppression of the 3D score map $H_j(Z)$, where the k-th node proposal $N^k_j ∈ R3$ is a putative 3D position of that anatomical landmark. Similarly, the set of part proposals is denoted by $P_{uv}$, where u and v are joints and (u, v) ∈ B is the set of body parts or bones composing a skeleton hierarchy. The k-th part proposal, $P^k_{uv} = (N^{k_u}_u, N^{k_v}_v) ∈ R^6$, is a putative body part connecting two node proposals, $N^{k_u}_u$ and $N^{k_v}_v$, where the index k enumerates all possible combinations of $k_u$ and $k_v$. As the output of the first stage, our algorithm produces skeletal proposals; we refer to the k-th proposal as $S^k = \{P^k_{uv}\}_{uv∈B}$. A skeletal proposal is generated by finding an optimal combination of part proposals using a dynamic programming method under the score function defined in subsection 5.3. Here, we abuse the notation to have $P^k_{uv}$ refer to the optimally assigned part u, v of skeleton k (the superscript k is understood to be the optimal mapping, from context). After reconstructing skeletal proposals at each time t independently, we associate skeletons from the same identities across time and generate skeletal trajectory proposals $˜S^k(t) = {˜P^k_{uv}(t)}_{uv∈B}$, where $˜P^k_{uv}(t)$ is a part trajectory proposal, a moving part across time, with k similarly overloaded to denote the optimal associations determined in each frame t.

我们的方法然后生成几个层次的建议，如图4所示。通过对3D分数图$H_j(Z)$的非最大抑制，对每个关节j生成节点建议集$N_j$，其中第k个节点建议$N^k_j ∈ R3$是那个解剖标志点的推定的3D位置。类似的，部位建议集用$P_{uv}$，其中u和v是关节，(u, v) ∈ B是身体部位集，或由骨架层次结构组成的骨骼结构。第k个部位建议，$P^k_{uv} = (N^{k_u}_u, N^{k_v}_v) ∈ R^6$，是连接两个节点建议的推定身体部位，$N^{k_u}_u$ and $N^{k_v}_v$，其中索引k枚举了所有可能的$k_u$和$k_v$的组合。作为第一阶段的输出，我们的算法生成了骨骼建议；我们称第k个建议为$S^k = \{P^k_{uv}\}_{uv∈B}$。在5.3中定义了一个分数函数，使用动态规划，通过找到一个最优的部位建议的组合，从而生成骨骼建议。这里，我们让$P^k_{uv}$也表示骨架k的最优指定部位u，v（上标k从上下文中理解为最优映射）。在每个时刻t独立的重建了骨骼建议，我们在不同的时间点上从相同的个体中关联骨架，生成骨骼轨迹建议$˜S^k(t) = {˜P^k_{uv}(t)}_{uv∈B}$，其中$˜P^k_{uv}(t)$是一个部位轨迹建议，身体部位在不同时间的运动，k表示在每个帧t中确定的最优的关联。

In the second stage, we refine the skeletal trajectory proposals generated in the first stage using dense 3D patch trajectories [43]. To produce evidence of the motion of different anatomical landmarks, we compute a set of dense 3D trajectories $F = \{f_i\}^{N_F}_{i=1}$, which we refer to as a 3D patch trajectory stream, by tracking each 3D patch independently. Each patch trajectory $f_i$ is initiated at an arbitrary time (every 20th frame in our results), and tracked for an arbitrary duration (30 frames backward-forward in our results) using the method of Joo et al. [43]. Our method associates a part trajectory $˜P^k_{uv}$ with a set of patch trajectories $F^k_{uv}$ out of F, and these trajectories determine rigid transformations, $T(t+1|t) ∈ SE(3)$, between any time t to t+1 for this part. These labeled 3D trajectories associated to each part provide surface deformation cues and also play a role in refining the quality by reducing motion jitter, filling missing parts, and detecting erroneous parts.

在第二阶段，我们使用密集3D块轨迹[43]，对第一阶段生成的骨骼轨迹建议进行提炼。为生成不同结构特征点的运动证据，我们计算密集3D轨迹集合$F = \{f_i\}^{N_F}_{i=1}$，我们称之为3D块轨迹流，对每个3D块进行独立追踪。每个块的轨迹$f_i$在任意时刻进行初始化（在我们的结果中，是每20帧），并跟踪任意的时长（在我们的结果中，是向前向后30帧），使用的Joo等[43]的方法。我们的方法将部位轨迹$˜P^k_{uv}$与块轨迹的集合$F^k_{uv}$相关联起来，这些轨迹确定了一个对这个部位在时刻t和t+1之间的刚性变换，$T(t+1|t) ∈ SE(3)$。这些标记的与每个部位相关的3D轨迹，给出了表面形变线索，在通过降低运动抖动、填充缺失的部位、检测错误的部位来提炼质量上也扮演了一个角色。

## 5. The First Stage: Skeletal Proposals Generation

Our algorithm integrates 2D pose detections across the many views of our massively multiview system, fusing simple 2D cues to estimate 3D skeletal poses at each time instance. While detections in any single view may be incomplete or inaccurate—typically due to occlusions—we find that aggregating these cues across many views yields very stable results. Our method is simple, yet robust thanks to the large number of views. In contrast, prior marker-less motion capture methods are typically “model-dependent”, requiring a 3D template model to constrain shape deformations, a motion model to constrain temporal deformations, and a relatively complex energy function minimization that trades off each of these priors (e.g., [19], [31], [40]). Our method in this stage is essentially based on triangulating detections at a single time instance, and, thus, does not suffer from error accumulation or drift. It does not require a 3D template model, prior assumptions about the subject or the motion, or an initial alignment for tracking. In this section, we describe how the proposals are generated and built up from 2D cues.

我们的算法集成了我们的大规模多视角系统的多个视角的2D姿态检测结果，在每个时刻将简单的2D线索融合以估计3D骨骼姿态。在任意单个视角的检测结果会是不完整的或不准确的，一般是因为遮挡，我们发现，将多个视角的这些线索结合到一起，会得到非常稳定的结果。我们的方法很简单，但是很稳健，主要是因为大量的视角。对比之下，之前的无标记运动捕获方法一般是依赖于模型的，需要一个3D模板模型来约束形状形变，一个运动模型来约束时间形变，和一个相对复杂的能量函数最小化，在各种先验之间折中。我们在这个阶段的方法基本上是基于，在单个时间时刻，对检测结果进行三角定位，因此，不会受到误差累积或漂移的困扰。不需要3D模板模型，也不需要关于目标或运动的先验假设，也不需要初始对齐以进行跟踪。本节中，我们描述了怎样从2D线索中生成和构建建议。

### 5.1 3D Node Score Map and Node Proposals

A single-view 2D pose detector is computed on all VGA views at each time instant, and is used to generate 2D pose detections and per-joint score maps in each image. Because the first stage of our method is performed at each time independently, we will consider a fixed time instant t, and drop the time variable for clarity. We use the detector of Wei et al. [35] without additional training. The method of [35] requires bounding box proposals for each human body as initialization, thus, we first apply a person detector similar to R-CNN [44], and run the pose detector on the detected person proposals represented as bounding boxes. Each 2D skeleton detection i in a camera view c is denoted by $s^c_i ∈ R^{2×15}$, and is composed of 15 anatomical landmarks or nodes (3 for the head/torso and 12 for the limbs), also referred to as joints. The position of the j-th node of the i-th person detection is denoted by $s^c_{ij}$ ∈ R2. The method of [35] also provides a score map representing the per-pixel detection confidence for each node $s^c_{ij}$, which we denote as $h^c_{ij}(z)$ ∈ [0, 1], where z ∈ R^2 indexes 2D image space. We also compute a merged score map by taking the maximum across all person detections at each pixel, $h^c_j(z) = max_i h^c_{ij}(z)$. Merged score maps of example views are shown in Figure 5.

在每个时刻，对所有VGA视角，都计算一个单视角2D姿态检测器，并用于在每幅图像中生成2D姿态检测结果和每个关节的分数图。因为我们方法的第一阶段是在每个时刻独立进行的，我们会考虑一个固定时刻t，并简化起见忽略时间变量。我们使用Wei等[35]的检测器，不进行额外的训练。[35]中的方法需要每个人体的边界框建议，以作为初始化，因此，我们首先用类似R-CNN的算法进行人体检测，在检测到的人体建议的边界框上进行姿态检测。在一个相机视角c中的每个2D骨骼检测结果i表示为$s^c_i ∈ R^{2×15}$，是由15个解剖特征结构或节点组成的（头/躯体3个，四肢12个），也就是关节。第i个人的第j个节点位置，表示为$s^c_{ij}$ ∈ R2。[35]方法还给出了分数图，表示每个节点$s^c_{ij}$在每个像素上的检测置信度，我们表示为$h^c_{ij}(z)$ ∈ [0, 1], 其中z ∈ R^2对2D图像空间进行索引。我们还计算了一个融合的分数图，对所有人体检测结果在每个像素上取最大值，$h^c_j(z) = max_i h^c_{ij}(z)$。融合分数图的视角例子如图5所示。

To combine 2D node score maps from multiple views into 3D, we generate a 3D score map for each node using a spatial voting method. We first index the 3D working space into a voxel grid (4cm in our implementation), and compute the node-likelihood score of each voxel by projecting the center of the voxel to all views and taking the average of the 2D scores at the projected locations. The 3D score map $H_j(Z)$ for a node j at the 3D position Z ∈ R^3 is defined as

为将多个视角的2D节点分数图结合成3D的，我们使用空间投票方法对每个节点生成了一个3D分数图。我们首先将3D工作空间索引成一个体素网格（在我们的实现中是4cm），将每个体素的中心投影到所有视角，计算投影位置的2D分数的平均，得到每个体素的节点可能性。对一个节点j在3D位置Z ∈ R^3上的3D分数图$H_j(Z)$定义为

$$H_j(Z) = \frac {1}{|V(Z)|} \sum_{c∈V(Z)} h_j^c(P_c(Z))$$(1)

where $P_c(·) ∈ R^2$ denotes projection into camera c, V(Z) is the set of cameras where the 3D location Z is visible, and |V(Z)| is the cardinality of the set. Note that the 3D score map for each node is computed separately, producing fifteen 3D score maps at each time instant.

其中$P_c(·) ∈ R^2$表示到相机c的投影，V(Z)是3D位置Z可见的相机集合，|V(Z)|是集合的基。注意，对每个节点的3D分数图是分别计算的，在一个时刻产生15个3D分数图。

From the 3D score map for each node at each time instance, we perform Non-Maxima Suppression (NMS), and keep all the candidates above a fixed threshold τ (we use τ=0.05). The results are shown in the Figure 4c, and the same results color-coded by the node scores are shown in Figure 6. Each node proposal, denoted as $N^k_j$ for the k-th proposal for node j, is a putative candidate for the j-th anatomical landmark of a participant.

从每个时刻每个节点的3D分数图中，我们进行NMS，将在固定阈值τ（我们使用τ=0.05）之上的候选全部保留。结果如图4c所示，同样的结果用颜色代表节点分数，如图6所示。对第j个节点的第k个候选表示为$N^k_j$，每个节点候选都是一个参与者的第j个解剖特征结构的一个推断候选。

### 5.2 Part Proposals

Given the generated node proposals, we infer part proposals by estimating connectivity between each pair of nodes that make up a possible body part. The 2D detector [35] uses appearance information during the inference, and, thus, the result tends to preserve connectivity information (e.g., left knee is connected to the left foot of the same person). Our approach fuses them by voting 2D connectivity into 3D. More specifically, we define a connectivity score between a pair of node proposals by projecting them onto all views, and checking in how many views they are actually connected, i.e., both nodes belong to the same person detection. Formally, the connectivity score of a part $P^k_{uv}$ between two node proposals ($N^{k_u}_u , N^{k_v}_v$), where (u, v) ∈ B, is defined as

给出了生成的节点候选，我们通过在每对能构成可能的身体部位的节点估计其连接性，来推断部位候选部位建议。2D检测器[35]在推理时使用外观信息，因此，结果倾向于保留连接性信息（如，左膝盖连接到同一个人的左脚）。我们的方法通过将2D连接性投影到3D来进行融合。更具体的，我们在一对节点建议之间定义了一个连接性分数，将其投影到所有视角，并检查在多少个视角中它们是实际连接起来的，即，两个节点属于同样的检测到的人。正式的，在两个节点建议($N^{k_u}_u , N^{k_v}_v$)之间的一个部位的连接性分数$P^k_{uv}$定义为，(u, v) ∈ B

$$Φ(P^k_{uv}) = \frac {1}{|V(P^k_{uv}|} \sum_{c∈V(P^k_{uv})} max_i φ^c_{iuv} (P_c(N^{k_u}_u), P_c(N^{k_v}_v))$$

$$φ^c_{iuv}(z_u, z_v) = w^c_{iuv}(z_u, z_v) δ^c_{iuv}(z_u, z_v)$$

where

$$w^c_{iuv}(z_u, z_v) = \frac{1}{2} (h^c_{iu}(z_u) + h^c_{iv}(z_v)), and$$

$$δ^c_{iuv}(z_u, z_v) = \left\{ \begin{matrix} 1 & if h^c_{iu}(z_u)>τ and h^c_{iv}(z_v)>τ \\ 0 & otherwise \end{matrix} \right.$$

Here, $P_c(N^{k_u}_u)$ and $P_c(N^{k_v}_v)$ are the projections of the two nodes of $P^k_{uv}$ in view c, and $V(P^k_{uv})$ is the set of cameras where the 3D part is visible. Intuitively, the part score Φ represents the average connectivity score across all views from all potentially corresponding 2D person detections. Because we do not know the correspondence from 3D parts to 2D person detections, we take the maximum score across all possible detections i in each view. Assuming that the projected part corresponds to the i-th person detection in camera c, the part connectivity score $φ^c_{iuv}$ is defined as the average score of the projected nodes, denoted by $w^c_{iuv}(z_u, z_v)$. The delta function $δ^c_{iuv}$ additionally ensures that $φ^c_{iuv}$ is nonzero only if both projected node locations have a sufficiently high score for the same detection i (i.e., both nodes are detected as part of a single person). An example of computed part scores is shown in Figure 6.

这里，$P_c(N^{k_u}_u)$和$P_c(N^{k_v}_v)$是$P^k_{uv}$的两个节点在视角c上的投影，$V(P^k_{uv})$是3D部位可见的相机的集合。直观上来说，部位分数Φ表示所有可能的对应2D人体检测结果中所有视角的平均连接性分数。因为我们不知道3D部位到2D人体检测结果的对应性，我们取在所有可能的检测结果中在每个视角上的最大分数。假设投影的部位对应着在相机c中的第i个人体检测，部位连接性分数$φ^c_{iuv}$定义为投影节点的平均分数，表示为$w^c_{iuv}(z_u, z_v)$。Delta函数$δ^c_{iuv}$另外确保了$φ^c_{iuv}$只在投影的节点位置对同样的检测i有足够高的分数时才是非零的（即，两个节点都检测为一个人的部位）。计算得到的部位分数的一个例子如图6所示。

### 5.3 Generating Skeletal Proposals by Dynamic Programming

Our method generates skeleton proposals by piecing together the part proposals. Since each skeleton is a tree structure, this can be computed efficiently using Dynamic Programming (DP)—but only for a single person. Therefore, we use DP to greedily find 3D skeletons Sk which maximize the sum of part scores,

我们的方法通过将部位建议拼到一起，生成骨架建议。由于每个骨架是一个树状结构，这可以使用动态规划(DP)进行高效计算，但只是对于单个人。因此，我们使用DP对3D骨架Sk进行贪婪搜索，最大化部位分数之和

$$Θ(S^k) = max_{(k_1, ..., k_J)} \sum_{(u,v)∈B} Φ(P^k_{uv})$$

A skeleton $S^k$ is given by the mapping k → (k1, · · · , kJ), where the J-tuple (k1, · · · , kJ) determines the assignment of node proposals $N^{k_j}_j$ for each joint j in the body. After picking the highest scoring skeleton Θ(Sk), the assigned nodes (k1, · · · , kJ) are removed from the pool of available node proposals and we run DP again to find the next highest scoring skeleton, and so on until all possible skeletons are found.

一个骨架$S^k$由映射k → (k1, · · · , kJ)给出，其中J元组(k1, · · · , kJ)确定了身体上对每个关节j的节点建议$N^{k_j}_j$。选择最高评分的骨架Θ(Sk)，指定的节点(k1, · · · , kJ)从可用的节点建议池中移除，然后我们再次运行DP，以找到下一个最高的评分骨架，重复此步骤，直到找到所有可能的骨架。

One option here would be to threshold the skeleton scores $Θ(S^k)$ at some minimum value to determine valid detections. However, we can do better: each 3D skeleton should be supported by 2D detections, and each 2D detection can correspond to only a single 3D skeleton. This observation is important because the voting used to generate 3D node proposals assigns equal score to all voxels along the line of sight of each 2D detection (Sect. 5.1), and, similarly, the max over detections in the part score Φ(·) makes $Θ(S^k)$ an overestimate.

这里的一个选项是，对骨架分数$Θ(S^k)$以某个最小值来进行阈值处理，以确定有效的检测结果。但是，我们可以做的更好：每个3D骨架都应当有2D检测结果支撑，每个2D检测结果只能对应着一个3D骨架。这个观察是很重要的，因为用于生成3D节点建议的投票对沿着每个2D检测结果视线的所有体素都指定了同样的分数（5.1节），类似的，在部位分数Φ(·)检测结果的最大值，使$Θ(S^k)$成为一个高估。

To avoid this form of double counting, our method places each 3D node $N^k_j$ in skeleton $S^k$ in correspondence with the closest 2D joint detection in each view. For each 3D node $N^k_j$, we create a set of correspondences $C^k_j$ with elements (c, i) such that the distance $||P_c(N^k_j) − $s^c_{ij}$||_2$ is the minimum across all detections i in view c and smaller than δ=10px. Once a 2D correspondence is established, we remove it from the set of available 2D detections, and, as above, this is performed greedily in order of decreasing skeleton score $Θ(S^k)$. Skeletons where the head node has fewer than two correspondences are discarded, i.e., if |$C^k_j$| < 2 for j the head.

为避免这种形式的双重计数，我们的方法使骨架$S^k$中的每个3D节点$N^k_j$，与每个视角中最接近的2D关节检测结果有对应性。对每个3D节点$N^k_j$，我们用元素(c,i)创建了一个对应性集合$C^k_j$，这样距离$||P_c(N^k_j) − $s^c_{ij}$||_2$在视角c中的所有检测结果i中是最小的，而且小于δ=10px。一旦确定了一个2D对应性，我们将其从可用2D检测结果的集合中移除，像上面一样，这个以贪婪的方式进行执行，以降低骨骼分数$Θ(S^k)$。小于2个对应性的头部节点的骨架都丢弃了，即，如果对于头部的j，有|$C^k_j$| < 2。

We additionally use the set of correspondences $C^k_j$ to refine the 3D node locations by minimizing reprojection error. This overcomes the discretization error introduced by the voxel grid resolution. The final 3D node location $\hat N^k_j$ is then

我们另外通过最小化重新投影的误差，使用对应性集合$C^k_j$来提炼3D节点位置。这克服了体素网格分辨率引入的离散化误差。最终的3D节点位置$\hat N^k_j$是

$$\hat N^k_j = argmin_Z \sum_{(c,i)∈C^k_j} ||P_c(Z) - s^c_{ij}||_2$$

The output of the algorithm described in this section is 3D skeletal proposals reconstructed independently at each time instance. After performing this process on all frames, our method associates skeletons from the same identity across time by simply considering spatial distance of the head node. That is, for a $S^{k_1}_t$ reconstructed at time t, we find a corresponding skeleton at $S^{k_1}_{t+1}$ with the closest head node location from $S^{k_1}_t$ within a threshold. To be somewhat robust to missing skeleton detections, our method associates across a window of time. If there is no corresponding skeleton at time t + 1, we also consider the next time t + 2 and find a corresponding skeleton.

本节中描述的算法的输出是，在每个时刻独立重建的3D骨架建议。在所有帧上进行了这个过程后，我们的方法仅仅考虑头部节点的空间距离，将从同样个体中的不同时间上的骨架关联起来。即，对于在时刻t重建的$S^{k_1}_t$，我们在$S^{k_1}_{t+1}$找到一个对应的骨架，有最近的头部位置距离。为对缺失的骨架检测结果有某种稳健性，我们的方法对一个时间窗口中的结果进行关联。如果在时刻t+1没有对应的骨架，我们还会考虑下一个时刻t+2，找到一个对应的骨架。

This first stage of our method is performed without considering any temporal cues. The advantages of this are that the method can easily handle a varying number of people, there is no need to impose priors on the motion or skeletons, and the bulk of the computation is easily parallelized across frames. In many cases, we find that the results from the first stage are already sufficient for many applications. However, the results exhibit some jitter—especially for complex scenes with limited views per person—and missed or noisy detections do not benefit from evidence found in adjacent frames. We address these issues in the second stage of our method.

我们方法的第一阶段没有考虑任何时域的线索。其优势是，我们的方法可以很容易的处理不同数量的人，不需要对运动或骨架施加任何先验，计算量在不同帧间很容易进行并行化。在很多情况中，我们发现，第一阶段的结果对很多应用就已经足够了。但是，结果仍然有一些抖动，尤其是对于复杂的场景，每个人的视角数量有限，缺失的或含噪的检测结果不会从临近帧中的证据中受益。我们在方法的第二阶段处理这些问题。

## 6. The Second Stage: Temporal Refinement and Trajectory Stream Labeling

The per-frame skeletal proposals from the first stage can be improved by using temporal coherence. We use motion cues from a 3D patch trajectory stream: dense 3D point tracks computed by the method of Joo et al. [43]. We find an association between each part trajectory proposal and a subset of the patches in the trajectory stream, and use it to reduce motion jitter, remove false detections, and fill in missing detections. The resulting labeled patch trajectories also capture rich motion information representing the subtle deformations of the surface for each body part (see Fig. 4f).

从第一阶段的每一帧的骨架建议，可以使用时域的连贯性进行改进。我们使用3D块轨迹流的运动线索：由Joo等[43]的方法计算得到的密集3D点的跟踪。我们在每个部位轨迹建议和轨迹流中块的子集之间找到一种关联性，用其来降低运动抖动，去除掉假的检测，填充缺失的检测。得到的标注的块轨迹也捕获了丰富的运动信息，表示了每个身体部位的表面的微小形变。

### 6.1 Patch Trajectory Stream Reconstruction

We can only observe surface motions, not the true motion of the underlying skeleton, so it is not immediately apparent how best to enforce temporal consistency in the motion of body parts. Clothing in particular makes the relationship between surface motion and body parts difficult to model. To keep the use of priors and models to a minimum, we therefore choose to measure surface motion independently from the underlying skeletal motion and postpone all decisions about part-to-surface associations.

我们只能观察表面运动，而不是骨架的真实运动，所以怎样最好的对身体部位的运动施加空域的一致性，这并不是立刻明显的。衣着特别使表面运动和身体部位的关系难以建模。为保持先验和模型的使用达到最小化，我们因此选择从潜在的骨架运动对表面运动独立测量，推迟关于部位到便面关联的所有决定。

To represent surface motion, we use the method of [43] to track a dense 3D patch cloud—a set of points with corresponding surface normal and a small spatial extent, representing the surface locally—and estimate the motion of each of these patches. Instead of generating the initial patches to track using SIFT matching and triangulation (as in [43]), we use the depth maps from our 10 RGB+D sensors to generate an initial set of 3D patches. For a single frame, a dense 3D point cloud is first generated from the depth maps, and planar local patches centered on each point are initialized. The size of patches is manually determined by considering image resolution and fixed for the entire processes at 6cm×6cm. To find the normal of each patch, we apply Singular Value Decomposition (SVD) to the coordinates of points within a neighborhood (determined by Euclidean distance from the center point with the patch size as a threshold), and the least principal axis is selected as the normal direction. The sign of the normal is disambiguated by considering camera visibility.

为表示平面运动，我们使用[43]的方法来跟踪密集3D块云 - 带有对应表面法线的点集和一个小的空间范围，在局部上表示这个表面 - 并估计每个这些块的运动。我们没有生成初始块去用SIFT匹配和三角定位去追踪（就像[43]一样），我们使用的是我们的10个RGB+D传感器的深度图，来生成3D块的初始集。对于单帧，首先从深度图中生成密集3D点云，对以每个点为中心的平面局部块进行初始化。块的大小通过考虑图像分辨率来手动确定，在整个过程中，保持在6cm×6cm。为找到每个块的法线，我们对一个小邻域中的点的坐标使用SVD（邻域由从块大小中间点的欧式距离作为阈值决定），最小主轴选择作为法线方向。法线的符号通过考虑相机的可见性，来去除疑义。

The remainder of the algorithm (3D patch tracking) is as described in [43]. As a brief overview, a patch is represented by a triplet points (the center point, and two orthogonal points on the patch plane), and it is tracked by projecting the triplet points on all views where the target patch is visible. Optical flow tracking is performed in 2D on each point, and the tracked 2D flows are triangulated into 3D. The core idea to fully leverage a large number of views is to reason about the time-varying camera visibility of each patch. The visibility is optimally estimated in a MAP framework that combines photometric consistency, motion consistency, and visibility priors, see [43] for more details. For our results, we initialize a 3D patch cloud every 20th frame, and track them backward and forward for 30 frames in each direction. As output, we obtain a dense 3D patch trajectory stream, $F = \{f_i\}^{N_F}_{i=1}$, where each $f_i(t) ∈ R^3$ is the time-varying position of a tracked patch.

算法的剩余部分（3D块跟踪）如[43]所描述。作为一个简要的概览，一个图像块由一个三元组点来表示（中心点，和在块平面的两个正交点），通过将三元组点投影到所有目标块可见的视角来进行跟踪。光流跟踪在2D的每个点上进行，跟踪的2D流进行三角剖分成3D。核心思想是，完全利用大量视角，是为了对随着时间变化的每个块的相机可见性进行推理。可见性在MAP框架中进行最优估计，结合了光度一致性，运动一致性，和可见性的先验，[43]中有更多细节。对我们的结果，我们每20帧初始化一个3D块的云，并对前后各30帧进行跟踪。作为输出，我们得到了一个密集3D块轨迹流$F = \{f_i\}^{N_F}_{i=1}$，其中每个$f_i(t) ∈ R^3$是随时间变化的跟踪的块的位置。

### 6.2 Associating Part Trajectory Proposals and Trajectory Stream

Part trajectory proposals $˜P_{uv}$ represent the moving body parts of a single person, and are given by the optimal assignment used to generate skeletal trajectory proposals. These part trajectories lack temporal coherence because they are reconstructed independently in each frame. However, the trajectory streams provide evidence of the motion of each limb, and can be used to refine the motion of each body part. We therefore find an association between each part trajectory proposal and a subset of patch trajectories. This can be seen as a semantic labeling of the patch trajectory stream with the corresponding body parts (see Fig.4f).

部位轨迹建议$˜P_{uv}$表示单人的移动身体部位，由用作生成骨架轨迹建议的最优指定给出。这些部位轨迹缺少时域一致性，因为是在每一帧中独立重建的。但是，轨迹流为每个肢体的运动提供了证据，可以用于提炼每个身体部位的运动。我们因此找到每个部位轨迹建议和块轨迹的子集的关联。这可以视为块轨迹流对对应的身体部位的语义标记（见图4f）。

Before performing this association, we first remove erroneous part detections which can readily be identified as outliers. We find that a simple yet robust method is to use the depth maps from the multiple RGB+D sensors. At any time instant, a part can be considered as an outlier if it is outside of every surface in the dense point cloud. We simply test this by checking whether a part proposal is in front of the measured depth in any view, and mark it as erroneous if it is. This is a necessary but not sufficient condition because we test this from only the 10 available depth map views. However, we find that this method works well in practice and is efficient to implement. After identifying these outliers, we remove and treat them as missing data. Then, we can assume that this filtered part trajectory only suffers from relatively small jitter and occasionally missing data.

在进行这个关联之前，我们首先移除错误的部位检测结果，这些都是可以立刻被识别为外点的。我们找到一个简单但稳定的方法，使用多个RGB+D传感器的深度图。在任意时刻，一个部位如果在密集点云的每个表面的外部，就可以被认为是一个外点。我们通过检查一个部位建议在任何视角中都是在测量的深度的前面，来简单的进行测试，如果是的话，就标记其为错误的。这是必须的，但并不是充分的条件，因为我们只从10个可用的深度图的视角来进行测试。但是，我们发现，这个方法在实践中效果很好，实现起来也很高效。在识别了这些外点后，我们将其移除，并认为其是缺失的数据。然后，我们可以假设，这些滤除过的部位轨迹，只会存在相对较小的抖动和偶尔的数据缺失的问题。

We associate a set of patch trajectories with a filtered part trajectory proposal if they move rigidly and the patch normal is a match. Intuitively, the part should be located inside the body surface, and, thus, a vector from the closest point on the part to the patch center should have a similar direction as the patch normal—their inner product should be positive. For a part trajectory proposal, we only consider patches for which the normal satisfies this criterion for the entire duration of the patch trajectory. As additional criteria, we compute a measure of rigidity between a patch trajectory and a part trajectory proposal. We define this as the difference between the minimum and maximum distance between them across all frames t in which they overlap:

如果他们运动是刚性的，块法线是一个匹配，我们就将块轨迹的集合和滤波过的部位轨迹建议关联起来。直观上来说，部位应当定位在身体表面内部，因此，从部位上的一个最近点到块的中央的向量，应当与块的法线有类似的方向 - 其点积应当是正的。对于一个部位轨迹建议，我们考虑的块，只是那些法线在整个块轨迹的时间段内都满足这个条件的。作为额外的规则，我们计算块轨迹和部位轨迹建议之间的刚性的度量。我们将其定义为，在他们有重叠的所有帧t中，其最大距离和最小距离之差：

$$d(f_i, \tilde P^k_{uv}) = max_t l(f_i(t), \tilde P^k_{uv}(t)) - min_t l(f_i(t), \tilde P^k_{uv}(t))$$

where l(·, ·) is the orthogonal distance between the patch center and the line segment of the body part, i.e., 其中l(·, ·)是块中央和身体部位的线段之间的正交距离，即

$$l(f_i(t), \tilde P^k_{uv}(t)) = min_α ||αN_u^{k_u}(t) + (1-α)N_v^{k_v}(t)-f_i(t)||_2$$

Here, the set of time instants t satisfies that both the patch trajectory and part trajectory streams are valid, and only patch trajectories i for which 0≤α≤1 at some time t are considered. Intuitively, this cost approximates how rigidly they move together over time, going to zero for completely rigid motion. Each part trajectory $\tilde P^k_{uv}$ is then associated with a set of patch trajectories $F^k_{uv}$, for which the rigidity cost is less than a threshold, i.e., $F^k_{uv} = \{f_i: d(f_i, \tilde P^k_{uv})≤10cm\}$. If a patch trajectory is selected by multiple body parts (e.g., a static scene as an extreme case), the trajectory is associated with the body part with minimum $max_t l(f_i(t), \tilde P^k_{uv}(t))$ distance. An example of this labeling is shown in Figure 4f.

这里，时刻t的集合满足，块轨迹和部位轨迹流都是有效的，只有在某些时刻0≤α≤1的块轨迹i被考虑。直观上来说，这个代价近似的是，他们随着时间移动的刚性程度，从0到完全刚性的运动。每个部位轨迹$\tilde P^k_{uv}$然后与块轨迹的集合$F^k_{uv}$进行关联，其中刚性代价低于一个阈值，即$F^k_{uv} = \{f_i: d(f_i, \tilde P^k_{uv})≤10cm\}$。如果一个块轨迹被多个身体部位选择（例如，在一个极端情况中，是静态的场景），那么这个轨迹就与距离$max_t l(f_i(t), \tilde P^k_{uv}(t))$最近的身体部位进行关联。这种标注的一个例子，如图4f所示。

### 6.3 Motion Refinement by Associated Patch Trajectories

From the set of patch trajectories $F^k_{uv}$ associated to the part trajectory proposal $\tilde P^k_{uv}$, we can compute the rigid transform between subsequent time instances from t to t+1, T(t+1|t), and, progressively, to any frame t' by concatenating transformations between subsequent frames, so that $T(t'|t) \tilde P^k_{uv}(t)$ represents the propagated part from time t to t'. Using the transformation it is possible to propagate a body part’s position to other time instants. Our method uses the propagated parts to reduce jitter and fill in missing holes by averaging multiple part locations propagated from different time instances. For a target time t, we can produce multiple proposals for the same part, including the proposal from the first stage of our method and propagated parts using the estimated transformations, creating a set

从关联到部位轨迹建议$\tilde P^k_{uv}$的块轨迹集合$F^k_{uv}$中，我们计算后续的时刻从t到t+1的刚性变换，T(t+1|t)，并逐渐的，通过将后续帧的变换进行拼接，到任意帧t'，这样$T(t'|t) \tilde P^k_{uv}(t)$表示从时刻t到t'传播的部位。使用这个变换，可能将一个身体部位的位置传播到任意时刻中。我们的方法使用传播的部位来降低抖动，通过平均多个从不同的时刻传播而来的部位位置，来填补缺失的洞。对一个目标时刻t，我们可以对同样的部位生成多个候选，包括从我们方法第一阶段的建议，和使用估计的变换传播来的部位，这样可以创建一个集合

$$\{ T(t|t-n) \tilde P^k_{uv}(t-n), ..., \tilde P^k_{uv}(t), ..., T(t|t+n)\tilde P^k_{uv}(t+n) \}$$

If there are elements in this set, we take the average. If the set is empty due to consistently severe occlusions, we determine that the part at time t is still missing. In practice, we use n=1. This procedure can also be iterated multiple times (including patch trajectory re-association) to fill in missing parts that are further than n frames from any part proposal. We iterate this refinement until no more missing parts can be filled. After refinement, a node connected to multiple body parts can have different locations corresponding to each of the averaged parts, and we simply take the average to determine the final node locations. It should be noted that our method is different from temporal smoothing (e.g., [40]). Instead, we use an actual measurement of 3D motion rather than impose a motion prior, which prevents over-smoothing even after several iterations.

如果在这个集合中有元素，我们就取其平均。如果由于严重的遮挡，这个集合是空的，我们确定，在时刻t时的部位仍然是缺失的。在实践中，我们使用n=1。这个过程也可以迭代多次（包括块轨迹的重新关联），以填补超过了n帧的对任意部位建议的缺失部位。我们对这个提炼过程进行迭代，直到没有缺失的部位可以填补。在提炼过程后，连接到多个身体部位的一个节点，由于对应着每个平均的部位，可以有不同的位置，我们取其平均，来确定最终的节点位置。需要说明的是，我们的方法与时域平滑（如[40]）是不同的。我们使用3D运动的实际度量，而不是施加一个运动先验，这即使在几次迭代后，也可以防止过度平滑。

## 7. Results

We quantitatively and qualitatively evaluate our method on various sequences captured in the Panoptic Studio. Our dataset includes diverse social games performed by multiple people. In the quantitative evaluation, we empirically show how the large number of views solves the challenging interaction capture problem; we compare performance using varying number of cameras on the scenes with different number of people. In the qualitative evaluation, we demonstrate the “model-free” advantage of our method by showing compelling motion reconstruction results on subjects of diverse appearance, body shapes, and body sizes.

我们将方法在全景工作室中的各种序列上进行定量和定性的测试评估。我们的数据集包括多人进行的各种社交游戏。在定量评估中，我们通过经验展现了，大量视角是怎样求解有挑战的互动捕获问题的；我们对场景中不同数量的人，使用不同数量的相机，来比较其性能。在定性评估上，我们证明了我们方法的无模型优势，在不同外观、身体形状和身体大小的目标上展现了很好的运动重建结果。

### 7.1 Dataset and Capture Procedures

We captured a group of people engaged in social interactions using the Panoptic Studio. To evoke natural interactions, we involved participants in various games: Ultimatum, Mafia, Haggling, and 007-Bang Game. The first two games are used in experimental economics and psychology to study conflict and cooperation, and the latter two games also induce a variety of rich non-verbal signals in participants. Example scenes of each game are shown in Figure 7. Refer to the supplementary material for descriptions of the games and capture procedures. In our captures, subjects were informed of the rules of the game but were otherwise not instructed about how to behave, nor was their clothing or appearance controlled. They were also not initially aware of our research goals to avoid potential biases in their gestures. The scenes in our dataset contain various natural motions which may commonly occur in the interactions of daily life, as shown in Figure 1 and 11.

我们使用全景工作室，捕获在社交互动的一群人。为进行自然的互动，我们让参与者进行多种游戏：Ultimatum, Mafia, Haggling, 和007-Bang游戏。前2个游戏用在试验经济学和心理学中，研究冲突和合作，后两个游戏也包括参与者的各种丰富的非语言信号。每个游戏的例子场景如图7所示。参考附加材料获得游戏和捕获过程的描述。在我们的捕获中，目标会被告知游戏的规则，但怎样表现就不会告知了，其衣着和外观也不受控。他们一开始也不知道我们的研究目标，以避免其姿态的可能偏差。我们数据集中的场景，包含各种自然运动，都是在日常生活中会发生的各种互动，如图1和11所示。

To additionally demonstrate the performance of our system and methods, we capture other challenging sequences, including a group of 8 seated people participating in a discussion (meeting sequence), a mother and a toddler at play (toddler sequence), musical performances with severe occlusions due to the instruments (drummer and cellist), and a sequence featuring various fast motions and challenging postures (dancer).

为额外证明我们系统和方法的性能，我们捕获了其他有挑战性的序列，包括8个坐着的人参与一个讨论（会议序列），一位妈妈和孩子在游戏（幼儿序列），被乐器严重遮挡的乐器表演（鼓手和大提琴手），和一个有各种快速运动和挑战性的姿态的序列（舞者）。

In aggregate, the dataset contains about 198 minutes (∼297K frames) of videos, for a total of about 154 million images. Our dataset is summarized in the supplementary material.

总计，数据集包含198分钟视频（约297K帧），总计大约154 million图像。我们的数据集在附录资料中进行了总结。

The main distinguishing features of this collection compared to previous markerless motion capture datasets are: (1) natural interactions in the scenes showing rich and subtle non-verbal cues, (2) social groups of up to 8 interacting people, and (3) coverage by a large number of views (up to 521). We make all the data available on our website, including all synchronized camera feeds, calibration, 3D pose reconstruction results, and 3D trajectory streams: https://domedb.perception.cs.cmu.edu.

与之前的无标记运动捕获数据集相比，这个集合的主要区别性特色是：(1)场景中的自然互动，展现了丰富和微妙的非语言线索；(2)最多8个人互动的社交群组；(3)由大量视角覆盖（最多521）。我们将所有数据都放在了网站上，包括所有同步的相机反馈，校准，3D姿态重建结果，和3D轨迹流: https://domedb.perception.cs.cmu.edu.

### 7.2 Processing Time

The time to process one minute of data (1500 frames) of 480 VGA views is summarized in Table 2. We use different computing devices for procedures. A machine with Intel i7 3.4GHz processor and 32GB RAM is used for general processing, a GTX Titan X is used for GPU computation, and a cluster server with 400 CPU cores (2.2GHz per processor) is used for trajectory stream reconstruction.

处理480个VGA视角的一分钟数据（1500帧）的时间，在表2中进行了总结。我们对不同过程使用不同的计算设备。通用处理使用的是一个带有Intel i7 3.4GHz的处理器，32GB RAM的计算机，使用了一个GTX Titan X进行GPU计算，轨迹流重建使用的是一个集群服务器，包含400个CPU核（每个处理器2.2GHz）。

In the first stage, most of the time is spent in running the 2D body pose detector. The detector runs at about 5 frames per second on a single GPU, but due to the large number of views (720K images per minute), processing a minute of video takes about 40 hours. In practice, we use multiple GPUs to process multiple images in parallel. In the second stage, the main computational bottleneck is the trajectory stream generation. Although they are tracked in parallel, the running time is long due to the large number of patches at each time. In our experiments, on average 15K patches are tracked per person.

在第一阶段，占用最多处理时间的是2D身体姿态检测器。检测器的运行速度为单GPU 5fps，但由于大量视角（每分钟720K图像），处理一分钟视频大约要用40小时。在实践中，我们使用多个GPUs来并行处理多幅图像。在第二阶段，主要的计算瓶颈是轨迹流生成。虽然是进行的并行跟踪，由于在每个时刻块的数量很大，运行时间仍然是非常长的。在我们的试验中，对每个人跟踪了平均15K个块。

### 7.3 Performance Analysis of The Panoptic Studio

We quantitatively evaluate the performance of our method for the 160226 ultimatum1 sequence by varying the number and type of cameras. We choose the ultimatum sequence because it captures varying number of people (from two to seven people) in each time period, which is suitable to study the relation between scene complexity and the number of cameras needed to reach a desired performance. In this experiment, we only evaluate the first stage of our method.

我们对160226个ultimatum1序列，通过变化相机的数量和类型，量化的评估我们的方法。我们选择ultimatum序列是因为在每个时间间隔内，其捕获的人数数量有变化（从2到7人），对研究场景复杂度和达到理想性能所需要的相机数量的关系非常合适。在这个试验中，我们只评估我们方法的第一阶段。

**Performance using all VGA cameras**: We first quantify the performance of our system when all 480 VGA cameras are used. Due to the absence of ground truth data, we manually annotate the correctness of the reconstructed 3D skeletons by verifying their projections in multiple 2D views. We labeled a 3D joint node as an outlier if the node is projected outside of the corresponding limb or far from the presumable target joint in multiple 2D views. We exclude the period where people come in and out of the system, since at the moment body parts lie on the edge of our system’s working volume. The result of the quantitative evaluation for the 15 min. of sequence is summarized in Table 3. There are 12 sessions in the sequence, and 61 temporally associated skeletal structures are reconstructed. Among about 1.2 million body joints, about 8.7K nodes are determined as outliers or missed (rejected by thresholds of our system), showing 99.29% accuracy in node reconstruction. And, 93.55% out of about 82K 3D skeletons are correctly reconstructed without any incorrect joints. The majority of the failures are caused by insufficient visibility of the target part. An example is the pose holding hands behind one’s back near the wall of the system as shown in Figure 10 (left). Although the hands are visible from few cameras, they are too close to be detected by the pose detector. Interestingly, our method still reconstructs the hands using the “guessed” 2D locations from 2D pose detector in frontal views, although the accuracy is limited as shown in Figure 10.

使用所有VGA相机的性能。我们首先在所有480个VGA相机都使用时，量化我们系统的性能。由于缺少真值数据，我们通过在多个2D视角中验证其投影，手工标注了重建的3D骨架的正确性。如果在多个2D视角中，节点的投影在对应的肢体以外，或与预设的目标关节距离太远，我们就标记这个3D关节节点作为一个外点。我们排除了人们进入和出去这个系统的时间段，由于在这些时刻身体部位在我们系统工作空间的边缘。对15分钟序列的量化评估结果总结于表3。序列中有12个会话，重建了61个时域相关的骨架结构。在大约1.2 million个身体关节中，大约8.7K个节点被认为是外点或缺失的（被系统阈值拒绝掉），节点重建准确率为99.29%。而且，大约82K个3D骨架中，93.55%得到了正确的重建，没有任何错误的节点。失败的主要部分是由于目标部位的可见性不足。一个例子是，将手背在身体后面的姿态，靠近系统的墙，如图10左所示。虽然手在几个相机中是可见的，但太近了，姿态检测器也不能检测到。有趣的是，我们的方法仍然从正面视角的2D姿态检测器中，用猜测的2D位置重建了手，虽然准确率有限，如图10所示。

**Comparison with varying number of cameras**: To evaluate the impact of the number of views, we perform our method using varying number of cameras. The cameras are uniformly sampled (except the 19 VGA camera case explained later); i.e., we sample the next camera as the one furthest from all the already sampled cameras, and, thus, the selected cameras are always a subset of the set of the larger number of cameras. To quantify the results, we treat the result with 480 VGA cameras as ground truth after excluding the manually annotated outliers. For evaluation, we only use every tenth frame to reduce computation time. As an evaluation metric, we use the PCK (Probability of Correct Keypoint) metric, which is commonly used to evaluate 2D pose detectors [45]. Here, we use 3D distance in physical scale (cm) obtained from calibration data for the threshold of PCK, in contrast to the 2D ratio of torso/head as in 2D pose detection cases [45]. Figure 8 shows the PCK accuracy by varying the camera number on the scenes with different number of people. In all the results, we find that using a larger number of views is beneficial. If the scene is simpler (e.g., the case with two or three people), we observe that the results with a smaller number of cameras, e.g., 160 cameras, show a similar performance with 480 cameras. However, if the scene becomes more complicated, e.g., seven people, we see clearer gaps according to the camera numbers. This results can be meaningfully used to design a multiple camera system to determine the required number of cameras given a desired group size. For example, assuming that the target scenes have about five people, we forecast that a system with 80 cameras can reach about 94% of accuracy with a 2cm threshold.

变化相机数量的比较。为评估视角数量的影响，我们使用不同数量的相机来执行我们的方法。相机均匀采样（除了在19个VGA相机的情况下，这在后面解释）；即，我们采样的下一个相机，是距离已有相机最远的那个，因此，选择的相机是更大数量相机集合的子集。为量化这个结果，我们认为排除了手工标注的外点的480个VGA相机的结果为真值。为评估，我们每10帧只使用1帧，来减少计算时间。作为评估度量方式，我们使用PCK的度量，这是经常用于评估2D姿态检测器的。这里，我们使用从校准数据中得到的物理尺度下的3D距离(cm)作为PCK的阈值，与之形成比较的是，[45]使用2D姿态检测结果的躯干/头部的2D比率。图8展示了在有不同数量的人的场景中，在变化相机数量时的PCK准确率。在所有的结果中，我们发现使用更多数量的视角是有好处的。如果场景更简单一些（如，2个或3个人的情况），我们观察到，用较少数量的相机的结果，如160个相机，与480个相机的结果类似。但是，如果场景变得更加复杂，比如，7个人，我们可以看到不同相机数量之间更明显的差距。这个结果是有意义的，在给定期望的人群数量时，要设计一个多相机系统，可以确定需要的相机数量。比如，假设目标场景有5个人，我们预测80个相机可以在2cm阈值的情况下达到94%的准确率。

**Comparison with varying resolutions**: As an additional evaluation, we perform a similar experiment for different camera resolutions using the multiple HD cameras installed in our system. Among 31 HD cameras, we use 19 HD cameras installed on the same panels with VGA cameras. To compose similar viewpoints, we choose the closest VGA cameras from the selected 19 HDs. Additionally, we generate 19 QVGA inputs (320 × 240 resolution) by resizing the selected VGA videos. Because the HD cameras are not perfectly synchronized with VGAs, we interpolate the result from HDs into the VGA time domain using the hardware sync data. The performance of a same number of HDs, VGAs, and QVGAs is shown as dashed lines in Figure 8. The result shows that the performance differences among them is marginal, although HD views have about 7 time more pixels than VGAs and about 27 times more pixels than QVGAs. The result demonstrates that the pose reconstruction performance of our method is marginally affected by the resolution changes compared to the changes by the number of views. Note that the integral of number of pixels in the 19 HD views are equivalent to about 128 VGA views, and the result clearly shows that it is more advantageous to have more unique camera views rather than having higher resolutions, given a fixed pixel budget. The main reason underlying this finding is that dealing with occlusions is more crucial in interaction capture scenarios, and, in particular, higher resolution is not beneficial in our method, since 2D joint localization accuracy is still limited by the 2D pose detector.

变化分辨率的比较。作为额外的评估，我们对不同相机分辨率进行类似的试验，使用我们系统中安装的多个HD相机。在31个HD相机中，我们使用与VGA相机安装在同一个面板上的19个HD相机。为构成类似的视角，我们选择与选择的19个HDs最接近的VGA相机。另外，我们生成了19个QVGA输入(320 × 240)，将选择的VGA视频改变其大小。因为HD相机并不是与VGAs完美同步的，我们使用硬件同步的数据，将HDs的结果插值到VGA时域中。相同数量的HDs，VGAs和QVGAs的性能，如图8的虚线所示。结果表明，它们的性能差异是很小的，虽然HD视角的像素数量，是VGAs的大约7倍，是QVGAs的大约27倍。结果证明了，我们方法的姿态重建性能，受到分辨率变化的影响很小，受到视角数量变化的影响较大。注意，19个HD视角的像素数量综合，与128个VGA视角的是大约等价的，结果清晰表明，在固定的像素数量下，更多的相机视角数量，明显比更高分辨率要更有优势。这之下的主要原因是，在交互捕获场景中，处理遮挡要更加关键，特别是，更高的分辨率对我们的方法并没有多少好处，因为2D关节定位准确率仍然受到2D姿态检测器所限制。

**Comparison to multiple Kinects**: We also compare our results with the result of multiple Kinects. Since Kinect with its accompanying SDK is one of the most commonly used sensors for markerless motion capture in various communities, using multiple Kinects can be considered as an option to handle severe occlusions for interaction capture. However, how to fuse multiple Kinect cues is not straight-forward, and, thus, we naively fuse them as follows. We first generate 3D skeletal proposals from all individual Kinects, and simply find the best candidate closest to our ground truth data in Euclidean space, assuming that an Oracle chooses the best one given the GT data. This can be considered an upper bound of a naive multiple Kinects method. Since the keypoint locations of the Kinects are not identical to the skeletons of our method, for a fair comparison, we adjust the Kinect skeletons by finding an offset vector from each Kinect node toward our node of GT’s skeleton in a person-centric coordinate system. As shown in Figure 8, the results of the Oracle Kinects is limited, showing less than 80% accuracy at a 5cm threshold.

多个Kinects的比较。我们还比较了使用多个Kinects的结果。由于Kinect是在无标记运动捕获中最常用的传感器，使用多个Kinects可以认为是在交互捕获中处理严重遮挡的一个选项。但是，怎样融合多个Kinects的线索并不是很直观的，因此，我们简单的将其融合如下。我们首先从所有单个Kinect生成3D骨架建议，简单的找到与我们真值数据在欧式空间中距离最近的候选，假设一个Oracle在给定GT数据下选择了最好的那个。这可以认为是简单多个Kinects方法的上限。由于Kinects的关键点位置与我们方法的骨架并不一致，为进行公平比较，我们调整了Kinect骨架，找到一个从每个Kinect节点到我们的GT的骨架的节点，在以人为中心的坐标系下的偏移向量。如图8所示，Oracle Kinects的结果是有限的，在5cm的阈值下得到了小于80%的准确率。

### 7.4 Refinement by Trajectory Stream

We compare the performance improvement of our refinement method (the second stage) over the output of the first stage. We choose a challenging scene in 160226 mafia2 sequence where the first stage of our method shows failures due to the erroneous 2D pose detection results. To see the performance change, we plot the X coordinate of the most erroneous node (right wrist of a subject) as shown in Figure 9. The frames denoted as gray regions are the time when the nodes are missed due to the consistent 2D pose detector failures. It is shown that our refinement method can recover the missing parts and also noticeably reduce the motion jitter for the unstable frames. Note that our refinement method is not just smoothing but based on the temporal transformation measured by a dense trajectory stream. Thus, it does not suffer from over-smoothing, even after several iterations.

我们比较了我们提炼的方法（第二阶段），比第一阶段输出的性能改进。我们选择了一个有挑战的场景，160226 mafia2序列，其中我们方法的第一阶段中，由于错误的2D姿态检测结果，表现出了失败。为看到性能变化，我们画出了误差最大的节点的X坐标（一个目标的右手腕），如图9所示。灰色区域表示的帧，是由于2D姿态检测器的错误导致节点缺失的时刻。可以看出，我们的提炼方法并不只是平滑，而是基于由密集轨迹流所度量的时域变换。因此，甚至在几次迭代之后，这也并不会受到过度平滑的困扰。

### 7.5 Comparison with The Method of Joo et al. [46]

We compare the presented method to the method introduced in [46]. In [46], due to the relatively unreliable 2D pose detection cue [47], the motion cues from trajectory stream play a core role to reconstruct valid parts. The method, however, tends to fail in regions where the trajectory stream is unavailable (e.g., the texture-less dark body parts). The method presented in this paper is composed of two sequential stages using an advanced 2D pose detector [35], and Stage 1 is still applicable in the region where trajectory stream is unavailable. Table 4 shows the comparison between two methods on the sequence 150129 007-Bang introduced in [46], where the accuracy is computed by manually annotating outliers. The major failures of [46] occur on the texture-less leg parts, or fast motion with motion blur when the trajectory stream is sparse and inaccurate. Refer to the supplementary video for the qualitative comparison.

我们比较了我们的方法，与[46]中提出的方法。在[46]中，由于相对不可靠的2D姿态检测结果线索[47]，轨迹流的运动线索在重建有效部位中扮演了核心角色。但是，这个方法在轨迹流不可用的区域会容易失败（如，无纹理的暗色身体部位）。本文中给出的方法，是由两个顺序阶段组成，使用了高级2D姿态检测器[35]，阶段1在轨迹流不可用的区域仍然可用。表4给出了两种方法在序列150129 007-Bang上的对比，其中准确率是由手工标注的外点来计算的。[46]的主要失败的地方，在无纹理的腿部，或在有运动模糊的快速运动中，这时候轨迹流是稀疏的，不准确的。附件的视频给出了定性的比较。

### 7.6 Qualitative Evaluation

We apply our method, producing about 3 hours of interaction capture results. Due to the computation time, the second stage of our method is applied on a subset of the dataset; yet the first stage of our method is applied on all the sequences. Example results are shown in Figure 11. Our result is fully automatic—given video streams and calibration data, our method generates temporally associated 3D skeletons (and labeled patch trajectory stream of each body part if the second stage is applied) for each individual without any human supervision. Refer to the supplementary videos and the live 3D viewer on our website.

用我们的方法生成了大约3小时的互动捕获结果。由于计算时间的原因，我们方法的第二阶段只用在了数据集的一个子集上；然而，我们方法的第一阶段是用在所有序列中的。图11给出了结果的例子。我们的结果是完全自动的，给定视频流和标定数据，我们的方法对每个个体生成时域相关的3D骨架（如果第二阶段也进行的话，还有每个身体部位的标记的块轨迹流），不需要任何监督。

**Group interaction capture**: Our method produces motion capture results on various social game scenarios performed by multiple people (up to 8 people). The number of subjects in the scenes is automatically determined by our method, and allowed to vary during the capture. The reconstructed results contain motions that frequently occur during communication, such as crossed-arms-on-chest, resting-chin-on-hand, mouth-guard, hands-on-back, hands-on-waist, and so on. In spite of their importance as non-verbal signals transmitting a variety of messages, such motions get little attention by prior work. In particular, severe topological changes and self-occlusions make it hard to apply 3D template-based motion capture approaches. Our method reconstructs the motion of such challenging scenes by fusing 2D pose detection cues and motion cues using a larger number of views, and demonstrates a compelling performance for social interaction capture.

人群交互捕获：我们的方法在多人进行的多种社交游戏场景中产生运动捕获结果（最多8人）。场景中目标的数量，由算法自动确定，而且在捕获的过程中可以变化。重建的结果包含运动，在通信的过程中这经常发生，比如在胸前交叉双臂，将下巴放到手上，捂住嘴巴，手背到后面，手放在手腕上，等等。尽管这些都是很重要的非语言信号，传递了大量信息，这样的运动在之前的工作中关注的很少。特别是，严重的拓扑变化和自我遮挡，使应用基于3D模板的运动捕获方法非常困难。我们的方法重建了这种有挑战性场景的运动，将大量视角的2D姿态检测结果的线索和运动线索进行融合，在社交交互捕获中展现出了非常好的性能。

**Robustness to appearance, body sizes, and topological changes**: Our results demonstrate robustness to subjects of diverse appearance, body types, and sizes. As mentioned, subjects’ clothing is not controlled, and the captured sequences contain people with various clothing such as black pants, thick padding jumpers, hoodies, short pants, scarfs, and so on. During the discussions, they also unconsciously adjust their clothing, for example by rolling up sleeves or relocating scarfs. The height of subjects varies from a two-year old toddler to adults more than 190 cm tall. The “model-free” nature of our method enables us to reconstruct their motions without changing any parameter. It demonstrates a major advantage of our system for social behavior studies in that it can be easily applied for captures at scale, without any laborious template generation or initial alignment step. Especially, the toddler scene is challenging to “model-heavy” approaches, since instructing young children to be stationary to generate their template models (e.g., laser scanning) may not be practical.

对外观，身体大小和拓扑变化的稳健性：我们的结果对不同外观，身体类型和大小的目标，展现出了稳健性。已经提到了，目标的衣着是不受控的，捕获的序列包含各种衣着的人，比如黑色短裤，厚垫针织外套，连帽外套，短裤，围巾，等等。在讨论的过程中，他们会无意识的调整其衣着，比如卷起袖子，或改变围巾的位置。目标的高度，从2岁大的小孩，到超过190cm高的成人。我们方法无模型的本质，使我们可以在不改变参数的情况下重建其运动。这说明我们的系统在社交行为研究中有很大的优势，因为可以很容易的应用到大量捕获，不需要进行繁复的模板生成，或初始对齐步骤。尤其是，小孩的场景对于重模型的方法是非常有挑战的，因为要让小孩保持静止，来生成其模板模型（如，激光扫描），这可能是不现实的。

**Other interesting scenes**: We also demonstrate the performance of our method on other atypical motion capture scenarios including musical performances (drummer and cellist) and dancer sequences. Motion capture for musical performance is a good application for markerless motion capture, because markers may interfere with their functional movements during the capture. Although the scenes are challenging due to the severe occlusions by musical instruments, our method shows good performance in reconstructing the performer’s subtle motions (e.g., the vibrato motion in the cellist sequences).

其他有趣的场景：我们还在其他非典型运动捕获场景中，证明我们方法的性能，包括乐器演奏（鼓手和大提琴手），和舞蹈。乐器演奏的运动捕获是无标记运动捕获的很好的应用，因为标记会在捕获时干扰其运动。虽然场景非常有挑战性，因为有乐器的严重遮挡，我们的方法在重建演奏者的微妙运动时得到了很好的性能（如，大提琴手序列中的颤音运动）。

On the other hand, the dancer sequences contain fast motion and unusual poses. Due to failures in reconstructing the trajectory stream for the extremely fast movement compared to our relatively low frame rate cameras (25 Hz), we only apply the first stage of our method. Separating reconstruction (Stage 1) from temporal refinement (Stage 2) is advantageous in this case, because the first stage, based on per-frame reconstruction, is not affected by motion magnitude and free from error accumulation. We can optionally apply temporal refinement (Stage 2) based on the quality of trajectory stream to further refine the results. We find that, however, in a few extremely unusual poses our method becomes unstable due to consistent 2D pose detection failures, which will be discussed in subsection 8.1.

另一方面，舞蹈者序列包含快速运动和不常见的姿态。由于相机帧率相对较低(25Hz)，对于非常快速的运动，轨迹流的重建失败了，我们只使用了我们方法的第一阶段。将重建（第1阶段）与时域提炼（第2阶段）分开，在这种情况下是有优势的，因为第1阶段是基于逐帧的重建，不会受到运动幅度的影响，免于误差累积。我们可以有选择的应用时域提炼（第2阶段），基于轨迹流的质量，以进一步提炼结果。但是我们发现，在一些非常不常见的姿态中，我们的方法会变得不稳定，因为2D姿态检测会持续失败，我们在8.1节中讨论这个问题。

## 8. Discussion

We present the Panoptic Studio and an interaction capture method that leverages a large number of views. To demonstrate the performance of our method, we collect a large scale social interaction dataset, and produce compelling motion capture results on it. In particular, we empirically find that having a larger number of views is more beneficial than having a higher resolution of views for social interaction capture. Our quantitative comparison on various number and type of cameras can be used as a meaningful resource to design follow-up multiview systems to estimate the required number of views to achieve a desired accuracy. Our method also demonstrates that highly-occluded social motion capture is possible by boosting 2D pose detection cues and motion cues in a larger number of views, without using any heavy prior or template model. Our method shows its advantages in the social interaction capture scenario by reconstructing subjects of diverse appearance, body sizes, and body topology for a long term without error accumulation issue.

我们提出了全景工作室，和一种利用大量视角的交互捕获方法。为证明我们方法的性能，我们收集了一个大规模社交交互数据，在其上产生了很好的运动捕获结果。特别是，我们通过经验发现，视角数量更多，比分辨率更高，对于社交交互捕获更为有利。我们在各种数量和类型的相机上的定量比较，可以用于设计后续的多视角系统的经验，以估计需要的视角数量，得到期望的准确率。我们的方法还证明了，在更多数量的视角中，提升2D姿态检测线索和运动线索，捕获高度遮挡的社交运动是可能的，不需要使用任何大量先验，或模板模型。我们的方法在社交交互捕获场景中展现出了优势，在很长时间内，重建了多种外观、身体大小、身体拓扑的目标，没有误差累积的问题。

### 8.1 Limitations

A limitation of our method is the dependency on a 2D pose detection method. State-of-the-art pose detectors are weak in detecting unusual poses and closely located people (as shown in Fig. 10). We also find that the pose detector sometimes gets confused in distinguishing left-right limbs (as shown in the center of Figure 10). Although our method can overcome these issues by fusing cues across views via spatial voting and across time via associating trajectory streams, if the 2D body pose detectors fail consistently, our method is unable to recover. The second limitation is the long computation time to process the large number of views. However, depending on the application, the computation time can be greatly reduced by using a smaller number of cameras with a trade-off in accuracy (see Fig. 8).

我们方法的一个局限，是依赖于2D姿态检测方法。目前最好的姿态检测器，在检测不常见的姿态和非常接近的人时，是很弱的（如图10所示）。我们还发现，姿态检测器在区分左右肢体的时候，会有混淆的情况（如图10中间所示）。虽然我们的方法可以通过融合不同视角的线索进行空间投票，和在不同的时刻通过关联轨迹流，来克服这些问题，但如果2D人体姿态检测器持续失败，我们的方法就无法恢复了。第二个局限是，在处理大量视角时，计算时间很长。但是，依赖于应用，使用的相机数量更少的话，计算时间可以极大的降低，但准确率也会有一定的下降（见图8）。

In terms of the hardware design, we also find two limitations which should be reconsidered for follow-up research. The first is the incompatible frame rates among heterogeneous sensors, especially between HD cameras and VGA cameras, which makes it hard to fuse them for 3D reconstruction. Due to this reason, our method currently uses VGA cameras only, but we expect that this issue can be dealt with by interpolating cues in the common time domain, because all the sensors are temporally aligned in millisecond level. The second issue is that all the camera views mainly focus on the center of the dome and, thus, fewer views are available at the edges of the capture volume. Such design is ideal given the assumption that subjects are located at the center of the system, but we observe that sometimes people tend to stand near the walls during social interactions. An alternative direction would be to make cameras focus on random locations so that view coverage can be uniformly spread throughout the working volume.

在硬件设计上，我们还发现两个局限，在后续的研究中应当重新考虑。第一个是，异质传感器的帧率不兼容，尤其是HD相机和VGA相机之间，这使其很难进行融合，以进行3D重建。由于这个原因，我们的方法目前只使用VGA相机，但我们期望这个问题会得到处理，在通用的时域对线索进行插值，因为所有的传感器是在ms级别进行时域对齐的。第二个问题是，所有相机视角主要聚焦在dome的中间，因此，在捕获体的边缘，可用的视角就很少。如果假设目标在系统中间，这样的设计就是合理的，但我们观察到，人们在社交互动时，有时候会倾向于站到墙边。另一种方向可能是，使相机聚焦在随机位置上，这样视角覆盖就会更均匀的在整个工作体中分布。

### 8.2 Future Work

There are various future directions expanding our system and outputs. First, analyzing human social behaviors using the measured social signals of our system is an interesting direction, which will facilitate social behavior understanding in a data-driven manner. Second, our interaction capture outputs can be used as labeled data to train new 2D detectors. By projecting the skeletal reconstruction outputs on all 521 views, our current dataset generates 153 million pose data in diverse views. Although the appearance diversity of the scenes would be limited compared to internet photos, our dataset is still meaningful in that: (1) it captures multiple interacting people showing severe inter-occlusions in the scenes; (2) it has annotations for entire video frames which is the key to study temporal relation of poses; (3) the scenes are taken by 521 diverse view points compared to the biased views of usual portrait photos (e.g., frontal views or side views). This type of labeled data would be hard to obtain by manual annotations. Lastly, a similar massively multiview approach can be applied to reconstruct 3D faces. This can be done by substituting the 2D body pose detector with a 2D face landmark detector in our method.

扩展我们的系统和输出，有几个未来的方向。第一，使用我们系统测量的社交信号来分析人类社交行为，是一个有趣的方向，这会促进数据驱动的社交行为理解。第二，我们的互动捕获输出可以用作标注的数据，以训练新的2D检测器。将骨架重建输出在所有521个视角进行投影，我们目前的数据集会在多个视角中生成153 million个姿态数据。虽然场景的外观多样性与互联网图像相比是很有限的，但是我们的数据集仍然是有意义的：(1) 捕获了多个互动的人，在场景中有严重的互相遮挡；(2) 对整个视频帧都有标注，这对研究姿态的时域关系非常关键；(3) 与通常的肖像图（前视或侧视）相比，场景是在521个不同的视角拍摄的。这种类型的标记数据通过手工标注很难获得。最后，可以用类似的大规模多视角方法来重建3D人脸，可以将我们方法中的2D人体姿态检测器替换为2D人脸关键点检测器。