# A Performance Counter Architecture for Computing Accurate CPI Components

Stijn Eyerman, Lieven Eeckhout, Tejas Karkhanis, James E. Smith @ Ghent University & University of Wisconsin–Madison

## 0. Abstract

Cycles per Instruction (CPI) stacks break down processor execution time into a baseline CPI plus a number of miss event CPI components. CPI breakdowns can be very helpful in gaining insight into the behavior of an application on a given microprocessor; consequently, they are widely used by software application developers and computer architects. However, computing CPI stacks on superscalar out-of-order processors is challenging because of various overlaps among execution and miss events (cache misses, TLB misses, and branch mispredictions).

CPI stacks将处理器执行时间分解成基准CPI，加上数个miss event CPI组成部分。CPI分解在洞察一个应用在给定的微处理器上的行为时，非常有用；结果是，软件应用开发者和计算机架构师都会广泛使用这个工具。但是，在一个超标量乱序处理器上计算CPI stacks是很有挑战的，因为正常执行和miss events会有各种情况的叠加(cache misses, TLB misses, and branch mispredictions)。

This paper shows that meaningful and accurate CPI stacks can be computed for superscalar out-of-order processors. Using interval analysis, a novel method for analyzing out-of-order processor performance, we gain understanding into the performance impact of the various miss events. Based on this understanding, we propose a novel way of architecting hardware performance counters for building accurate CPI stacks. The additional hardware for implementing these counters is limited and comparable to existing hardware performance counter architectures while being significantly more accurate than previous approaches.

本文表明，可以对超标量乱序处理器计算得到有意义的，准确的CPI stacks。区间分析是分析乱序处理器性能的新方法，使用这种工具，我们会更深入的理解各种miss events对性能的影响。基于这个理解，我们提出一种硬件性能计数器架构的新方法，构建准确的CPI stacks。实现这种计数器的额外硬件是有限的，与现有的硬件性能计数器架构是可比的，而比之前的方法要明显更加准确。

## 1. Introduction

A key role of user-visible hardware performance counters is to provide clear and accurate performance information to the software developer. This information provides guidance regarding the kinds of software changes that are necessary for improved performance. One intuitively appealing way of representing the major performance components is in terms of their contributions to the average cycles per instruction (CPI). However, for out-of-order superscalar processors, conventional performance counters do not provide the type of information from which accurate CPI components can be determined. The reason is that performance counters have historically been constructed in a bottom up fashion by focusing on the events that affect performance, for example the various cache miss rates, without regard for how the performance counts should be combined to form an overall picture of CPI.

用户硬件计数器的一个关键作用，是向软件开发者提供清晰准确的性能信息。这种信息在改进性能时可以提供软件变化的指引。表示主要的性能组件，一种直觉上很有吸引力的方法是，按照其对平均CPI的贡献。但是，对于乱序超标量处理器，传统的性能计数器不会提供这种可以确定准确的CPI组件的信息。原因是，性能计数器在历史上是以一种自下而上的方式构建的，聚焦在影响性能的事件上，比如，各种cache miss rates，而与性能计数怎样结合到一起，以形成CPI的总体picture，则没有关注。

In contrast, by viewing performance in a top down manner with accurate CPI measures as the goal, a set of performance counters can be defined that do provide basic data from which an accurate overall picture of CPI can be built. We have taken such a top down approach, using interval analysis, a superscalar processor performance model we have developed, as a guide. The performance model gives an in-depth understanding of the relationships among miss events and related performance penalties. The insights from the performance model are used to design a novel hardware performance counter architecture for computing accurate CPI components — within a few percent of the components computed by detailed simulations. This is significantly more accurate than previously proposed CPI breakdown approaches with errors that are higher than 30%. Moreover, the hardware complexity for our counter architecture is limited and comparable to existing hardware performance counter architectures.

对比起来，以自上而下的方式查看性能，并带有准确的CPI测量，以这个为目标的话，可以定义一系列性能计数器，提供一些基础数据，从中构建准确的总体CPI picture。我们采用了这样一种自上而下的方法，使用区间分析，我们提出的超标量处理器性能模型，作为指引。性能模型给出了miss events和相关的性能惩罚之间的关系的深度理解。性能模型给出的洞见，用于设计一种新型的硬件性能计数器架构，以计算准确的CPI组件，这是详细仿真计算的组件的一小部分。这比之前提出的CPI分解方法明显更加准确，之前的方法误差率超过了30%。而且，我们的计数器架构的硬件复杂度是有限的，与现有的硬件性能计数器架构是类似的。

We first revisit the CPI stack and existing approaches to measuring CPI stacks (section 2). We then use a performance model, interval analysis, to determine what the miss penalties are for the various miss events (section 3). Based on these insights we subsequently propose our hardware performance counter mechanism for building accurate CPI stacks (section 4). Subsequently, the proposed mechanism is evaluated (section 5) and related work is discussed (section 6). Finally, we conclude in section 7.

我们首先重温了CPI stack，和现有的测量CPI stacks的方法。然后，使用一个性能模型，即区间分析，来确定对各种miss event是是什么样的miss惩罚。基于这些洞见，我们然后提出了我们的硬件性能计数器机制，构建准确的CPI stacks。然后，我们提出的机制进行了评估，讨论了相关的工作。最后，我们给出了结论。

## 2. Constructing CPI Stacks

The average CPI for a computer program executing on a given microprocessor can be divided into a base CPI plus a number of CPI components that reflect the ‘lost’ cycle opportunities because of miss events such as branch mispredictions and cache and TLB misses. The breakdown of CPI into components is often referred to as a CPI ‘stack’ because the CPI data is typically displayed as stacked histogram bars where the CPI components stack on top of one another with the base CPI being shown at the bottom of the histogram bar. A CPI stack reveals valuable information about how a given application behaves on a given microprocessor and gives more insight into an application’s behavior than raw miss rates do.

计算机程序在给定的微处理器上执行，其平均CPI可以分解成基础CPI，和几个CPI组件，反应了由于miss events而丢失的周期机会，比如分支误预测，cache misses，TLB misses。将CPI分解成各种组件，这通常称为CPI stack，因为CPI数据通常展示为堆叠的直方图，底部是基础CPI，上面堆叠着各种CPI组件。CPI stack反应了一个给定的应用在给定的微处理器上的行为的宝贵信息，比原始的miss rates给出了更多的应用程序行为的洞见。

Figure 1 shows an example CPI stack for the twolf benchmark for a 4-wide superscalar out-of-order processor detailed in section 5.1. The CPI stack reveals that the base CPI (in the absence of miss events) equals 0.3; other substantial CPI components are for the L1 I-cache (0.18), the L2 D-cache (0.56) and the branch predictor (0.16). The overall CPI equals 1.35 which is shown by the top of the CPI stack. Application developers can use a CPI stack for optimizing their software. For example, if the intruction cache miss CPI is relatively high, then improving instruction locality is the key to reducing CPI and increasing performance. Or, if the L2 D-cache CPI component is high as is the case for the twolf example, changing the data layout or adding software prefetching instructions are viable optimizations. Note that a CPI stack also shows what the maximum performance improvement is for a given optimization. For example for twolf, improving the L2 D-cache behavior can improve overall performance by at most 41%, i.e., the L2 D-cache CPI component divided by the overall CPI.

图1展示了在一个4宽度超标量乱序处理器上，运行twolf benchmark得到的CPI stack的例子。CPI stack中表明，基准CPI为0.3（在没有miss events的情况下）；其他重要的CPI组件是，L1 I-cache (0.18)，L2 D-cache (0.56)和the branch predictor (0.16)。总计的CPI等于1.35，即CPI stack的顶部。应用开发者可以用CPI stack来优化其软件。比如，如果指令cache miss CPI相对较高，那么改进指令的局部性是减小CPI，增加性能的关键。或者，如果L2 D-cache CPI组件很高，在twolf的例子中就是这个情况，改变数据的布局，或增加软件预取指令，就是可行的优化方法。注意，CPI stack还展示了，对于一个给定的优化，最大的性能改进是什么样的。比如，对于twolf，改进L2 D-cache行为可以最多改进总体性能41%，即，L2 D-cache CPI组件除以总体的CPI。

Although the basic idea of a CPI stack is simple, computing accurate CPI stacks on superscalar out-of-order processors is challenging because of the parallel processing of independent operations and miss events. A widely used naive approach for computing the various components in a CPI stack is to multiply the number of miss events of a given type by an average penalty per miss event [1, 10, 11, 17, 20]. For example, the L2 data cache miss CPI component is computed by multiplying the number of L2 misses with the average memory access latency; the branch misprediction CPI contributor is computed by multiplying the number of branch mispredictions with an average branch misprediction penalty. We will refer to this approach as the naive approach throughout the paper.

虽然CPI stack的基本思想是很简单的，在超标量乱序处理器上计算准确的CPI stack是很有挑战的，因为要对独立的操作和miss events进行并行处理。一种广泛使用的天然方法来计算CPI stack中各种组件，是将给定类型的miss events的数量，乘以每个miss event的平均penalty。比如，L2 data cache miss的CPI组件的计算，是将L2 misses的数量，乘以平均内存访问延迟；分支误预测CPI的贡献，是将分支误预测的数量，乘以平均分支误预测的penalty。我们在本文中称这种方法为naive方法。

There are a number of pitfalls to the naive approach, however. First, the average penalty for a given miss event may vary across programs, and, in addition, the number of penalty cycles may not be obvious. For example, previous work [4] has shown that the branch misprediction penalty varies widely across benchmarks and can be substantially larger than the frontend pipeline length — taking the frontend pipeline length as an estimate for the branch misprediction penalty leads to a significant underestimation of the real branch misprediction penalty. Second, the naive approach does not take into consideration that some of the miss event penalties can be hidden (overlapped) through out-of-order processing of independent instructions and miss events. For example, L1 data cache misses can be hidden almost completely in a balanced, out-of-order superscalar processor. As another example, two or more L2 data cache misses may overlap with each other. Not taking these overlapping miss events into account can give highly skewed estimates of the CPI components. And finally, the naive approach makes no distinction between miss events along mispredicted control flow paths and miss events along correct control flow paths. A naive method may count events on both paths, leading to inaccuracy.

但是，naive方法有几个缺点。第一，给定miss event的平均penalty在整个程序中可能是变化的，而且，penalty周期的数量可能不是明显的。比如，之前的工作[4]表明，分支误预测的penalty在不同的benchmarks中变化很大，比前端的流水的长度要长很多，如果只是将前端流水线的长度作为分支误预测penalty的估计，会明显的低估真实的分支误预测penalty。第二，naive方法并没有考虑，一些miss event的penalties在独立指令的乱序处理和miss events中是隐藏的（重叠的）。比如，L1 data cache misses在一个平衡的乱序超标量处理器中可能是完全隐藏的。另一个例子，两个或多个L2 data cache misses可能会互相重叠。不将这些重叠的miss events纳入考虑，会得到误差很大的CPI部件估计。最后，naive方法并没有区分沿着误预测的控制流路径的miss events和沿着正确的控制流路径的miss events，naive方法会将两种路径的events都进行技术，从而导致不准确性。

To overcome the latter problem, some processors, such as the Intel Pentium 4 [18], feature a mechanism for obtaining nonspeculative event counts. This is achieved by implementing a tagging mechanism that tags instructions as they flow through the pipeline, and the event counters get updated only in case the instruction reaches completion. In case the instruction is not completed, i.e., the instruction is from a misspeculated path, the event counter does not get updated. We will refer to this approach as the naive_non_spec approach; this approach differs from the naive approach in that it does not count miss events along mispredicted paths.

为克服后面这个问题，一些处理器，比如Intel Pentium 4，会有一种特性，可以得到非推测事件计数的能力。这是实现了一个tagging的机制，在指令流过流水线的时候，加上了一个tag，只有在指令达到完成的时候，事件计数器才进行更新。在指令没有完成的情况下，如，指令是来自于误推测的路径上，那么事件计数器就不会进行更新。我们称这种方法为naive_non_spec方法；这种方法与naive方法的不同是，它不会沿着误预测的路径对miss events进行计数。

In response to some of the above shortcomings, the designers of the IBM POWER5 microprocessor implemented a dedicated hardware performance counter mechanism with the goal of computing the CPI stack [12, 13]. And to the best of our knowledge, the IBM POWER5 is the only out-of-order processor implementing a dedicated hardware performance counter architecture for measuring the CPI components similar to our approach — in fact, other approaches have been proposed which we discuss in section 6. The IBM POWER5 has hardware performance counters that can be programmed to count particular completion stall conditions such as I-cache miss, branch misprediction, L2 D-cache miss, L1 D-cache miss, etc. The general philosophy for the IBM POWER5 CPI mechanism is to inspect the completion stage of the pipeline. And if no instructions can be completed in a given cycle, the appropriate completion stall counter is incremented. As such, the completion stall counters count the number of stall cycles for a given stall condition. There are two primary conditions for a completion stall.

为回应上述的一些缺陷，IBM POWER5微处理器的设计者实现了一种精细的硬件性能计数器机制，目的是计算CPI stack。据我们所知，IBM POWER5是唯一的一个乱序处理器，实现了一个精细的硬件性能计数器架构，测量CPI组件，与我们的方法类似；实际上，提出的其他方法，我们在第6部分进行了讨论。IBM POWER5的硬件性能计数器可以进行编程，对特定的completion stall条件进行计数，如I-cache miss, branch misprediction, L2 D-cache miss, L1 D-cache miss等。IBM POWER5 CPI机制的总体哲学是，去检查流水线的完成阶段。如果在给定的周期中没有完成任何指令，就对适当的completion stall计数器进行递增。这样，completion stall计数器对给定的stall condition，对stall周期的数量进行计数。对一个completion stall，有两种主要的情况：

First, the reorder buffer (ROB) is empty. There are two possible causes for this. 第一，ROB是空的。对此，有两个可能的原因。

- First, an I-cache miss, or an I-TLB miss occurred, and the pipeline stops feeding new instructions into the ROB. This causes the ROB to drain, and, eventually, the ROB may become empty. When the ROB is empty, the POWER5 mechanism starts counting lost cycles in the I-cache completion stall counter until instructions start entering the ROB again.

第一，是发生了一个I-cache miss，或一个I-TLB miss，流水线停止将新的指令送入到ROB。这导致ROB流失，最终，ROB变成空的。当ROB是空的后，POWER5的机制开始在I-cache completion stall计数器中对丢失的周期进行计数，直到指令开始再次进入ROB。

- Second, a branch is mispredicted. When the mispredicted branch gets resolved, the pipeline needs to be flushed and new instructions need to be fetched from the correct control flow path. At that point in time, the ROB is empty until newly fetched instructions have traversed the frontend pipeline to reach the ROB. The POWER5 mechanism counts the number of cycles with an empty ROB in the branch misprediction stall counter.

第二，一个分支被误预测了。当误预测的分支被解决后，流水线会被冲刷掉，从正确的控制流路径上取新的指令。在那个时间，ROB是空的，直到新取的指令穿过前端流水线，到达ROB。POWER5机制会在分支误预测stall计数器上，计数ROB空的周期数。

The second reason for a completion stall is that the instruction at the head of the ROB cannot be completed for some reason. The zero-completion cycle can be attributed to one of the following.

completion stall的第二个原因是，在ROB头部的指令由于某种原因不能完成执行。这个zero-completion周期可以归因到下面的原因。

- The instruction at the head of the ROB is stalled because it suffered a D-cache miss or a D-TLB miss. This causes the D-cache or D-TLB completion stall counter to be incremented every cycle until the memory operation is resolved.

ROB头部的指令停止了，因为存在D-cache miss或D-TLB miss的情况。这会导致D-cache或D-TLB completion stall计数器每周期递增，直到内存操作解决掉。

- The instruction at the head of the ROB is an instruction with latency greater than one cycle, such as a multiply, divide, or a long latency floating-point operation, and the instruction has not yet completed. The long latency completion stall counter is incremented every cycle until completion can make progress again.

在ROB头部的指令，是一条延迟大于1周期的指令，比如乘法，除法，或一个长延迟浮点运算，这条指令还没有完成。长延迟completion stall计数器每周期递增，直到指令完成。

These three CPI stack building approaches, the two naive approaches and the more complex IBM POWER5 approach, are both built in a bottom up fashion. These bottom up approaches are inadequate for computing the true performance penalties due to each of the miss events (as will be shown in detail in this paper). And as a result, the components in the resulting CPI stack are inaccurate.

这3个CPI stack构建方法，2个naive方法，和更复杂的IBM POWER5方法，都是以自下而上的方式构建的。这些自下而上的方法，对于计算真实的性能penalties是不够的，因为这些miss events。结果是，得到的CPI stack的部件都是不准确的。

## 3. Underlying Performance Model

We use a model for superscalar performance evaluation that we call interval analysis in our top down approach to designing a hardware performance counter architecture. Interval analysis provides insight into the performance of a superscalar processor without requiring detailed tracking of individual instructions. With interval analysis, execution time is partitioned into discrete intervals by the disruptive miss events (such as cache misses, TLB misses and branch mispredictions). Then, statistical processor and program behavior allows superscalar behavior and performance to be determined for each interval type. Finally, by aggregating the individual interval performance, overall performance is estimated.

我们使用了一个超标量性能评估模型，在我们自上而下的设计一个硬件性能计数器架构的方法中，我们称之为区间分析。区间分析给出了超标量处理器的性能的洞见，而不需要详细的追踪每条指令的情况。在区间分析中，执行时间由干扰性的miss events（如cache misses，TLB misses和分支误预测）分割成离散的区间。然后，统计的处理器和程序行为可以对每个区间类型确定其超标量的行为和性能。最后，通过累积单个的区间性能，来估计整体的性能。

The basis for the model is that a superscalar processor is designed to stream instructions through its various pipelines and functional units, and, under optimal conditions, a well-balanced design sustains a level of performance more-or-less equal to its pipeline (dispatch/issue) width. For a balanced processor design with a large enough ROB and related structures for a given processor width, the achieved issue rate indeed very closely approximates the maximum processor rate. This is true for the processor widths that are of practical interest – say 2-way to 6- or 8-way. We are not the first to observe this. Early studies such as by Riseman and Foster [] showed a squared relationship between instruction window size and IPC; this observation was also made in more recent work, see for example [] and []. There are exceptions though where there are very long dependence chains due to loop carried dependences and where the loop has relatively few other instructions. These situations are uncommon for the practical issue widths, however. Nevertheless, our counter architecture handles these cases as resource stalls or L1 Dcache misses, as described in section 4.3.

模型的基础是，超标量处理器的设计，是将指令流过其各种流水线和功能单元，在最优的情况下，一个均衡的设计，其性能会约等于其流水线（dispatch/issue）宽度。对于一个均衡的处理器设计，对给定的处理器宽度，有足够大的ROB和相关结构，得到的issue率，确实会非常接近最大的处理器速度。这对于实际的处理器宽度是正确的，比如2-way，到6-或8-way。我们并不是第一个观察到这个的。早期的研究表明，指令窗口大小和IPC是开方的关系；这种观察在最近的工作中也有。在一些存在非常长的依赖链的情况下，比如由于loop carried dependences，这个loop的其他指令相对较少，会有一些例外情况。但是，这些情况对于实际的issue宽度是不常见的。尽管如此，我们的计数器架构也会将这些情况处理为资源stalls或L1 Dcache misses，如4.3节所述。

However, the smooth flow of instructions is often disrupted by miss events. When a miss event occurs, the issuing of useful instructions eventually stops; there is then a period when no useful instructions are issued until the miss event is resolved and instructions can once again begin flowing. Here we emphasize useful; instructions on a mispredicted branch path are not considered to be useful.

但是，指令的平滑流动，经常会被miss events打断。当一个miss event发生时，有用指令的issue就会停止；那么肯定就会有一段时间，没有有用的指令进行issue，直到miss event解决掉，指令就可以再一次开始流动。这里，我们强调有用的指令，因为在误预测的分支的路径上的指令不被认为是有用的。

This interval behavior is illustrated in Figure 2. The number of instructions issued per cycle (IPC) is shown on the vertical axis and time (in clock cycles) is on the horizontal axis. As illustrated in the figure, the effects of miss events divide execution time into intervals. We define intervals to begin and end at the points where instructions just begin issuing following recovery from the preceding miss event. That is, the interval includes the time period where no instructions are issued following a particular miss event. By dividing execution time into intervals, we can analyze the performance behavior of the intervals individually. In particular, we can, based on the type of interval (the miss event that terminates it), describe and evaluate the key performance characteristics. Because the underlying interval behavior is different for frontend and backend miss events, we discuss them separately. Then, after having discussed isolated miss events, we will discuss interactions between miss events.

图2给出了这种区间行为。每周期issue的指令数量(IPC)如纵轴所示，时间如横轴所示。如图所述，miss events的效果将执行时间分割成区间。我们定义区间的开始和结束，是在指令刚开始issue的点，从之前的miss event刚刚恢复之后。即，区间要包含在特定的miss event发生后，没有指令进行issue的时间段。将执行时间分割成区间后，我们可以单独分析区间的性能行为。特别是，基于区间的类型（将其终结的miss event），我们可以描述和评估关键的性能特征。因为对于前端和后端的miss events，其区间行为是不同的，我们将其分别讨论。然后，在讨论孤立的miss events后，我们讨论miss events的相互作用。

### 3.1 Frontend misses

The frontend misses can be divided into I-cache and I-TLB misses, and branch mispredictions. 前端的misses可以分为I-cache和I-TLB misses，和分支误预测。

#### 3.1.1 Instruction cache and TLB misses

The interval execution curve for an L1 or L2 I-cache miss is shown in Figure 3 — because I-cache and I-TLB misses exhibit similar behavior (the only difference being the amount of delay), we analyze them collectively as ‘I-cache misses’. This graph plots the number of instructions issued (on the vertical axis) versus time (on the horizontal axis); this is typical behavior, and the plot has been smoothed for clarity. At the beginning of the interval, instructions begin to fill the window at a sustained maximum dispatch width and instruction issue and commit ramp up; as the window fills, the issue and commit rates increase toward the maximum value. Then, at some point, an instruction cache miss occurs. All the instructions already in the pipeline frontend must first be dispatched into the window before the window starts to drain. This takes an amount of time equal to the number of frontend pipeline stages, i.e., the number of clock cycles equal to the frontend pipeline length. Offsetting this effect is the time required to re-fill the frontend pipeline after the missed line is accessed from the L2 cache (or main memory). Because the two pipeline-length delays exactly offset each other, the overall penalty for an instruction cache miss equals the miss delay. Simulation data verifies that this is the case for the L1 I-cache, see Figure 4; we obtained similar results for the L2 I-cache and I-TLB. The L1 I-cache miss penalty seems to be (fairly) constant across all benchmarks. In these experiments we assume an L2 access latency of 9 cycles. The slight fluctuation of the I-cache miss penalty between 8 and 9 cycles is due to the presence of the fetch buffer in the instruction delivery subsystem.

L1或L2 I-cache miss的区间执行曲线如图3所示，由于I-cache和I-TLB misses表现出了类似的行为（唯一的区别在于延迟的量），我们将其统一按照I-cache misses进行分析。这幅图画的是发射的指令数量（纵轴）vs 时间（横轴）；这是典型的行为，图已经进行了平滑，这样可以看的更清楚。在区间的开始，指令开始注入窗口，有最大的派遣宽度，指令issue和commit开始爬坡；随着窗口的指令注入，issue和commit逐渐增加到最大值。然后，在某点上，发生了指令cache miss。已经在流水线前端的所有指令，都需要先派遣到窗口中，然后窗口才开始逐渐排空。这个所需要的时间，与前端流水线级数的级数相等，即，时钟周期数等于前端流水线长度。补偿这个效果所需的时间，是在missed line在L2 cache或主存内访问后，重新填满前端流水线的时间。因为两个流水线长度延迟是严格互相补偿的，指令cache miss的整体惩罚，就等于miss延迟。仿真数据验证了，对于L1 I-cache就是这个情况，见图4；对L2 I-cache和I-TLB，我们得到了类似的结果。L1 I-cache miss惩罚在所有benchmarks中似乎是常数。在这些试验中，我们假设L2访问延迟为9时钟周期。I-cache miss惩罚在8周期到9周期之间波动，是因为在指令传递子系统中存在fetch buffer。

Our proposed hardware performance counter mechanism, which will be detailed later, effectively counts the miss delay, i.e., it counts the number of cycles between the time the instruction cache miss occurs and the time newly fetched instructions start filling the frontend pipeline. These counts are then ascribed to either I-cache or I-TLB misses. The naive approach also computes the I-cache (or I-TLB) penalty in an accurate way multiplying the number of misses by the miss delay. The IBM POWER5 mechanism, in contrast, counts only the number of zero-completion cycles due to an empty ROB; this corresponds to the zero-region in Figure 3 after the ROB has drained. This means that the IBM POWER5 mechanism does not take into account the time to drain the ROB. This leads to a substantial underestimation of the real instruction cache (or TLB) miss penalty as shown in Figure 4. Note that in some cases, the IBM POWER5 mechanism may not ascribe any cycles to the instruction cache miss. This is the case when the window drain time takes longer than the miss delay, which can happen in case a largely filled ROB needs to be drained and there is low ILP or a significant fraction long latency instructions.

我们提出的硬件性能计数器机制，后面会详述，可以有效的对miss delay进行计数，如，会对指令cache miss发生的时间，新fetch的指令开始注入前端流水线的时间，准确的计数其周期数。这些计数然后被归因到I-cache或I-TLB misses。naive方法也用一种准确的方式来计算I-cache或I-TLB的惩罚，将misses的数量乘以miss延迟。而对比之下，IBM POWER5的机制对由于ROB为空造成的zero-completion的数量进行计数；这在图3中对应着ROB排空后的zero区域。这意味着，IBM POWER5机制并没有将排空ROB的时间进行计入。这带来了图4中对真实指令cache（或TLB）的miss惩罚的低估。注意，在一些情况中，IBM POWER5的机制不会将任何时钟周期归因到I-cache miss。当窗口的排空时间比miss延迟要长时，就会这样；这在ROB接近充满的时候需要排空，而且有一个低ILP或明显的长延迟指令时，就会发生。

#### 3.1.2 Branch mispredictions

Figure 5 shows the timing for a branch misprediction interval. At the beginning of the interval, instructions begin to fill the window and instruction issue ramps up. Then, at some point, the mispredicted branch enters the window. At that point, the window begins to be drained of useful instructions (i.e., those that will eventually commit). Miss-speculated instructions following the mispredicted branch will continue filling the window, but they will not contribute to the issuing of good instructions. Nor, generally speaking, will they inhibit the issuing of good instructions if it is assumed that the oldest ready instructions are allowed to issue first. Eventually, when the mispredicted branch is resolved, the pipeline is flushed and is re-filled with instructions from the correct path. During this re-fill time, there is a zero-issue region where no instructions issue nor complete, and, given the above observation, the zero-region is approximately equal to the time it takes to re-fill the frontend pipeline.

图5展示了一个分支误预测的区间情况。在区间的开始，指令开始注入窗口，指令issue开始爬坡。然后，在某点，误预测的分支进入窗口。在那个点，窗口开始排空有用的指令（即，那些最终要commit的指令）。在误预测分支后，误预测的指令会持续的注入窗口，但它们不会对好的指令的issue有任何贡献。或者，一般来说，如果假设最老的已经ready的指令要首先进行issue，也不会阻碍好的指令的issue。最后，当误预测的分支解决后，流水线被冲刷，重新填入正确路径的指令。在重新填入的时间内，会有一个zero-issue的区域，没有指令issue或complete，有了上述观察后，这个zero区域约等于重新填满前端流水线所需的时间。

Based on the above interval analysis, it follows that the overall performance penalty due to a branch misprediction equals the difference between the time the mispredicted branch first enters the window and the time the first correct-path instruction enters the window following discovery of the misprediction. In other words, the overall performance penalty equals the branch resolution time, i.e., the time between the mispredicted branch entering the window and the branch being resolved, plus the frontend pipeline length. Eyerman et al. [4] have shown that the branch resolution time is subject to the interval length and the amount of ILP in the program; i.e., the longer the interval and the lower the ILP, the longer the branch resolution time takes. For many benchmarks, the branch resolution time is the main contributor to the overall branch misprediction penalty.

基于上面的区间分析我们可以得出，由于一个分支误预测导致的总体性能惩罚，等于误预测的分支首次进入窗口的时间，与发现误预测后正确路径上的第一条指令进入窗口的时间之差。换句话说，整体性能惩罚等于分支解析的时间，即，误预测的分支进入窗口的时间，与分支被解决的时间之差，加上前端流水线长度。Eyerman等[4]已经展示了，分支解析的时间，取决于区间长度和程序中ILP的数量；即，区间越长，ILP越低，分支解析所耗的时间越长。对很多benchmarks，分支解析时间是对整体分支误预测惩罚的主要贡献者。

Based on the interval analysis it follows that in order to accurately compute the branch misprediction penalty, a hardware performance counter mechanism requires knowledge of when a mispredicted branch entered the ROB. And this has to be reflected in the hardware performance counter architecture (as is the case in our proposed architecture). None of the existing approaches, however, employ such an architecture, and, consequently, these approaches are unable to compute the true branch misprediction penalty; see Figure 6. The naive approach typically ascribes the frontend pipeline length as the branch misprediction penalty which is a significant underestimation of the overall branch misprediction penalty. The IBM POWER5 mechanism only counts the number of zero-completion cycles on an empty ROB as a result of a branch misprediction. This is even worse than the naive approach as the number of zero-completion cycles can be smaller than the frontend pipeline length.

基于区间分析我们可以得出，为了准确的计算分支误预测的惩罚，硬件性能计数器机制需要误预测分支进入ROB的时间的知识。这需要在硬件性能计数器架构中进行反应（在我们提出的架构中，就是这个情况）。但是，现有的方法中，没有采用这样一种架构的，结果是，这些方法不能计算出真实的分支误预测惩罚；见图6。naive方法一般将前端流水线长度就作为分支误预测的惩罚，这是对整体的分支误预测的惩罚的显著低估。IBM POWER5的机制只对分支误预测导致的空ROB的zero-completion周期进行计数。这比naive方法还要差，因为zero-completion的周期数比前端流水线长度还要小。

### 3.2 Backend misses

For backend miss events, we make a distinction between events of short and long duration. The short backend misses are L1 data cache misses; the long backend misses are the L2 data cache misses and D-TLB misses.

对于后端的miss events，我们进行了长持续时间和短持续时间事件的区分。短后端misses是L1 data cache misses；长后端misses是L2 data cache misses和D-TLB misses。

#### 3.2.1 Short misses

Short (L1) data cache misses in general do not lead to a period where zero instructions can be issued. Provided that the processor design is reasonably well-balanced, there will be a sufficiently large ROB (and related structures) so that the latency of short data cache misses can be hidden (overlapped) by the out-of-order execution of independent instructions. As such, we consider loads that miss in the L1 data cache in a similar manner as the way we consider instructions issued to long latency functional units (see section 4.3).

短(L1)data cache misses总体上不会导致有一段时间不能issue任何指令。如果处理器设计是良好均衡的，会有一个充分大的ROB（和相关的结构），这样短data cache misses的延迟可以很好的被无关指令的乱序执行隐藏掉（重叠掉）。这样，我们将L1 data cache中的load miss认为是与issue到长延迟功能单元的情况类似的（见4.3节）。

#### 3.2.2 Long misses

When a long data cache miss occurs, i.e., from the L2 to main memory, the memory delay is typically quite large — on the order of a hundred or more cycles. Similar behavior is observed for D-TLB misses. Hence, both are handled in the same manner.

当长的data cache miss发生时，即，从L2到主存，内存延迟通常是非常大的，通常是一百或更多周期的级别。在D-TLB misses中也观察到了类似的行为。因此，两者都以相同的方式处理。

On an isolated long data cache miss, the ROB eventually fills because the load blocks the ROB head, then dispatch stops, and eventually issue and commit cease [8]. Figure 7 shows the performance of an interval that contains a long data cache miss where the ROB fills while the missing load instruction blocks at the head of the ROB. After the miss data returns from memory, instruction issuing resumes. The total long data cache miss penalty equals the time between the ROB fill and the time data returns from memory.

在一个孤立的长data cache miss中，ROB最终会充满，因为load阻碍了ROB头，然后dispatch停止，最终issue和commit停止。图7展示了一个包含长data cache miss的区间的性能，其中missing load指令阻碍了ROB的头部，所以ROB充满了。在确实的数据从内存中返回时，指令发射恢复。总计的长数据cache miss惩罚，等于ROB充满，和数据从内存返回的时间之差。

Next, we consider the influence of a long D-cache miss that closely follows another long D-cache miss — we assume that both L2 data cache misses are independent of each other, i.e., the first load does not feed the second load. By ‘closely’ we mean within the W (window size or ROB size) instructions that immediately follow the first long D-cache miss; these instructions will make it into the ROB before it blocks. If additional long D-cache misses occur within the W instructions immediately following another long D-cache miss, there is no additional performance penalty because their miss latencies are essentially overlapped with the first. This is illustrated in Figure 8. Here, it is assumed that the second miss follows the first by S instructions. When the first load’s miss data returns from memory, then the first load commits and no longer blocks the head of the ROB. Then, S new instructions are allowed to enter the ROB. This will take approximately S/I cycles with I being the dispatch width — just enough time to overlap the remaining latency from the second miss. Note that this overlap holds regardless of S, the only requirement is that S is less than or equal to the ROB size W. A similar argument can be made for any number of other long D-cache misses that occur with W instructions of the first long D-cache miss.

下一步，我们考虑一个长D-cache miss紧随其后有另一个长D-cache miss的影响，我们假设，两个L2 data cache misses是相互独立的，即，第一个load与第二个load是没关系的。紧随的意思是，在第一个long D-cache miss后的W条指令后（W为window大小或ROB大小）；这些指令会在阻塞之前进入ROB。如果在一个长D-cache miss后的W条指令内，又发生了一个长D-cache misses，就不会有额外的性能惩罚，因为其miss延迟，与第一个的延迟是重叠的，如图8所示。这里假设，第二个miss是在第一个之后S条指令。当第一个load的miss数据从内存中返回，那么第一个load commits，就不会阻碍到ROB的头部。然后，S条新指令可以进入ROB。这大约会需要S/I个周期，其中I是dispatch的宽度，这个时间足以与第二个miss的剩余的延迟相重叠。注意，这种重叠与S无关，唯一的要求是，S要小于或等于ROB的大小W。在第一个长的D-cache miss后W条指令，如果有任意条长D-cache misses指令，都可以得出类似的结论。

Based on this analysis, we conclude that the penalty for an isolated miss as well as for overlapping long data cache misses, equals the time between the ROB filling up and the data returning from main memory. And this is exactly what our proposed hardware performance counter mechanism counts. In contrast, the naive approach ascribes the total miss latency to all long backend misses, i.e., the naive approach does not take into account overlapping long backend misses. This can lead to severe overestimations of the real penalties, see Figure 9. The IBM POWER5 mechanism on the other hand, makes a better approximation of the real penalty and starts counting the long data cache miss penalty as soon as the L2 data cache miss reaches the head of the ROB. By doing so, the IBM POWER5 ascribes a single miss penalty to overlapping backend misses. This is a subtle difference with our mechanism; the IBM POWER5 approach starts counting as soon as the L2 data cache miss reaches the head of the ROB, whereas the method we propose waits until the ROB is effectively filled up. As such, our method does not count for the amount of work that can be done in overlap with the long D-cache miss, i.e., filling up the ROB. This is a small difference in practice, however; see Figure 9.

基于这种分析，我们得出结论，对一个孤立的miss，以及重叠的长data cache misses，其惩罚等于，ROB充满和数据从主存中返回所需要的时间，而这正是我们提出的硬件性能计数器的机制所给出的结果。对比之下，naive方法将总计的miss延迟归因为所有的长后端misses，即，naive方法并没有将重叠的长后端misses纳入考虑。这会导致对真实惩罚的严重的过度估计，如图9。IBM POWER5则对真实的惩罚进行了更好的近似，在L2 data cache miss达到了ROB的头部时，就开始对长data cache miss惩罚进行计数。这样，IBM POWER5将单个miss惩罚归因于重叠的后端misses。这与我们的机制有略微的区别；IBM POWER5方法在L2 data cache miss达到了ROB头部时就开始计数，而我们的方法则等待直到ROB已经充满了以后。这样，我们的方法并没有对那些与长D-cache miss有重叠的部分进行计数，即，填充ROB的时间。这在实践中是一个很小的差异；见图9。

### 3.3 Interactions between miss events

Thus far, we have considered the various miss event types in isolation. However, in practice, miss events do not occur in isolation; they interact with other miss events. Accurately dealing with these interactions is crucial for building meaningful CPI stacks since we do not want to double-count miss event penalties. We first treat interactions between frontend miss events. We then discuss interactions between frontend and backend miss events.

迄今，我们考虑孤立的考虑了各种miss event的类型。但是，在实践中，miss events不会孤立的发生；它们会与其他miss events相互作用。准确的处理这些相互作用，对构建有意义的CPI stacks是关键的，因为我们不希望对miss events的惩罚进行双重计数。我们首先处理前端miss events之间的相互作用，然后我们讨论前端和后端miss events之间的相互作用。

#### 3.3.1 Interactions between frontend miss events

The degree of interaction between frontend pipeline miss events (branch mispredictions, I-cache misses and I-TLB misses) is limited because the penalties do not overlap. That is, frontend pipeline miss events serially disrupt the flow of good instructions so their negative effects do not overlap. The only thing that needs to be considered when building accurate CPI stacks is that the penalties due to frontend pipeline miss events along mispredicted control flow paths should not be counted. For example, the penalty due to an I-cache miss along a mispredicted path should not be counted as such. The naive approach does count all I-cache and I-TLB misses, including misses along mispredicted paths, which could lead to inaccurate picture of the real penalties. The naive_non_spec method, the IBM POWER5 mechanism as well as our method do not count I-cache and I-TLB misses along mispredicted paths.

前端流水线miss events (branch mispredictions, I-cache misses和I-TLB misses)之间相互作用的程度是有限的，因为它们的惩罚不会重叠。即，前端流水线miss events顺序的打断好指令流，所以其负面效应不会叠加。在构建准确的CPI stacks时，唯一需要考虑的事，是由于前端流水线沿着误预测的控制流路径的miss events的惩罚不应当被计入考虑。比如，沿着一个误预测的路径的I-cache miss的惩罚，不应当被计入。naive方法对所有I-cache和I-TLB的miss而是都进行计数，包括沿着误预测路径的misses，这会对真实的惩罚进行不准确的估计。e而naive_non_spec方法，IBM POWER5机制以及我们的方法，并不会沿着误预测的路径对I-cache和I-TLB的misses进行计数。

#### 3.3.2 Interactions between frontend and long backend miss events

The interactions between frontend pipeline miss events and long backend miss events are more complex because frontend pipeline miss events can be overlapped by long backend miss events. The question then is: how do we account for both miss event penalties? For example, in case a branch misprediction overlaps with a long D-cache miss, do we account for the branch misprediction penalty, or do we ignore the branch misprediction penalty, saying that it is completely hidden under the long D-cache miss? In order to answer these questions we measured the fraction of the total cycle count for which overlaps are observed between frontend miss penalties (L1 and L2 I-cache miss, I-TLB miss and branch mispredictions) and long backend miss penalties. The fraction of overlapped cycles is generally very small, as shown in Table 1; no more than 1% for most benchmarks, and only as much as 5% for a couple of benchmarks. Since the fraction overlapped cycles is very limited, any mechanism for dealing with it will result in relatively accurate and meaningful CPI stacks. Consequently, we opt for a hardware performance counter implementation that assigns overlap between frontend and long backend miss penalties to the frontend CPI component, unless the ROB is full (which triggers counting the long backend miss penalty). This implementation results in a simple hardware design.

前端流水线miss events和长的后端miss events的相互作用是更复杂的，因为前端流水线miss events可以与长后端miss events相互叠加。这样，问题就变成了：我们怎么对两种miss event的惩罚进行计数？比如，在一个分支误预测与一个长D-cache miss叠加时，我们是计入分支误预测惩罚，还是忽略分支误预测惩罚，认为这是完全隐藏在长D-cache miss后？为了回答这些问题，我们测量了前端miss和长后端miss叠加情况下的总计周期技术惩罚的比例。叠加的周期的比例，一般是很小的，如表1所示；对于多数benchmarks，都少于1%，对于几个benchmarks，最多到5%。由于叠加周期的比例很小，处理这种情况的任何机制，都会得到相对精确和有意义的CPI stacks。结果是，我们选择的硬件性能计数器实现，将前端和长后端miss的惩罚指定给前端CPI组件，除非ROB满了（这会触发对长后端miss惩罚进行计数）。这种实现会得到更简单的硬件设计。

## 4. Counter Architecture

In our proposed hardware performance counter architecture, we assume one total cycle counter and 8 global CPI component cycle counters for measuring ‘lost’ cycles due to L1 I-cache misses, L2 I-cache misses, I-TLB misses, L1 D-cache misses, L2 D-cache misses, D-TLB misses, branch mispredictions and long latency functional unit stalls. The idea is to assign every cycle to one of the global CPI component cycle counters when possible; the steady-state (baseline) cycle count then equals the total cycle count minus the total sum of the individual global CPI component cycle counters. We now describe how the global CPI component cycle counters can be computed in hardware. We make a distinction between frontend misses, backend misses, and long latency functional unit stalls.

在我们提出的硬件性能计数器架构中，我们假设有1个总计的周期计数器，和8个全局的CPI组件周期计数器，测量由于L1 I-cache misses, L2 I-cache misses, I-TLB misses, L1 D-cache misses, L2 D-cache misses, D-TLB misses, branch mispredictions和长延迟功能单元的stalls导致的丢失的周期。其思想是，当可能的时候，将每个周期都赋给全局CPI组件的周期计数器的其中一个；稳态（基准）周期计数，等于总计的周期计数，减去单个的全局CPI组件的周期计数器的和。我们现在描述，全局CPI组件周期计数器怎样在硬件中进行计算。我们对前端misses，后端misses和长延迟功能单元的stalls进行区分。

### 4.1 Frontend misses

#### 4.1.1 Initial design: FMT

To measure lost cycles due to frontend miss events, we propose a hardware table, called the frontend miss event table (FMT), that is implemented as shown in Figure 10. The FMT is a circular buffer and has as many rows as the processor supports outstanding branches. The FMT also has three pointers, the fetch pointer, the dispatch head pointer, and the dispatch tail pointer. When a new branch instruction is fetched and decoded, an FMT entry is allocated by advancing the fetch pointer and by initializing the entire row to zeros. When a branch dispatches, the dispatch tail pointer is advanced to point to that branch in the FMT, and the instruction's ROB ID is inserted in the 'ROB ID' column. When a branch is resolved and turns out to be a misprediction, the instruction's ROB ID is used to locate the corresponding FMT entry, and the 'mispredict' bit is then set. The retirement of a branch causes the dispatch head pointer to increment which de-allocates the FMT entry.

为测量由于前端miss events导致的丢失的时钟周期，我们提出了一个硬件表，称为前端miss event表(FMT)，其实现如图10所示。FMT是一个环形buffer，有很多行，等于处理器支持的outstanding分支数量。FMT还有三个指针，fetch指针，dispatch头指针，和dispatch尾指针。当一个新的分支指令被取指和译码，就分配一个FMT entry，递增fetch pointer，初始化整行为0。当一个分支dispatch，那么dispatch tail指针推进到指向FMT中的那个分支，指令的ROB ID插入到ROB ID列。当一个分支解析好，结果是一个误预测，指令的ROB ID被用于定位对应的FMT entry，然后设置mispredict位。一个分支的退休，导致dispatch head指针递增，将FMT entry释放掉。

The frontend miss penalties are then calculated as follows. Any cycle in which no instructions are fed into the pipeline due to an L1 or L2 I-cache miss or an I-TLB miss causes the appropriate local counter in the FMT entry (the one pointed to by the fetch pointer) to be incremented. For example, an L1 I-cache miss causes the local L1 I-cache miss counter in the FMT (in the row pointed to by the fetch pointer) to be incremented every cycle until the cache miss is resolved. By doing so, the miss delay computed in the local counter corresponds to the actual I-cache or I-TLB miss penalty (according to interval analysis).

前端miss惩罚计算如下。由于L1或L2 I-cache miss或I-TLB miss，导致没有指令送入流水线的任何时钟周期，会导致FMT entry中合适的局部计数器（fetch pointer指向的那个）来递增。比如，L1 I-cache miss导致FMT中的局部L1 I-cache miss counter（在fetch指针指向的那一行）每个周期递增，直到cache miss解决掉。这样，在局部计数器中计算得到的miss延迟，对应着实际的I-cache或I-TLB miss惩罚（根据区间分析）。

For branches, the local FMT 'branch penalty' counter keeps track of the number of lost cycles caused by a presumed branch misprediction. Recall that the branch misprediction penalty equals the number of cycles between the mispredicted branch entering the ROB and new instructions along the correct control flow path entering the ROB after branch resolution. Because it is unknown at dispatch time whether a branch is mispredicted, the proposed method computes the number of cycles each branch resides in the ROB. That is, the 'branch penalty' counter is incremented every cycle for all branches residing in the ROB, i.e., for all branches between the dispatch head and tail pointers in the FMT. This is done unless the ROB is full — we will classify cycles with a full ROB as long backend misses or long latency misses; this is the easiest way not to double-count cycles under overlapping miss events.

对于分支，局部的FMT分支惩罚计数器会追踪由于假设的分支误预测导致的丢失的周期数。回忆一下，分支误预测惩罚等于误预测的分支进入ROB和在分支解析后沿着正确的控制流路径的新指令进入ROB的周期时间之差。因为在dispatch时，一个分支是否是误预测是不知道的，提出的方法计算的是每个分支在ROB中停留的时钟周期数。即，分支惩罚计数器对所有的在ROB中停留的分支都会每周期递增，即，对所有的在FMT中dispatch head和tail指针的分支。除非ROB是满的，就这样做，我们会将满ROB的周期分类为长后端misses，或长延迟misses；这是在重叠的miss events下不进行重复计数的最简单的方法。

The global CPI component cycle counters are updated when a branch instruction completes: the local L1 I-cache, L2 I-cache and I-TLB counters are added to the respective global cycle counters. In case the branch is incorrectly predicted, then the value in the local 'branch penalty' counter is added to the global branch misprediction cycle counter. And from then on, the global branch misprediction cycle counter is incremented every cycle until new instructions enter the ROB. The resolution of a mispredicted branch also places the FMT dispatch tail pointer to point to the mispredicted branch entry and the FMT fetch pointer to point to the next FMT entry.

全局CPI组件周期计数器，在分支指令完成的时候进行更新：局部L1 I-cache, L2 I-cache和I-TLB计数器分别加入到全局周期计数器中。在分支被误预测时，然后在局部分支惩罚计数器中的值，要加入到全局分支误预测周期计数器中。从那时候开始，全局分支误预测周期计数器每周期递增，直到新指令进入到ROB中。误预测的分支的解析，也会将FMT dispatch tail指针放到指向误预测分支的entry，FMT fetch指针指向下一个FMT entry。

#### 4.1.2 Improved design: sFMT

The above design using the FMT makes a distinction between I-cache and I-TLB misses past particular branches, i.e., the local I-cache and I-TLB counters in the FMT are updated in the FMT entry pointed to by the fetch pointer and the fetch pointer is advanced as each branch is fetched. This avoids counting I-cache and I-TLB miss penalties past branch mispredictions. The price paid for keeping track of I-cache and I-TLB miss penalties along mispredicted paths is an FMT that requires on the order of a few hundred bits for storing this information. The simplified FMT design, which is called the shared FMT or sFMT, has only one shared set of local I-cache and I-TLB counters; see Figure 10. The sFMT requires that an ‘I-cache/I-TLB miss’ bit be provided with every entry in the ROB — this is also done in the Intel Pentium 4 and IBM POWER5 for tracking I-cache misses in the completion stage. Since there are no per-branch I-cache and I-TLB counters in the sFMT, the sFMT only requires a fraction of storage bits compared to the FMT. The sFMT operates in a similar fashion as the FMT: the local I-cache and I-TLB counters get updated on I-cache and I-TLB misses. The completion of an instruction with the ‘I-cache/I-TLB miss’ bit set (i) adds the local I-cache and I-TLB counters to the respective global counters, (ii) resets the local I-cache and I-TLB counters, and (iii) resets the ‘I-cache/I-TLB miss’ bits of all the instructions in the ROB. In case a mispredicted branch is completed, the local ‘branch penalty’ counter is added to the global branch misprediction cycle counter and the entire sFMT is cleared (including the local I-cache and I-TLB counters).

上述使用FMT的设计，区分了I-cache和I-TLB在特定分支上的misses，即，在FMT中的局部I-cache和I-TLB计数器，在fetch指针指向的FMT entry中进行更新，fetch指针在fetch到每个分支的时候进行推进。这避免了对分支误预测上的I-cache和I-TLB miss惩罚进行计数。在误预测的路径上追踪I-cache和I-TLB miss的惩罚所付出的代价，是需要一个有几百个bits来存储这些信息的FMT。简化的FMT设计，称为共享FMT或sFMT，只有一个共享的局部I-cache和I-TLB计数器集合；见图10。sFMT要求对ROB中的每个entry，都有一个I-cache/I-TLB miss bit，这在Intel Pentium 4和IBM POWER5中，在completion阶段追踪I-cache misses也进行了。由于在sFMT中没有每个分支的I-cache和I-TLB计数器，sFMT与FMT相比，只需要一部分存储bits。sFMT与FMT的操作方式是类似的：局部的I-cache和I-TLB计数器在I-cache和I-TLB misses的时候进行更新。一条指令的完成，带有I-cache/I-TLB位设置，(i)将局部I-cache和I-TLB计数器加到分别的全局计数器中，(ii)重置局部I-cache和I-TLB计数器，(iii)重置ROB中所有指令的I-cache/I-TLB miss位。在误预测的分支完成的情况下，局部的分支惩罚计数器加到全局分支误预测周期计数器上，整个sFMT清除（包括局部I-cache和I-TLB计数器）。

Clearing the sFMT on a branch misprediction avoids counting I-cache and I-TLB miss penalties along mispredicted paths. However, when an I-cache or I-TLB miss is followed by a mispredicted branch that in turn is followed by an I-cache or I-TLB miss, then the sFMT incurs an inaccuracy because it then counts I-cache and I-TLB penalties along mispredicted control flow paths. However, given the fact that I-cache misses and branch mispredictions typically occur in bursts, the number of cases where the above scenario occurs is very limited. As such, the additional error that we observe for sFMT compared to FMT, is very small, as will be shown later in the evaluation section.

在分支误预测的情况下，清除sFMT，会避免沿着误预测的路径对I-cache和I-TLB miss惩罚进行计数。但是，当I-cache或I-TLB miss后是一个误预测的分支，然后又是一个I-cache或I-TLB miss，那么sFMT会带来一个不准确的地方，因为会沿着误预测的控制流路径对Icache和I-TLB的惩罚进行计数。但是，既然I-cache misses和分支误预测通常都是突发的，上述场景发生的情况是非常少的。这样，与FMT相比，sFMT发生额外错误的情况是很少的，这会在评估的部分进行展示。

### 4.2 Long backend misses

Hardware performance counters for computing lost cycles due to long backend misses, such as long D-cache misses and D-TLB misses, are fairly easy to implement. These counters start counting when the ROB is full and if the instruction blocking the ROB is a L2 D-cache miss or D-TLB miss, respectively. For every cycle that these two conditions hold true, the respective cycle counters are incremented. Note that by doing so, we account for the long backend miss penalty as explained from interval analysis.

计算长后端misses所丢失的周期的硬件性能计数器，比如长D-cache misses和D-TLB misses，相对比较容易实现。这些计数器在ROB是满的，且阻挡ROB的指令是L2 D-cache miss或D-TLB miss时，开始计数。对每个周期，如果这两个条件为真，那么就递增分别的周期计数器。注意，通过这样做，我们对区间分析中的长后端miss惩罚进行了计数。

### 4.3 Long latency unit stalls

The hardware performance counter mechanism also allows for computing resource stalls under steady-state behavior. Recall that steady-state behavior in a balanced processor design implies that performance roughly equivalent to the maximum processor width is achieved in the absence of miss events, and that the ROB needs to be filled to achieve the steady state behavior. Based on this observation we can compute resource stalls due to long latency functional unit instructions (including short L1 data cache misses). If the ROB is filled and the instruction blocking the head of the ROB is an L1 D-cache miss, we count the cycle as an L1 D-cache miss cycle; or, if the instruction blocking the head of a full ROB is another long latency instruction, we count the cycle as a resource stall.

硬件性能计数器机制还可以对稳态行为下的计算资源stalls进行计数。回忆一下，平衡的处理器设计中的稳态行为，意味着在没有miss events的时候，性能大致等于最大的处理器宽度，要达到稳态行为，ROB需要被充满。基于这种观察，我们可以计算由于长延迟功能单元指令（包括短L1 data cache misses）导致的资源stalls。如果ROB充满了，阻塞ROB头部的指令，是L1 D-cache miss，我们将这个周期计数为L1 D-cache miss周期；或者，如果阻塞一个满的ROB头部的是另一个长延迟指令，我们将这个周期计数为一个资源stall。

## 5. Evaluation

### 5.1 Experimental Setup

We used SimpleScalar/Alpha v3.0 in our validation experiments. The benchmarks used, along with their reference inputs, are taken from the SPEC CPU 2000 benchmark suite,see Table 1. The binaries of these benchmarks were taken from the SimpleScalar website. In this paper, we only show results for the CPU2000 integer benchmarks. We collected results for the floating-point benchmarks as well; however, the CPI stacks for the floating-point benchmarks are less interesting than the CPI stacks for the integer benchmarks. Nearly all the floating-point benchmarks show very large L2 D-cache CPI components; only a few benchmarks exhibit significant L1 I-cache CPI components and none of the benchmarks show substantial branch misprediction CPI components. The baseline processor model is given in Table 2.

我们在验证试验中使用SimpleScalar/Alpha v3.0。使用的benchmarks，和其参考输入，是从SPEC CPU 2000 benchmark包中取出的，见表1。这些benchmarks的binaries是从SimpleScalar网站上取到的。本文中，我们只展示了CPU2000整数benchmarks的结果。我们收集了浮点benchmarks的这些结果；但是，浮点benchmarks的CPI stacks，比整数benchmarks的CPI stacks更加无趣。几乎所有的浮点benchmarks都有很大的L2 D-cache CPI部分；；只有几个benchmarks展现出了显著的L1 I-cache CPI组件，没有benchmarks有明显的分支误预测CPI部分。基准处理器模型如表2所示。

### 5.2 Results

This section evaluates the proposed hardware performance counter mechanism. We compare our two hardware implementations, FMT and sFMT, against the IBM POWER5 mechanism, the naive and naive_non_spec approaches and two simulation-based CPI stacks.

本节评估了提出的硬件性能计数器机制。我们与IBM POWER5机制，比较了我们的两个硬件实现，FMT和sFMT，还有naive和naive_non_spec方法，还有两种基于仿真的CPI stacks。

The simulation-based CPI stacks will serve as a reference for comparison. We use two simulation-based stacks because of the difficulty in defining what a standard ‘correct’ CPI stack should look like. In particular, there will be cycles that could reasonably be ascribed to more than one miss event. Hence, if we simulate CPI values in a specific order, we may get different numbers than if they are simulated in a different order. To account for this effect, two simulation-based CPI stacks are generated as follows. We first run a simulation assuming perfect branch prediction and perfect caches, i.e., all branches are correctly predicted and all cache accesses are L1 cache hits. This yields the number of cycles for the base CPI. We subsequently run a simulation with a real L1 data cache. The additional cycles over the first run (which assumes a perfect L1 data cache) gives the CPI component due to L1 data cache misses. The next simulation run assumes a real L1 data cache and a real branch predictor; this computes the branch misprediction CPI component. For computing the remaining CPI components, we consider two orders. The first order is the following: L1 I-cache, L2 I-cache, I-TLB, L2 D-cache and D-TLB; the second order, called the ‘inverse order’, first computes the L2 D-cache and D-TLB components and then computes the L1 I-cache, L2 I-cache and I-TLB CPI components. Our simulation results show that the order in which the CPI components are computed only has a small effect on the overall results. This follows from the small percentages of cycles that process overlapping frontend and backend miss event penalties, as previously shown in Table 1.

Figure 11 shows normalized CPI stacks for the SPECint2000 benchmarks for the simulation-based approach, the naive and naive_non_spec approach, the IBM POWER5 approach, and the proposed FMT and sFMT approaches. Figure 12 summarizes these CPI stacks by showing the maximum CPI component errors for the various CPI stack building methods.

Figure 11 shows that the naive approach results in CPI stacks that are highly inaccurate (and not even meaningful) for some of the benchmarks. The sum of the miss event counts times the miss penalties is larger than the total cycle count; this causes the base CPI, which is the total cycle count minus the miss event cycle count, to be negative. This is the case for a number of benchmarks, such as gap, gcc, mcf, twolf and vpr, with gcc the most notable example. The reason the naive approach fails in building accurate CPI stacks is that the naive approach does not adequately deal with overlapped long backend misses, does not accurately compute the branch misprediction penalty, and in addition, it counts I-cache (and I-TLB) misses along mispredicted paths. However, for benchmarks that have very few overlapped backend misses and very few I-cache misses along mispredicted paths, the naive approach can be fairly accurate, see for example eon and perlbmk. The naive_non_spec approach which does not count miss events along mispredicted paths, is more accurate than the naive approach, however, the CPI stacks are still not very accurate compared to the simulation-based CPI stacks.

The IBM POWER5 approach clearly is an improvement compared to the naive approaches. For the benchmarks where the naive approaches failed, the IBM POWER5 mechanism succeeds in producing meaningful CPI stacks. However, compared to the simulation-based CPI stacks, the IBM POWER5 CPI stacks are still inaccurate, see for example crafty, eon, gap, gzip, perlbmk and vortex. The reason the IBM POWER5 approach falls short is that the IBM POWER5 mechanism underestimates the I-cache miss penalty as well as the branch misprediction penalty.

The FMT and sFMT CPI stacks track the simulation-based CPI stacks very closely. Whereas both the naive and IBM POWER5 mechanisms show high errors for several benchmarks, the FMT and sFMT architectures show significantly lower errors for all benchmarks. All maximum CPI component errors are less than 4%, see Figure 12. The average error for FMT and sFMT is 2.5% and 2.7%, respectively.

## 6. Related Work

The Intel Itanium processor family provides a rich set of hardware performance counters for computing CPI stacks [7]. These hardware performance monitors effectively compute the number of lost cycles under various stall conditions such as branch mispredictions, cache misses, etc. The Digital Continuous Profiling Infrastructure (DCPI) [2] is another example of a hardware performance monitoring tool for an in-order architecture. Computing CPI stacks for in-order architectures, however, is relatively easy compared to computing CPI stacks on out-of-order architectures.

Intel Itanium处理器族提供了大量硬件性能计数器，以计算CPI stacks。这些硬件性能计数器有效的计算了在各种stall条件下丢失的时钟周期数，比如分支误预测，cache misses，等。DCPI是在顺序架构上另一个硬件性能监控工具的例子。但是，与在乱序架构上计算CPI stacks相比，对一个顺序架构计算CPI stacks是相对容易的。

Besides the IBM POWER5 mechanism, other hardware profiling mechanisms have been proposed in the recent past for out-of-order architectures. However, the goal for those methods is quite different from ours. Our goal is to build simple and easy-to-understand CPI stacks, whereas the goal for the other approaches is detailed per-instruction profiling. For example, the ProfileMe framework [3] randomly samples individual instructions and collects cycle-level information on a per-instruction basis. Collecting aggregate CPI stacks can be done using the ProfileMe framework by profiling many randomly sampled instructions and by aggregating all of their individual latency information. An inherent limitation with this approach is that per-instruction profiling does not allow for modeling overlap effects. The ProfileMe framework partially addresses this issue by profiling two potentially concurrent instructions. Shotgun profiling [5] tries to model overlap effects between multiple instructions by collecting miss event information within hot spots using specialized hardware performance counters. A postmortem analysis then determines, based on a simple processor model, the amount of overlaps and interactions between instructions within these hot spots. Per-instruction profiling has the inherent limitation of relying on (i) sampling which may introduce inaccuracy, (ii) per-instruction information for computing overlap effects, and (iii) interrupts for communicating miss event information from hardware to software which may lead to overhead and/or perturbation issues.

除了IBM POWER5机制，对乱序架构最近还提出了其他硬件profiling机制。但是，这些方法的目的与我们相比是非常不同的。我们的目标是构建简单的，容易理解的CPI stacks，而其他方法的目的是详细的每条指令的profiling。比如，ProfileMe框架随机采样单条指令，在逐条指令的基础上收集周期级的信息。收集累积的CPI stacks可以使用ProfileMe框架进行，对很多随机采样的指令进行profiling，然后将其单个的延迟信息进行累积。这种方法的内在限制是，每条指令的profiling，不会对重叠的效果进行建模。ProfileMe框架部分的解决了这个问题，对两个可能是并行的指令进行profiling。Shotgun profiling[5]尝试对多条指令的重叠效果进行建模，使用专用的硬件性能计数器，对热点中的miss event信息进行收集。程序运行完成后的分析，在基于简单处理器模型，可以确定这些热点中指令之间的重叠和相互作用。每指令的profiling的内在局限是，依赖于(i)采样，这会引入不准确性，(ii)在计算重叠效果时，会基于每条指令的信息，(iii)将miss event信息在硬件到软件进行通信时，要用中断，这可能会导致有代价，和/或打扰的问题。

A number of researchers have looked at superscalar processor models [9, 14, 15, 16, 19], but there are three primary efforts that led to the interval model. First, Michaud et al. [14] focused on performance aspects of instruction delivery and modeled the instruction window and issue mechanisms. Second, Karkhanis and Smith [9] extended this type of analysis to all types of miss events and built a complete performance model, which included a sustained steady state performance rate punctuated with gaps that occurred due to miss events. Independently, Taha and Wilson [19] broke instruction processing into intervals (which they call ‘macro blocks’). However, the behavior of macro blocks was not analytically modeled, but was based on simulation. Interval analysis combines the Taha and Wilson approach with the Karkhanis and Smith approach to miss event modeling. The interval model represents an advance over the Karkhanis and Smith ‘gap’ model because it handles short interval behavior in a more straightforward way. The mechanistic interval model presented here is similar to an empirical model of Hartstein and Puzak [6]; however, being an empirical model, it cannot be used as a basis for understanding the mechanisms that contribute to the CPI components.

一些研究者研究了超标量处理器模型，但有三个主要的工作，与区间模型有关。第一，[14]等聚焦在指令传递和性能方面，对指令窗口和发射机制进行了建模。第二，[9]拓展这种类型的分析，到所有类型的miss events，构建了一个完整的性能模型，这包括了持续的稳态性能，以及由于各种miss event所形成的gaps。[19]将指令处理分解成区间（他们称之为macro blocks）。但是，macro blocks的行为并没有进行解析建模，而是进行仿真。区间分析将[19]与[9]的工作结合了起来，形成了miss event建模。区间模型是[9]的gap模型的推进，因为对短区间行为以更直接的形式进行了处理。这里提出的区间模型与[6]的经验模型类似；但是，作为一种经验模型，不能用作理解对CPI组件进行贡献的机制的基础。

## 7. Conclusion

Computing CPI stacks on out-of-order processors is challenging because of various overlap effects between instructions and miss events. Existing approaches fail in computing accurate CPI stacks, the main reason being the fact that these approaches build CPI stacks in a bottom up fashion by counting miss events without regard on how these miss events affect overall performance. A top down approach on the other hand, starts from a performance model, interval analysis, that gives insight into the performance impacts of miss events. These insights then reveal how the hardware performance counter architecture should look like for building accurate CPI stacks. This paper proposed such a hardware performance counter architecture that is comparable to existing hardware performance counter mechanisms in terms of complexity, yet it achieves much greater accuracy.

在乱序处理器上计算CPI stack是很有挑战的，因为各种指令和miss events的重叠效果。现有的方法不能计算准确的CPI stacks，主要的原因是，这些方法以自下而上的方式构建CPI stacks，对miss events进行计数，而没有考虑这些miss events怎样影响整体的性能。另一方面，自顶向下的方法从性能模型开始，区间分析，给出了miss events读性能的影响的洞见。这些洞见揭示了，硬件性能计数器架构应该是怎样的，才能构建准确的CPI stacks。本文提出了这样一种硬件性能计数器架构，在复杂度上与现有的硬件性能计数器机制可比，但获得了高的多的准确率。