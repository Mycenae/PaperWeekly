# Visualizing Data using t-SNE

Laurens van der Maaten, Geoffrey Hinton

## 0. Abstract

We present a new technique called "t-SNE" that visualizes high-dimensional data by giving each datapoint a location in a two or three-dimensional map. The technique is a variation of Stochastic Neighbor Embedding (Hinton and Roweis, 2002) that is much easier to optimize, and produces significantly better visualizations by reducing the tendency to crowd points together in the center of the map. t-SNE is better than existing techniques at creating a single map that reveals structure at many different scales. This is particularly important for high-dimensional data that lie on several different, but related, low-dimensional manifolds, such as images of objects from multiple classes seen from multiple viewpoints. For visualizing the structure of very large data sets, we show how t-SNE can use random walks on neighborhood graphs to allow the implicit structure of all of the data to influence the way in which a subset of the data is displayed. We illustrate the performance of t-SNE on a wide variety of data sets and compare it with many other non-parametric visualization techniques, including Sammon mapping, Isomap, and Locally Linear Embedding. The visualizations produced by t-SNE are significantly better than those produced by the other techniques on almost all of the data sets.

我们提出了一种新技术，称为t-SNE，对高维数据进行可视化，方法是对每个数据点在二维或三位图中都给出一个位置。这个技术是随机邻域嵌入(SNE)的一种变化，优化起来容易的多，通过降低在图中央点聚集的趋势，可以产生明显更好的可视化效果。t-SNE比现有的方法要好，创建了一个图，在很多不同尺度揭示了数据的结构。这对于高维数据来说非常重要，在几个不同的，但相关的，低维流行上存在，比如多个类别从多个视角观察的目标的图像。为对非常大型数据集的结构进行可视化，我们展示了，t-SNE可以使用在邻域图上的随机行走，使所有数据的隐式结构来影响数据子集展示的方式。我们在很多数据集上展示了t-SNE的性能，与很多其他非参数化的可视化技术进行了比较，包括Sammon mapping, Isomap, 和Locally Linear Embedding。在几乎所有数据集中，t-SNE产生的可视化都比其他方法产生的明显要好。

**Keywords**: visualization, dimensionality reduction, manifold learning, embedding algorithms, multidimensional scaling

## 1. Introduction

Visualization of high-dimensional data is an important problem in many different domains, and deals with data of widely varying dimensionality. Cell nuclei that are relevant to breast cancer, for example, are described by approximately 30 variables (Street et al., 1993), whereas the pixel intensity vectors used to represent images or the word-count vectors used to represent documents typically have thousands of dimensions. Over the last few decades, a variety of techniques for the visualization of such high-dimensional data have been proposed, many of which are reviewed by de Oliveira and Levkowitz (2003). Important techniques include iconographic displays such as Chernoff faces (Chernoff, 1973), pixel-based techniques (Keim, 2000), and techniques that represent the dimensions in the data as vertices in a graph (Battista et al., 1994). Most of these techniques simply provide tools to display more than two data dimensions, and leave the interpretation of the data to the human observer. This severely limits the applicability of these techniques to real-world data sets that contain thousands of high-dimensional datapoints.

高维数据的可视化，在很多不同的领域都是一个重要的问题，处理的数据维度变化非常大。比如，与乳腺癌相关的细胞核，是由大约30个变量描述的，而用来表示图像的像素灰度向量，或用户表示文档的单词数量向量，一般维度都会达到数千。在过去几十年，提出了很多这样的高维数据可视化的方法，03年的文献综述了其中的很多。重要的技术包括，iconographic展示，比如Chernoff faces，基于像素的技术，和将数据中的维度表示为图中的顶点的技术。多数这些技术只是给出了展示二维数据的工具，将数据的插值留给了人类观察者。这严重限制了这些技术在真实世界数据集中的应用，因为包含了数千个高维数据点。

In contrast to the visualization techniques discussed above, dimensionality reduction methods convert the high-dimensional data set X = {x1,x2,...,xn} into two or three-dimensional data Y = {y1,y2,...,yn} that can be displayed in a scatterplot. In the paper, we refer to the low-dimensional data representation Y as a map, and to the low-dimensional representations yi of individual datapoints as map points. The aim of dimensionality reduction is to preserve as much of the significant structure of the high-dimensional data as possible in the low-dimensional map. Various techniques for this problem have been proposed that differ in the type of structure they preserve. Traditional dimensionality reduction techniques such as Principal Components Analysis (PCA; Hotelling, 1933) and classical multidimensional scaling (MDS; Torgerson, 1952) are linear techniques that focus on keeping the low-dimensional representations of dissimilar datapoints far apart. For high-dimensional data that lies on or near a low-dimensional, non-linear manifold it is usually more important to keep the low-dimensional representations of very similar datapoints close together, which is typically not possible with a linear mapping.

与上面讨论的可视化技术形成对比的是，降维方法将高维数据集X = {x1,x2,...,xn}转换成二维或三维数据Y = {y1,y2,...,yn}，可以在一个散点图中进行展示。本文中，我们称低维数据表示Y为一个图，将单个数据点的低维表示yi称为图中的点。降维的目的，是在低维图中，尽可能的保留高维数据的显著结构。对这个问题，提出了各种技术，其差别是保留的结构的类型。传统的降维技术，比如PCA和经典MDS是线性技术，关注的是保持不相似数据的低维表示仍然是分开的。对在或接近一个低维非线性流行上的高维数据，更重要的是，保持非常相似的数据点在低维表示中仍然是在一起的，用线性映射，这通常是不太可能的。

A large number of nonlinear dimensionality reduction techniques that aim to preserve the local structure of data have been proposed, many of which are reviewed by Lee and Verleysen (2007). In particular, we mention the following seven techniques: (1) Sammon mapping (Sammon, 1969), (2) curvilinear components analysis (CCA; Demartines and H´erault, 1997), (3) Stochastic Neighbor Embedding (SNE; Hinton and Roweis, 2002), (4) Isomap (Tenenbaum et al., 2000), (5) Maximum Variance Unfolding (MVU; Weinberger et al., 2004), (6) Locally Linear Embedding (LLE; Roweis and Saul, 2000), and (7) Laplacian Eigenmaps (Belkin and Niyogi, 2002). Despite the strong performance of these techniques on artificial data sets, they are often not very successful at visualizing real, high-dimensional data. In particular, most of the techniques are not capable of retaining both the local and the global structure of the data in a single map. For instance, a recent study reveals that even a semi-supervised variant of MVU is not capable of separating handwritten digits into
their natural clusters (Song et al., 2007).

已经提出了很多非线性降维技术，其目标是保持数据的局部结构。特别是，我们提到了下面的其中技术：(1) Sammon mapping, (2) 曲线成分分析CCA, (3) 随机邻域嵌入SNE, (4) Isomap, (5) 最大方差upfold(MVU), (6) 局部线性嵌入LLE, (7) Laplacian Eigenmaps. 在人工数据集上这些技术都有很好的性能，在对真实的、高维数据的可视化上，它们通常都不太好。特别是，多数这些技术都不能在单个图中保持数据的局部和全局结构。比如，最近的研究表明，即使是MVU的半监督变体，也不能将手写数字分成其自然的聚类。

In this paper, we describe a way of converting a high-dimensional data set into a matrix of pairwise similarities and we introduce a new technique, called "t-SNE", for visualizing the resulting similarity data. t-SNE is capable of capturing much of the local structure of the high-dimensional data very well, while also revealing global structure such as the presence of clusters at several scales. We illustrate the performance of t-SNE by comparing it to the seven dimensionality reduction techniques mentioned above on five data sets from a variety of domains. Because of space limitations, most of the (7+1)×5 = 40 maps are presented in the supplemental material, but the maps that we present in the paper are sufficient to demonstrate the superiority of t-SNE. The outline of the paper is as follows. In Section 2, we outline SNE as presented by Hinton and Roweis (2002), which forms the basis for t-SNE. In Section 3, we present t-SNE, which has two important differences from SNE. In Section 4, we describe the experimental setup and the results of our experiments. Subsequently, Section 5 shows how t-SNE can be modified to visualize realworld data sets that contain many more than 10,000 datapoints. The results of our experiments are discussed in more detail in Section 6. Our conclusions and suggestions for future work are presented in Section 7.

本文中，我们描述了将高维数据集转化成一个成对相似性的矩阵的一种方法，提出了一种新技术，称为t-SNE，对得到的相似性数据进行可视化。t-SNE可以很好的捕获大部分高维数据的局部结构，还能揭示全局结构，比如几种尺度上聚类的存在。我们将t-SNE与上述的7种降维技术，在不同领域中的5个数据集上进行比较，展示了其性能。因为空间限制，40个图多数都在附属材料中，但我们在本文中展示的图，足以证明t-SNE的优势。本文梗概如下。在第2部分，我们概述了SNE，这是t-SNE的基础。在第3部分，我们给出了t-SNE，与SNE相比有两个重要差异。在第4部分，我们描述了试验设置，和试验的结果。最后，第5部分表明，t-SNE可以进行修改，以对真实世界数据集进行可视化，数据集包含超过10000个数据点。第6部分讨论了试验结果的细节。第7部分给出了结论和未来工作的建议。

## 2. Stochastic Neighbor Embedding

Stochastic Neighbor Embedding (SNE) starts by converting the high-dimensional Euclidean distances between datapoints into conditional probabilities that represent similarities. The similarity of datapoint $x_j$ to datapoint $x_i$ is the conditional probability, $p_{j|i}$, that $x_i$ would pick $x_j$ as its neighbor if neighbors were picked in proportion to their probability density under a Gaussian centered at $x_i$. For nearby datapoints, $p_{j|i}$ is relatively high, whereas for widely separated datapoints, $p_{j|i}$ will be almost infinitesimal (for reasonable values of the variance of the Gaussian, $σ_i$). Mathematically, the conditional probability $p_{j|i}$ is given by

SNE的开始，将数据点之间的高维欧式距离，转化成表示相似度的条件概率。数据点$x_j$与$x_i$的相似度就是条件概率$p_{j|i}$，如果邻域是按照以$x_i$为中心的高斯函数的概率密度的比例挑选的，那么$x_i$就会挑选$x_j$在其邻域中。对于附近的数据点，$p_{j|i}$是相对很高的，而对于广泛的分隔的数据点，$p_{j|i}$的值就很小了（对于高斯方差的合理值，$σ_i$)。数学上来说，条件概率$p_{j|i}$由下式给出

$$p_{j|i} = \frac {exp(-||x_i-x_j||^2/2σ_i^2)} {\sum_{k \neq i} exp(-||x_i-x_k||^2/2σ_i^2)}$$(1)

where $σ_i$ is the variance of the Gaussian that is centered on datapoint $x_i$. The method for determining the value of $σ_i$ is presented later in this section. Because we are only interested in modeling pairwise similarities, we set the value of $p_{i|i}$ to zero. For the low-dimensional counterparts $y_i$ and $y_j$ of the high-dimensional datapoints $x_i$ and $x_j$, it is possible to compute a similar conditional probability, which we denote by $q_{j|i}$. We set the variance of the Gaussian that is employed in the computation of the conditional probabilities $q_{j|i}$ to $1/ \sqrt2$. Hence, we model the similarity of map point $y_j$ to map point $y_i$ by

其中$σ_i$是中心在数据点$x_i$上的高斯函数的方差。确定$σ_i$的值的方法，本节后面再进行介绍。因为我们只对建模成对的相似性有兴趣，我们设$p_{i|i}$为0。$x_i$和$x_j$的低维对应为$y_i$和$y_j$，可以计算得到一个类似的条件概率，我们用$q_{j|i}$表示。我们设计算条件概率$q_{j|i}$时的高斯方差为$1/ \sqrt2$。因此，我们计算图中的点$y_j$到点$y_i$的相似度图为

$$q_{j|i} = \frac {exp(-||y_i-y_j||^2)} {\sum_{k \neq i} exp(-||y_i-y_k||^2)}$$

Again, since we are only interested in modeling pairwise similarities, we set $q_{i|i}$ = 0. 这里我们还是只对成对的相似度感兴趣，所以我们再次设$q_{i|i}$ = 0。

If the map points $y_i$ and $y_j$ correctly model the similarity between the high-dimensional datapoints $x_i$ and $x_j$, the conditional probabilities $p_{j|i}$ and $q_{j|i}$ will be equal. Motivated by this observation, SNE aims to find a low-dimensional data representation that minimizes the mismatch between $p_{j|i}$ and $q_{j|i}$. A natural measure of the faithfulness with which $q_{j|i}$ models $p_{j|i}$ is the Kullback-Leibler divergence (which is in this case equal to the cross-entropy up to an additive constant). SNE minimizes the sum of Kullback-Leibler divergences over all datapoints using a gradient descent method. The cost function C is given by

如果图中的点$y_i$和$y_j$对高维数据点$x_i$和$x_j$的相似度进行了正确的建模，条件概率$p_{j|i}$和$q_{j|i}$将会相等。受到这个观察推动，SNE的目标是找到一个低维数据表示，使得$p_{j|i}$和$q_{j|i}$的不匹配最小化。$q_{j|i}$对$p_{j|i}$建模的忠实性的一个自然度量，是Kullback-Leibler散度（在这个情况中，等于交叉熵和一个加性常数）。SNE对所有数据点上的Kullback-Leibler散度的最小化，使用梯度下降方法进行。代价函数C由下式给出

$$C = \sum_i KL(P_i||Q_i) = \sum_i \sum_j p_{j|i} log \frac {p_{j|i}}{q_{j|i}}$$(2)

in which $P_i$ represents the conditional probability distribution over all other datapoints given datapoint $x_i$, and $Q_i$ represents the conditional probability distribution over all other map points given map point $y_i$. Because the Kullback-Leibler divergence is not symmetric, different types of error in the pairwise distances in the low-dimensional map are not weighted equally. In particular, there is a large cost for using widely separated map points to represent nearby datapoints (i.e., for using a small $q_{j|i}$ to model a large $p_{j|i}$), but there is only a small cost for using nearby map points to represent widely separated datapoints. This small cost comes from wasting some of the probability mass in the relevant Q distributions. In other words, the SNE cost function focuses on retaining the local structure of the data in the map (for reasonable values of the variance of the Gaussian in the high-dimensional space, $σ_i$).

其中$P_i$代表给定数据点$x_i$时，在所有其他数据点上的条件概率分布，$Q_i$代表在给定图上的点$y_i$时，所有其他图点的条件概率分布。因为Kullback-Leibler散度并不是对称的，在低维图中的成对距离的不同类型的错误，并不是相同加权的。特别是，使用分布很广的图点来代表附近的数据点，其代价很大（即，使用小的$q_{j|i}$，来对大的$p_{j|i}$进行建模），但使用附近的图点来代表广泛分布的数据点，其代价很小。这种很小的代价，是在相关的Q分布中浪费一些概率得到的。换句话说，SNE代价函数聚焦在在图中保持数据的局部结构（在高维空间中的高斯函数的方差$σ_i$的值要合理）。

The remaining parameter to be selected is the variance $σ_i$ of the Gaussian that is centered over each high-dimensional datapoint, $x_i$. It is not likely that there is a single value of $σ_i$ that is optimal for all datapoints in the data set because the density of the data is likely to vary. In dense regions, a smaller value of $σ_i$ is usually more appropriate than in sparser regions. Any particular value of $σ_i$ induces a probability distribution, $P_i$, over all of the other datapoints. This distribution has an entropy which increases as $σ_i$ increases. SNE performs a binary search for the value of $σ_i$ that produces a $P_i$ with a fixed perplexity that is specified by the user. The perplexity is defined as

剩下要选择的参数，是高斯函数的方差$σ_i$，该函数的中心是在每个高维数据点$x_i$上的。数据集中所有数据点都有一个最优值$σ_i$，这是不太可能的，因为数据的密度很可能变化。在密集区域，更小的$σ_i$更合适一些；更稀疏的区域，更大的$σ_i$值更合适一些。$σ_i$值的任意具体选择都会带来在所有其他数据点上的一个概率分布$P_i$。$σ_i$增加，该分布的熵就增加。SNE对$σ_i$的值进行一个二值搜索，使产生的$P_i$的具有用户指定的固定的复杂度。该复杂度定义为

$$Perp(P_i) = 2^{H(P_i)}$$

where $H(P_i)$ is the Shannon entropy of $P_i$ measured in bits 其中$H(P_i)$是$P_i$的香侬熵，以bits来度量

$$H(P_i) = -\sum_j p_{j|i} log_2 p_{j|i}$$

The perplexity can be interpreted as a smooth measure of the effective number of neighbors. The performance of SNE is fairly robust to changes in the perplexity, and typical values are between 5 and 50.

该复杂度可以解释为有效邻域数量的平滑度量。SNE的性能对复杂度的变化是比较稳健的，典型的值为5到50之间。

The minimization of the cost function in Equation 2 is performed using a gradient descent method. The gradient has a surprisingly simple form 式(2)中的代价函数的最小化，是使用梯度下降方法进行的。梯度的形式非常简单

$$\frac {δC}{δy_i} = 2 \sum_j (p_{j|i}-q_{j|i}+p_{i|j}-q_{i|j})(y_i-y_j)$$

Physically, the gradient may be interpreted as the resultant force created by a set of springs between the map point $y_i$ and all other map points $y_j$. All springs exert a force along the direction ($y_i−y_j$). The spring between $y_i$ and $y_j$ repels or attracts the map points depending on whether the distance between the two in the map is too small or too large to represent the similarities between the two high-dimensional datapoints. The force exerted by the spring between $y_i$ and $y_j$ is proportional to its length, and also proportional to its stiffness, which is the mismatch $(p_{j|i}-q_{j|i}+p_{i|j}-q_{i|j})$ between the pairwise similarities of the data points and the map points.

从物理上来说，梯度可以解释为，图点$y_i$与所有其他图点$y_j$中的弹簧集创建的力的结果。所有弹簧都沿着方向($y_i−y_j$)施加力。$y_i$和$y_j$之间的弹簧吸引或排斥图点，这依赖于，图中两个点之间的距离，对于表示两个高维数据点之间的相似性，是太大还是太小。$y_i$和$y_j$之间的弹簧之间施加的力，正比于其长度，也正比于其刚度，这是数据点和图点之间成对的相似度的不匹配程度$(p_{j|i}-q_{j|i}+p_{i|j}-q_{i|j})$。

The gradient descent is initialized by sampling map points randomly from an isotropic Gaussian with small variance that is centered around the origin. In order to speed up the optimization and to avoid poor local minima, a relatively large momentum term is added to the gradient. In other words, the current gradient is added to an exponentially decaying sum of previous gradients in order to determine the changes in the coordinates of the map points at each iteration of the gradient search. Mathematically, the gradient update with a momentum term is given by

梯度下降的初始化，是从中心为原点，小方差的各向同性高斯函数中，随机采样图点。为加速优化，避免很差的局部最小值，给梯度加上了一个相对更大的动量项。换句话说，目前的梯度，加上了之前的梯度的指数衰减的和，以在梯度搜素的每个迭代中，确定图点坐标的变化。数学上来说，动量项的梯度更新，由下式给出

$$Y^{(t)} = Y^{(t−1)} + η\frac {δC}{δY} + α(t)(Y^{(t−1)} −Y^{(t−2)})$$

where $Y^{(t)}$ indicates the solution at iteration t, η indicates the learning rate, and α(t) represents the momentum at iteration t. 其中$Y^{(t)}$表示在迭代t时的解，η为学习速率，α(t)表示在迭代t时的动量。

In addition, in the early stages of the optimization, Gaussian noise is added to the map points after each iteration. Gradually reducing the variance of this noise performs a type of simulated annealing that helps the optimization to escape from poor local minima in the cost function. If the variance of the noise changes very slowly at the critical point at which the global structure of the map starts to form, SNE tends to find maps with a better global organization. Unfortunately, this requires sensible choices of the initial amount of Gaussian noise and the rate at which it decays. Moreover, these choices interact with the amount of momentum and the step size that are employed in the gradient descent. It is therefore common to run the optimization several times on a data set to find appropriate values for the parameters. In this respect, SNE is inferior to methods that allow convex optimization and it would be useful to find an optimization method that gives good results without requiring the extra computation time and parameter choices introduced by the simulated annealing.

此外，在优化的早期阶段，对图点在每个迭代后，都加入了高斯噪声。逐渐降低这个噪声的方差，是一种模拟退火，帮助优化过程从代价函数的局部极值中逃离出来。如果噪声的方差在关键点处变化很慢，在这些点处，图的全局结构开始形成，SNE倾向于找到具有更好的全局组织的图。不幸的是，这需要对高斯噪声的初始量和衰减率，进行很好的选择。而且，这些选择与动量的量和梯度下降中采用的步长相互作用。因此，在一个数据集上运行优化多次，以找到参数的合适值，这是很常见的。这样来说，SNE是比不上进行凸优化的方法的，如果一种优化方法能够不需要模拟退火的额外的计算时间和参数选择，就可以得到很好的结果，那会是非常有用的。

## 3. t-Distributed Stochastic Neighbor Embedding

Section 2 discussed SNE as it was presented by Hinton and Roweis (2002). Although SNE constructs reasonably good visualizations, it is hampered by a cost function that is difficult to optimize and by a problem we refer to as the "crowding problem". In this section, we present a new technique called "t-Distributed Stochastic Neighbor Embedding" or "t-SNE" that aims to alleviate these problems. The cost function used by t-SNE differs from the one used by SNE in two ways: (1) it uses a symmetrized version of the SNE cost function with simpler gradients that was briefly introduced by Cook et al. (2007) and (2) it uses a Student-t distribution rather than a Gaussian to compute the similarity between two points in the low-dimensional space. t-SNE employs a heavy-tailed distribution in the low-dimensional space to alleviate both the crowding problem and the optimization problems of SNE.

第2部分讨论了SNE。虽然SNE构建了还不错的可视化，但其代价函数很难进行优化，会产生一种我们称为“群聚的问题”。本节中，我们提出了一种新技术，称为t-SNE，可以缓解这个问题。t-SNE使用的代价函数与SNE所使用的有两点不同：(1)使用了一种对称化的SNE代价函数，梯度更加简单；(2)使用了student-t分布而不是高斯分布来计算低维空间中两个点之间的相似度。t-SNE采用了低维空间中的长拖尾分布，以缓解群聚问题和SNE的优化问题。

In this section, we first discuss the symmetric version of SNE (Section 3.1). Subsequently, we discuss the crowding problem (Section 3.2), and the use of heavy-tailed distributions to address this problem (Section 3.3). We conclude the section by describing our approach to the optimization of the t-SNE cost function (Section 3.4).

本节中，我们首先在3.1节讨论SNE的对称版本，然后在3.2节讨论群聚问题，3.3节讨论了长拖尾分布的使用以解决这个问题，3.4节中描述了我们优化t-SNE代价函数的方法，以进行总结。

### 3.1 Symmetric SNE

As an alternative to minimizing the sum of the Kullback-Leibler divergences between the conditional probabilities $p_{j|i}$ and $q_{j|i}$, it is also possible to minimize a single Kullback-Leibler divergence between a joint probability distribution, P, in the high-dimensional space and a joint probability distribution, Q, in the low-dimensional space:

前面我们对条件概率$p_{j|i}$和$q_{j|i}$之间的KL散度的和进行最小化，我们也可以最小化P和Q之间的KL散度以进行替代，其中P是高维空间中的联合概率分布，Q是低维空间中的联合概率分布

$$C = KL(P||Q) = \sum_i \sum_j p_{ij} log \frac {p_{ij}}{q_{ij}}$$

where again, we set $p_{ii}$ and $q_{ii}$ to zero. We refer to this type of SNE as symmetric SNE, because it has the property that $p_{ij} = p_{ji}$ and $q_{ij} = q_{ji}$ for ∀i, j. In symmetric SNE, the pairwise similarities in the low-dimensional map $q_{ij}$ are given by

我们再次设$p_{ii}$和$q_{ii}$为0。我们称这种SNE为对称SNE，因为有性质$p_{ij} = p_{ji}$和$q_{ij} = q_{ji}$，∀i, j。在对称SNE中，低维图中的成对相似性$q_{ij}$由下式给出

$$q_{ij} = \frac {exp(-||y_i - y_j||^2)} {\sum_{k \neq l} exp(-||y_k - y_l||^2}$$(3)

The obvious way to define the pairwise similarities in the high-dimensional space $p_{ij}$ is 在高维空间中的成对相似性$p_{ij}$的定义也是很明显的

$$p_{ij} = \frac {exp(-||x_i - x_j||^2/2σ^2)} {\sum_{k \neq l} exp(-||x_k - x_l||^2/2σ^2)}$$

but this causes problems when a high-dimensional datapoint $x_i$ is an outlier (i.e., all pairwise distances $||x_i − x_j||^2$ are large for $x_i$). For such an outlier, the values of $p_{ij}$ are extremely small for all j, so the location of its low-dimensional map point $y_i$ has very little effect on the cost function. As a result, the position of the map point is not well determined by the positions of the other map points. We circumvent this problem by defining the joint probabilities $p_{ij}$ in the high-dimensional space to be the symmetrized conditional probabilities, that is, we set $p_{ij} = (p_{j|i}+p_{i|j})/2n$. This ensures that $\sum_j p_{ij} > 1/2n$ for all datapoints $x_i$, as a result of which each datapoint $x_i$ makes a significant contribution to the cost function. In the low-dimensional space, symmetric SNE simply uses Equation 3. The main advantage of the symmetric version of SNE is the simpler form of its gradient, which is faster to compute. The gradient of symmetric SNE is fairly similar to that of asymmetric SNE, and is given by

但在一个高维数据点$x_i$是一个离群点时（即对$x_i$，所有的成对距离$||x_i − x_j||^2$都很大），这会造成问题。对于这样一个离群点，$p_{ij}$值对于所有j都很小，所以其对应的低维图点$y_i$的位置对代价函数的影响很小。结果是，图点的位置由其他图点的位置不太容易确定。我们通过定义高维空间中的联合概率$p_{ij}$为对称条件概率，来避免这个问题，即，我们设$p_{ij} = (p_{j|i}+p_{i|j})/2n$。这确保了对所有数据点$x_i$，$\sum_j p_{ij} > 1/2n$，其结果是，每个数据点$x_i$都对代价函数有显著的贡献。在低维空间中，对称SNE就使用式3。SNE的对称版的主要优势是，梯度形式更简单，计算起来更快。对称版SNE的梯度，与非对称版SNE的很类似，由下式给出

$$\frac {δC} {δy_i} = 4\sum_j (p_{ij}-p_{ji}) (y_i-y_j)$$

In preliminary experiments, we observed that symmetric SNE seems to produce maps that are just as good as asymmetric SNE, and sometimes even a little better. 在初步的试验中，我们观察到，对称SNE产生的图与非对称SNE一样好，有时候甚至更好一些。

### 3.2 The Crowding Problem

Consider a set of datapoints that lie on a two-dimensional curved manifold which is approximately linear on a small scale, and which is embedded within a higher-dimensional space. It is possible to model the small pairwise distances between datapoints fairly well in a two-dimensional map, which is often illustrated on toy examples such as the "Swiss roll" data set. Now suppose that the manifold has ten intrinsic dimensions and is embedded within a space of much higher dimensionality. There are several reasons why the pairwise distances in a two-dimensional map cannot faithfully model distances between points on the ten-dimensional manifold. For instance, in ten dimensions, it is possible to have 11 datapoints that are mutually equidistant and there is no way to model this faithfully in a two-dimensional map. A related problem is the very different distribution of pairwise distances in the two spaces. The volume of a sphere centered on datapoint i scales as r^m, where r is the radius and m the dimensionality of the sphere. So if the datapoints are approximately uniformly distributed in the region around i on the ten-dimensional manifold, and we try to model the distances from i to the other datapoints in the two-dimensional map, we get the following “crowding problem”: the area of the two-dimensional map that is available to accommodate moderately distant datapoints will not be nearly large enough compared with the area available to accommodate nearby datapoints. Hence, if we want to model the small distances accurately in the map, most of the points that are at a moderate distance from datapoint i will have to be placed much too far away in the two-dimensional map. In SNE, the spring connecting datapoint i to each of these too-distant map points will thus exert a very small attractive force. Although these attractive forces are very small, the very large number of such forces crushes together the points in the center of the map, which prevents gaps from forming between the natural clusters. Note that the crowding problem is not specific to SNE, but that it also occurs in other local techniques for multidimensional scaling such as Sammon mapping.

考虑二维弯曲流形中的一个数据点集，在很小的尺度上是近似线性的，嵌入到更高维度的空间中。在二维图中，数据点之间的较小成对距离，可以得到很好建模，这在toy例子中经常得到展示，比如Swiss卷数据集。现在假设，这个流形有10个内蕴维度，嵌入到维度高很多的空间中。二维图中的成对距离，不能对10维流形中的点的距离进行很忠实的建模，有几个原因。比如，在10维中，可以有11个点是等距的，在二维图中无法对此进行忠实建模。一个相关的问题是，在这两个空间中，成对距离分布是非常不同的。以数据点i为中心的球体的体积的缩放为r^m，其中r是半径，m是球体的维度。所以如果数据点在这个10维流行的区域中围绕i近似均匀分布，我们在二维空间中对i和其他数据点之间的距离进行建模，我们会得到下面的群聚问题：二维图中可用于容纳中等距离数据点的面积，与可用于容纳附近数据点的距离相比，是远远不够大的。因此，如果我们希望对图中的很小距离能很准确的建模，与数据点i距离中等的数据点的多数，在二维图中必须要放到远的多的地方。在SNE中，连接数据点i和每个很远的图点的弹簧，施加的吸引力是非常的小。虽然这些吸引力非常小，这样的力的数量很多，压碎了图中央的点，避免了自然聚类中的间隙的形成。注意，群聚问题并不是SNE特有的，在其他多维缩放的局部技术中也存在，比如Sammon映射。

An attempt to address the crowding problem by adding a slight repulsion to all springs was presented by Cook et al. (2007). The slight repulsion is created by introducing a uniform background model with a small mixing proportion, ρ. So however far apart two map points are, $q_{ij}$ can never fall below 2ρ/n(n−1) (because the uniform background distribution is over n(n−1)/2 pairs). As a result, for datapoints that are far apart in the high-dimensional space, $q_{ij}$ will always be larger than $p_{ij}$, leading to a slight repulsion. This technique is called UNI-SNE and although it usually outperforms standard SNE, the optimization of the UNI-SNE cost function is tedious. The best optimization method known is to start by setting the background mixing proportion to zero (i.e., by performing standard SNE). Once the SNE cost function has been optimized using simulated annealing, the background mixing proportion can be increased to allow some gaps to form between natural clusters as shown by Cook et al. (2007). Optimizing the UNI-SNE cost function directly does not work because two map points that are far apart will get almost all of their $q_{ij}$ from the uniform background. So even if their $p_{ij}$ is large, there will be no attractive force between them, because a small change in their separation will have a vanishingly small proportional effect on $q_{ij}$. This means that if two parts of a cluster get separated early on in the optimization, there is no force to pull them back together.

处理群聚问题的一种尝试，是给所有弹簧加上一个小的斥力。这个小的斥力，是由引入了一个均匀的背景模型和一个小的混合比例ρ创建的。所以无论两个图点多远，$q_{ij}$永远也不会低于2ρ/n(n−1)（因为均匀的背景分布超过了n(n−1)/2对）。结果是，对于在高维空间中非常远的数据点，$q_{ij}$永远要比$p_{ij}$要大，带来了一个很小的斥力。这个技术称为UNI-SNE，虽然其经常比标准SNE性能要好，UNI-SNE的代价函数的优化是冗繁的。已知的最好的优化方法是，首先设背景混合比例为0（即，进行标准SNE），一旦SNE代价函数使用模拟退火进行了优化，背景混合比例就可以增加，以在自然聚类之间形成一些间隙。UNI-SNE代价函数的直接优化，效果不好，因为两个很远的图点的$q_{ij}$，几乎全部都来自于均匀的背景。所以即使它们的$p_{ij}$很大，它们之间不会有任何吸引力，因为它们的分隔之间很小的变化，对$q_{ij}$的影响是几乎消失的。这意味着，如果一个聚类中的两个部分，在优化的早期分开了，就不会有将其拉回来到一起的力。

### 3.3 Mismatched Tails can Compensate for Mismatched Dimensionalities

Since symmetric SNE is actually matching the joint probabilities of pairs of datapoints in the high-dimensional and the low-dimensional spaces rather than their distances, we have a natural way of alleviating the crowding problem that works as follows. In the high-dimensional space, we convert distances into probabilities using a Gaussian distribution. In the low-dimensional map, we can use a probability distribution that has much heavier tails than a Gaussian to convert distances into probabilities. This allows a moderate distance in the high-dimensional space to be faithfully modeled by a much larger distance in the map and, as a result, it eliminates the unwanted attractive forces between map points that represent moderately dissimilar datapoints.

由于对称SNE实际上是匹配数据点对再高维空间和低维空间的联合概率，而不是其距离，我们有一种很自然的方法来缓解群聚问题。在高维空间中，我们使用高斯分布将距离转化成概率；在低维图中，我们可以使用一个比高斯分布有很长拖尾的概率分布，将距离转化成概率。这使得高维空间中的中等距离，可以在图中很忠实的用大的多的距离来进行建模，结果是，消除了图点之间表示中等不相似数据点的不需要的吸引力。

In t-SNE, we employ a Student t-distribution with one degree of freedom (which is the same as a Cauchy distribution) as the heavy-tailed distribution in the low-dimensional map. Using this distribution, the joint probabilities $q_{ij}$ are defined as

在t-SNE中，我们采用一个自由度的student t-分布（与Cauchy分布一样），作为低维图中的长尾分布。使用这个分布，联合概率$q_{ij}$定义为

$$q_{ij} = \frac {(1+||y_i-y_j||^2)^{-1}} {\sum_{k\neq l} (1+||y_k-y_l||^2)^{-1}}$$(4)

We use a Student t-distribution with a single degree of freedom, because it has the particularly nice property that $(1+||y_i − y_j||^2)^{−1}$ approaches an inverse square law for large pairwise distances $||y_i − y_j||$ in the low-dimensional map. This makes the map’s representation of joint probabilities (almost) invariant to changes in the scale of the map for map points that are far apart. It also means that large clusters of points that are far apart interact in just the same way as individual points, so the optimization operates in the same way at all but the finest scales. A theoretical justification for our selection of the Student t-distribution is that it is closely related to the Gaussian distribution, as the Student t-distribution is an infinite mixture of Gaussians. A computationally convenient property is that it is much faster to evaluate the density of a point under a Student t-distribution than under a Gaussian because it does not involve an exponential, even though the Student t-distribution is equivalent to an infinite mixture of Gaussians with different variances.

我们使用一个自由度的student t-分布，因为有特别好的性质，$(1+||y_i − y_j||^2)^{−1}$对于$||y_i − y_j||$在低维图中的成对距离很大时，接近平方逆定律。这使得联合概率的图的表示，对很远的图点来说，对图的尺度变化几乎不变。这也意味着很远的点的大聚类，其相互作用与单个点是一样方式的，所以优化过程在除了最精细尺度以外，在所有尺度上都是一样的。我们选择student t-分布的理论证明是，它很接近于高斯分布，因为student t-分布是高斯函数的无限混合。在计算上很便捷的一个性质是，在计算一个点的student t-分布密度，比高斯分布密度要快的多，因为不需要指数运算，虽然student t-分布等价于无限多不同方差的高斯分布的混合。

The gradient of the Kullback-Leibler divergence between P and the Student-t based joint probability distribution Q (computed using Equation 4) is derived in Appendix A, and is given by

P和基于student t-分布的联合概率分布Q（用式4计算的）之间的KL散度的梯度，在附录A中推导得到，如下式所示

$$\frac {δC} {δy_i} = 4\sum_j (p_{ij}-q_{ij}) (y_i - y_j) (1+||y_i-y_j||^2)^{-1}$$(5)

In Figure 1(a) to 1(c), we show the gradients between two low-dimensional datapoints $y_i$ and $y_j$ as a function of their pairwise Euclidean distances in the high-dimensional and the low-dimensional space (i.e., as a function of $||x_i − x_j||$ and $||y_i − y_j||$) for the symmetric versions of SNE, UNI-SNE, and t-SNE. In the figures, positive values of the gradient represent an attraction between the low-dimensional datapoints $y_i$ and $y_j$, whereas negative values represent a repulsion between the two datapoints. From the figures, we observe two main advantages of the t-SNE gradient over the gradients of SNE and UNI-SNE.

在图1a到图1c中，我们展示了两个低维数据点$y_i$和$y_j$之间的梯度，作为它们在高维和低维空间的成对欧式距离的函数（即，作为$||x_i − x_j||$和$||y_i − y_j||$的函数），在对称版SNE，UNI-SNE和t-SNE时的不同情况。在图中，梯度正值代表低维数据点$y_i$和$y_j$之间的吸引力，而负值代表两个数据点之间的斥力。从图中，我们观察到t-SNE梯度相对于SNE和UNI-SNE梯度的两个主要的优势。

First, the t-SNE gradient strongly repels dissimilar datapoints that are modeled by a small pairwise distance in the low-dimensional representation. SNE has such a repulsion as well, but its effect is minimal compared to the strong attractions elsewhere in the gradient (the largest attraction in our graphical representation of the gradient is approximately 19, whereas the largest repulsion is approximately 1). In UNI-SNE, the amount of repulsion between dissimilar datapoints is slightly larger, however, this repulsion is only strong when the pairwise distance between the points in the low-dimensional representation is already large (which is often not the case, since the low-dimensional representation is initialized by sampling from a Gaussian with a very small variance that is centered around the origin).

第一，t-SNE梯度非常排斥在低维表示中成对距离很小的不相似数据点。SNE也有这种斥力，但与在梯度其他地方的很强的吸引力相比较，其效果是极小的（在我们的梯度图形化表示中，最大的吸引力大约是19，而最大的斥力大约是1）。在UNI-SNE中，不相似数据点之间的斥力略高，但是，只在低维空间表示中数据点之间的距离已经很大的时，这种斥力才很大（还通常不是这种情况，因为低维表示是从以原点为中心很小方差的高斯分布中采样初始化的）。

Second, although t-SNE introduces strong repulsions between dissimilar datapoints that are modeled by small pairwise distances, these repulsions do not go to infinity. In this respect, t-SNE differs from UNI-SNE, in which the strength of the repulsion between very dissimilar datapoints is proportional to their pairwise distance in the low-dimensional map, which may cause dissimilar datapoints to move much too far away from each other.

第二，虽然t-SNE在成对距离很小的不相似点的斥力很强，但这些斥力不会无限变大。从这个意义来说，t-SNE与UNI-SNE不同，在UNI-SNE中，非常不相似的数据点之间的斥力，与其低维空间中的成对距离是成比例的，这会使非常不相似的点之间的距离非常大。

Taken together, t-SNE puts emphasis on (1) modeling dissimilar datapoints by means of large pairwise distances, and (2) modeling similar datapoints by means of small pairwise distances. Moreover, as a result of these characteristics of the t-SNE cost function (and as a result of the approximate scale invariance of the Student t-distribution), the optimization of the t-SNE cost function is much easier than the optimization of the cost functions of SNE and UNI-SNE. Specifically, t-SNE introduces long-range forces in the low-dimensional map that can pull back together two (clusters of) similar points that get separated early on in the optimization. SNE and UNI-SNE do not have such long-range forces, as a result of which SNE and UNI-SNE need to use simulated annealing to obtain reasonable solutions. Instead, the long-range forces in t-SNE facilitate the identification of good local optima without resorting to simulated annealing.

综合起来，t-SNE强调了，(1)用大的成对距离来建模不相似的数据点，(2)用小的成对距离来建模相似的数据点。而且，t-SNE代价函数的这些特征的结果是（以及student t-分布的近似尺度不变性的结果），t-SNE代价函数的优化，比SNE和UNI-SNE代价函数的优化要简单的多。具体的，t-SNE在低维图中引入了长程力，可以将两个相似点（的聚类）拉回到一起，这些点在早期的优化中被分开了。SNE和UNI-SNE并没有这种长程力，这样结果是，SNE和UNI-SNE需要使用模拟退火来得到合理的解。而t-SNE中的长程力在不需要模拟退火的情况下就可以找到很好的局部最优值。

### Optimization Methods for t-SNE

We start by presenting a relatively simple, gradient descent procedure for optimizing the t-SNE cost function. This simple procedure uses a momentum term to reduce the number of iterations required and it works best if the momentum term is small until the map points have become moderately well organized. Pseudocode for this simple algorithm is presented in Algorithm 1. The simple algorithm can be speed up using the adaptive learning rate scheme that is described by Jacobs (1988), which gradually increases the learning rate in directions in which the gradient is stable.

我们给出一种相对简单的，梯度下降方法来优化t-SNE代价函数。这个简单的过程使用动量项来降低需要的迭代数量，如果动量项很小，效果最好，直到图点已经相对组织较好。这个简单算法的伪代码如Algorithm 1所示。这个简单算法可以用自适应学习速率方案进行加速，在梯度稳定的方向逐渐增加学习速率。

Although the simple algorithm produces visualizations that are often much better than those produced by other non-parametric dimensionality reduction techniques, the results can be improved further by using either of two tricks. The first trick, which we call "early compression", is to force the map points to stay close together at the start of the optimization. When the distances between map points are small, it is easy for clusters to move through one another so it is much easier to explore the space of possible global organizations of the data. Early compression is implemented by adding an additional L2-penalty to the cost function that is proportional to the sum of squared distances of the map points from the origin. The magnitude of this penalty term and the iteration at which it is removed are set by hand, but the behavior is fairly robust across variations in these two additional optimization parameters.

虽然这个简答的算法产生的可视化效果，通常比其他非参数降维方法产生的要好很多，但结果仍然可以用两个技巧进一步改进。第一个技巧，我们称之为“早压缩”，是在优化开始时，迫使图点保持距离很近。当图点之间距离很小时，聚类之间互相穿过是很容易的，所以探索数据的全局组织的可能性就很简单。早压缩的实现，是给代价函数加入了额外的L2-惩罚，与图点到原点的距离平方和成正比。这个惩罚项的幅度，以及在哪个迭代中移除，是通过手动设定的，但算法行为对于这两个额外的优化参数的变化，还是很稳健的。

A less obvious way to improve the optimization, which we call "early exaggeration", is to multiply all of the $p_{ij}$’s by, for example, 4, in the initial stages of the optimization. This means that almost all of the $q_{ij}$’s, which still add up to 1, are much too small to model their corresponding $p_{ij}$’s. As a result, the optimization is encouraged to focus on modeling the large $p_{ij}$’s by fairly large $q_{ij}$’s. The effect is that the natural clusters in the data tend to form tight widely separated clusters in the map. This creates a lot of relatively empty space in the map, which makes it much easier for the clusters to move around relative to one another in order to find a good global organization.

改进优化的一个不那么明显的方法，我们称之为“早夸大”，是在优化的初始阶段，将所有$p_{ij}$乘以一个数，比如4。这意味着所有的$q_{ij}$，其和为1，要建模其对应的$p_{ij}$都太小了。结果是，优化过程鼓励通过相对较大的$q_{ij}$来聚焦大$p_{ij}$的建模。其效果是，数据中的自然聚类，倾向于形成图中紧凑的分布广泛的聚类。这在图中形成了大量相对很空的空间，这使得聚类可以相互之间很容易的移动，找到一个很好的全局组织关系。

In all the visualizations presented in this paper and in the supporting material, we used exactly the same optimization procedure. We used the early exaggeration method with an exaggeration of 4 for the first 50 iterations (note that early exaggeration is not included in the pseudocode in Algorithm 1). The number of gradient descent iterations T was set 1000, and the momentum term was set to α^(t) = 0.5 for t < 250 and α^(t) = 0.8 for t ≥ 250. The learning rate η is initially set to 100 and it is updated after every iteration by means of the adaptive learning rate scheme described by Jacobs (1988). A Matlab implementation of the resulting algorithm is available at http://ticc.uvt.nl/˜lvdrmaaten/tsne.

在本文和支撑材料中给出的所有可视化中，我们使用了相同的优化过程。我们对前50次迭代使用早夸大方法，夸大系数为4（Algorithm 1中并没有包含早夸大）。梯度下降迭代次数T设为1000，动量项在t<250时设为α^(t)=0.5，t ≥ 250时设为α^(t) = 0.8。学习速率η开始设为100，通过自适应学习速率方案在每个迭代中进行调整。代码已开源。

## 4. Experiments

To evaluate t-SNE, we present experiments in which t-SNE is compared to seven other non-parametric techniques for dimensionality reduction. Because of space limitations, in the paper, we only compare t-SNE with: (1) Sammon mapping, (2) Isomap, and (3) LLE. In the supporting material, we also compare t-SNE with: (4) CCA, (5) SNE, (6) MVU, and (7) Laplacian Eigenmaps. We performed experiments on five data sets that represent a variety of application domains. Again because of space limitations, we restrict ourselves to three data sets in the paper. The results of our experiments on the remaining two data sets are presented in the supplemental material.

为评估t-SNE，我们进行了试验，将t-SNE与其他7种非参数降维技术进行了比较。由于空间限制，本文中，我们只将t-SNE与(1) Sammon mapping, (2) Isomap, and (3) LLE进行了比较。在支持材料中，我们还将t-SNE与(4) CCA, (5) SNE, (6) MVU, and (7) Laplacian Eigenmaps进行了比较。我们在5个数据集上进行了试验，代表了很多应用领域。还是由于空间限制，我们在本文中只给出三个数据集的比较结果。我们在剩下两个数据集上的试验结果在附属材料中。

In Section 4.1, the data sets that we employed in our experiments are introduced. The setup of the experiments is presented in Section 4.2. In Section 4.3, we present the results of our experiments.

在4.1节中，介绍了我们试验中采用的数据集。试验的设置在4.2节给出。在4.3节中，我们给出试验结果。

### 4.1 Data Sets

The five data sets we employed in our experiments are: (1) the MNIST data set, (2) the Olivetti faces data set, (3) the COIL-20 data set, (4) the word-features data set, and (5) the Netflix data set. We only present results on the first three data sets in this section. The results on the remaining two data sets are presented in the supporting material. The first three data sets are introduced below.

我们在试验中采用的5个数据集是：(1) MNIST, (2) Olivetti人脸，(3) COIL-20, (4) 字特征数据集，(5) Netflix数据集。我们在本节中只给出前3个数据集的结果。剩下的2个数据集的结果在附属材料中给出。前3个数据集介绍如下。

The MNIST data set contains 60,000 grayscale images of handwritten digits. For our experiments, we randomly selected 6,000 of the images for computational reasons. The digit images have 28×28 = 784 pixels (i.e., dimensions). The Olivetti faces data set consists of images of 40 individuals with small variations in viewpoint, large variations in expression, and occasional addition of glasses. The data set consists of 400 images (10 per individual) of size 92×112 = 10,304 pixels, and is labeled according to identity. The COIL-20 data set (Nene et al., 1996) contains images of 20 different objects viewed from 72 equally spaced orientations, yielding a total of 1,440 images. The images contain 32×32 = 1,024 pixels.

MNIST数据集包含60000幅手写数字灰度图像。在我们的试验中，我们随机选择6000幅，以减少计算量。数字图像大小为28×28 = 784像素（即，维度）。Olivetti人脸数据集包含40个人的图像，其视角变化很小，表情变化很大，有时候有眼睛。数据集包含400幅图像（每个人10幅），大小为92×112 = 10,304像素，根据身份进行标注。COIL-20数据集包含20个不同目标的图像，每个从从72个等间距的方向进行观察，总计有1440幅图像。图像大小为32×32 = 1,024像素。

### 4.2 Experimental Setup

In all of our experiments, we start by using PCA to reduce the dimensionality of the data to 30. This speeds up the computation of pairwise distances between the datapoints and suppresses some noise without severely distorting the interpoint distances. We then use each of the dimensionality reduction techniques to convert the 30-dimensional representation to a two-dimensional map and we show the resulting map as a scatterplot. For all of the data sets, there is information about the class of each datapoint, but the class information is only used to select a color and/or symbol for the map points. The class information is not used to determine the spatial coordinates of the map points. The coloring thus provides a way of evaluating how well the map preserves the similarities within each class.

在所有试验中，我们开始使用PCA将数据降维到30。这加速了数据点成对距离的计算，抑制了一些噪声，而且没有严重扭曲点之间的距离。我们然后使用每种降维技术，将30维表示转化到二维图中，将得到的图展示为散点图。对所有的数据集，有每个数据点的类别信息，但类别信息只用于给图点选择一个颜色或/和符号。类别信息没有用于确定图点的空间坐标。这些颜色就给出了评估图保持每个类别中的相似性的一种方法。

The cost function parameter settings we employed in our experiments are listed in Table 1. In the table, Perp represents the perplexity of the conditional probability distribution induced by a Gaussian kernel and k represents the number of nearest neighbors employed in a neighborhood graph. In the experiments with Isomap and LLE, we only visualize datapoints that correspond to vertices in the largest connected component of the neighborhood graph. For the Sammon mapping optimization, we performed Newton’s method for 500 iterations.

我们在试验中采用的代价函数参数设置，如表1所示。在表中，Perp表示高斯核的条件概率分布的复杂度，k表示在邻域图中采用的最近邻的数量。在Isomap和LLE的试验中，我们只对对应着邻域图的最大连接部分的顶点进行可视化。对于Sammon mapping优化，我们进行了500次牛顿法迭代。

### 4.3 Results

In Figures 2 and 3, we show the results of our experiments with t-SNE, Sammon mapping, Isomap, and LLE on the MNIST data set. The results reveal the strong performance of t-SNE compared to the other techniques. In particular, Sammon mapping constructs a "ball" in which only three classes (representing the digits 0, 1, and 7) are somewhat separated from the other classes. Isomap and LLE produce solutions in which there are large overlaps between the digit classes. In contrast, t-SNE constructs a map in which the separation between the digit classes is almost perfect. Moreover, detailed inspection of the t-SNE map reveals that much of the local structure of the data (such as the orientation of the ones) is captured as well. This is illustrated in more detail in Section 5 (see Figure 7). The map produced by t-SNE contains some points that are clustered with the wrong class, but most of these points correspond to distorted digits many of which are difficult to identify. Figure 4 shows the results of applying t-SNE, Sammon mapping, Isomap, and LLE to the Olivetti faces data set. Again, Isomap and LLE produce solutions that provide little insight into the class structure of the data. The map constructed by Sammon mapping is significantly better, since it models many of the members of each class fairly close together, but none of the classes are clearly separated in the Sammon map. In contrast, t-SNE does a much better job of revealing the natural classes in the data. Some individuals have their ten images split into two clusters, usually because a subset of the images have the head facing in a significantly different direction, or because they have a very different expression or glasses. For these individuals, it is not clear that their ten images form a natural class when using Euclidean distance in pixel space.

在图2和图3中，我们展示了用t-SNE, Sammon mapping, Isomap, 和LLE在MNIST数据集上的结果。结果表明，t-SNE比其他方法效果要好很多。特别是，Sammon mapping构建了一个球，其中只有三类（表示数字0，1，7）与其他类别大概分开了。Isomap和LLE给出的解，在不同数字类别之间有很大的重叠。比较起来，t-SNE构建的图中，数字类别之间的区分几乎是完美的。而且，仔细检查t-SNE图还可以发现，大部分数据的局部结构（比如方向）也得到了捕获。这在第5部分有更加详细的描述（见图7）。t-SNE产生的图有一些点聚类到了错误的类中，但这些点中的大多数，对应着变形的数字，很多很难辨认。图4给出了将t-SNE, Sammon mapping, Isomap, 和LLE应用到Olivetti人脸数据集的结果。Isomap和LLE产生的解对数据的类别结构没有任何洞见。Sammon mapping构建的图明显要好，因为其建模的每个类别的很多成员距离都比较近，但在Sammon图中，这些类都没有明显分开。比较起来，t-SNE在揭示数据中的自然类别中有更好的结果。有些类别将其10幅图像分成了两个聚类，通常是因为一些图像的人脸方向明显不同，或因为有非常不同的表情或眼镜。对这些类别，其10幅图像使用欧式距离在像素空间中是否形成了一个自然类别，还不是很明确。

Figure 5 shows the results of applying t-SNE, Sammon mapping, Isomap, and LLE to the COIL-20 data set. For many of the 20 objects, t-SNE accurately represents the one-dimensional manifold of viewpoints as a closed loop. For objects which look similar from the front and the back, t-SNE distorts the loop so that the images of front and back are mapped to nearby points. For the four types of toy car in the COIL-20 data set (the four aligned "sausages" in the bottom-left of the t-SNE map), the four rotation manifolds are aligned by the orientation of the cars to capture the high similarity between different cars at the same orientation. This prevents t-SNE from keeping the four manifolds clearly separate. Figure 5 also reveals that the other three techniques are not nearly as good at cleanly separating the manifolds that correspond to very different objects. In addition, Isomap and LLE only visualize a small number of classes from the COIL-20 data set, because the data set comprises a large number of widely separated submanifolds that give rise to small connected components in the neighborhood graph.

图5给出了将t-SNE, Sammon mapping, Isomap, 和 LLE应用到COIL-20数据集的结果。对20个目标中的很多个，t-SNE准确的将视角的一维流形表示为一个闭合回路。对于从前部和后面看起来很类似的目标，t-SNE对这个回路进行了变形，这样前面的图像和后面的图像映射到了接近的点。对于COIL-20数据集中四种类型的玩具车，这四个旋转流形是按照车的方向对齐的，捕获了在相同方向的不同车的高度相似性。这使t-SNE没有将四个流形很明显的区分开。图5还揭示了，另外三种技术在区分对应着非常不同的目标的流形上也非常差。另外，Isomap和LLE只对COIL-20数据集中的少数类别进行了可视化，因为数据集由大量分布很广的子流形组成，带来了邻域图中很小的连接部分。

## 5. Applying t-SNE to Large Data Sets

Like many other visualization techniques, t-SNE has a computational and memory complexity that is quadratic in the number of datapoints. This makes it infeasible to apply the standard version of t-SNE to data sets that contain many more than, say, 10,000 points. Obviously, it is possible to pick a random subset of the datapoints and display them using t-SNE, but such an approach fails to make use of the information that the undisplayed datapoints provide about the underlying manifolds. Suppose, for example, that A, B, and C are all equidistant in the high-dimensional space. If there are many undisplayed datapoints between A and B and none between A and C, it is much more likely that A and B are part of the same cluster than A and C. This is illustrated in Figure 6. In this section, we show how t-SNE can be modified to display a random subset of the datapoints (so-called landmark points) in a way that uses information from the entire (possibly very large) data set.

与很多其他可视化技术意义，t-SNE的计算复杂度和存储复杂度，都是数据点数量的平方级的。这使标准版t-SNE很难应用到很大型的数据集上，比如超过1000个数据点。显然，可以选择一个随机子集，使用t-SNE进行展示，但这个方法就不能利用没有展示的数据点的潜在流形信息。比如，假设A、B和C在高维空间中是等距的。如果在A和B直接按有很多未展示的数据点，而A和C之间都展示了，那么A和B就很可能是数据同一聚类的，A和C就可能不是。这在图6中进行了展示。本节中，我们展示了t-SNE可以进行修改，展示数据点的随机子集（所谓的特征点），但使用了整个数据集的信息。

We start by choosing a desired number of neighbors and creating a neighborhood graph for all of the datapoints. Although this is computationally intensive, it is only done once. Then, for each of the landmark points, we define a random walk starting at that landmark point and terminating as soon as it lands on another landmark point. During a random walk, the probability of choosing an edge emanating from node $x_i$ to node $x_j$ is proportional to $e^{−||x_i−x_j||^2}$. We define $p_{j|i}$ to be the fraction of random walks starting at landmark point $x_i$ that terminate at landmark point $x_j$. This has some resemblance to the way Isomap measures pairwise distances between points. However, as in diffusion maps (Lafon and Lee, 2006; Nadler et al., 2006), rather than looking for the shortest path through the neighborhood graph, the random walk-based affinity measure integrates over all paths through the neighborhood graph. As a result, the random walk-based affinity measure is much less sensitive to “short-circuits” (Lee and Verleysen, 2005), in which a single noisy datapoint provides a bridge between two regions of dataspace that should be far apart in the map. Similar approaches using random walks have also been successfully applied to, for example, semi-supervised learning (Szummer and Jaakkola, 2001; Zhu et al., 2003) and image segmentation (Grady, 2006).

The most obvious way to compute the random walk-based similarities $p_{j|i}$ is to explicitly perform the random walks on the neighborhood graph, which works very well in practice, given that one can easily perform one million random walks per second. Alternatively, Grady (2006) presents an analytical solution to compute the pairwise similarities $p_{j|i}$ that involves solving a sparse linear system. The analytical solution to compute the similarities $p_{j|i}$ is sketched in Appendix B. In preliminary experiments, we did not find significant differences between performing the random walks explicitly and the analytical solution. In the experiment we present below, we explicitly performed the random walks because this is computationally less expensive. However, for very large data sets in which the landmark points are very sparse, the analytical solution may be more appropriate.

Figure 7 shows the results of an experiment, in which we applied the random walk version of t-SNE to 6,000 randomly selected digits from the MNIST data set, using all 60,000 digits to compute the pairwise affinities $p_{j|i}$. In the experiment, we used a neighborhood graph that was constructed using a value of k = 20 nearest neighbors.9 The inset of the figure shows the same visualization as a scatterplot in which the colors represent the labels of the digits. In the t-SNE map, all classes are clearly separated and the “continental” sevens form a small separate cluster. Moreover, t-SNE reveals the main dimensions of variation within each class, such as the orientation of the ones, fours, sevens, and nines, or the “loopiness” of the twos. The strong performance of t-SNE is also reflected in the generalization error of nearest neighbor classifiers that are trained on the low-dimensional representation. Whereas the generalization error (measured using 10-fold cross validation) of a 1-nearest neighbor classifier trained on the original 784-dimensional datapoints is 5.75%, the generalization error of a 1-nearest neighbor classifier trained on the two-dimensional data representation produced by t-SNE is only 5.13%. The computational requirements of random walk t-SNE are reasonable: it took only one hour of CPU time to construct the map in Figure 7.

## 6. Discussion

The results in the previous two sections (and those in the supplemental material) demonstrate the performance of t-SNE on a wide variety of data sets. In this section, we discuss the differences between t-SNE and other non-parametric techniques (Section 6.1), and we also discuss a number of weaknesses and possible improvements of t-SNE (Section 6.2).

### 6.1 Comparison with Related Techniques

### 6.2 Weaknesses

## 7. Conclusions

The paper presents a new technique for the visualization of similarity data that is capable of retaining the local structure of the data while also revealing some important global structure (such as clusters at multiple scales). Both the computational and the memory complexity of t-SNE are O(n2), but we present a landmark approach that makes it possible to successfully visualize large real-world data sets with limited computational demands. Our experiments on a variety of data sets show that t-SNE outperforms existing state-of-the-art techniques for visualizing a variety of real-world data sets. Matlab implementations of both the normal and the random walk version of t-SNE are available for download at http://ticc.uvt.nl/˜lvdrmaaten/tsne.

本文提出了一种相似数据可视化的新技术，可以保持数据的局部结构，同时揭示一些重要的全局结构（比如在多个尺度上的聚类）。t-SNE的计算复杂度和存储复杂度都是O(n2)，但我们给出了一种特征点方法，使其可能成功的对大型真实世界数据集进行可视化，所需计算量也不大。我们在很多数据集上的试验表明，t-SNE超过了目前最好的技术，可以很好的可视化很多真实世界的数据集。代码已开源。

In future work we plan to investigate the optimization of the number of degrees of freedom of the Student-t distribution used in t-SNE. This may be helpful for dimensionality reduction when the low-dimensional representation has many dimensions. We will also investigate the extension of t-SNE to models in which each high-dimensional datapoint is modeled by several low-dimensional map points as in Cook et al. (2007). Also, we aim to develop a parametric version of t-SNE that allows for generalization to held-out test data by using the t-SNE objective function to train a multilayer neural network that provides an explicit mapping to the low-dimensional space.

未来，我们计划对t-SNE中，student t-分布的自由度数量进行优化。这在低维表示有很多维时，对降维会有帮助。我们还会研究t-SNE的拓展，如高维数据用几个低维图点进行建模。而且，我们还计划开发t-SNE的参数化版本。