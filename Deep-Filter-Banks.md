# Deep Filter Banks for Texture Recognition and Segmentation

Mircea Cimpoi, Subhransu Maji, Andrea Vedaldi Oxford & MIT

## 0. Abstract

Research in texture recognition often concentrates on the problem of material recognition in uncluttered conditions, an assumption rarely met by applications. In this work we conduct a first study of material and describable texture attributes recognition in clutter, using a new dataset derived from the OpenSurface texture repository. Motivated by the challenge posed by this problem, we propose a new texture descriptor, FV-CNN, obtained by Fisher Vector pooling of a Convolutional Neural Network (CNN) filter bank. FV-CNN substantially improves the state-of-the-art in texture, material and scene recognition. Our approach achieves 79.8% accuracy on Flickr material dataset and 81% accuracy on MIT indoor scenes, providing absolute gains of more than 10% over existing approaches. FV-CNN easily transfers across domains without requiring feature adaptation as for methods that build on the fully-connected layers of CNNs. Furthermore, FV-CNN can seamlessly incorporate multi-scale information and describe regions of arbitrary shapes and sizes. Our approach is particularly suited at localizing “stuff” categories and obtains state-of-the-art results on MSRC segmentation dataset, as well as promising results on recognizing materials and surface attributes in clutter on the OpenSurfaces dataset.

纹理识别的研究通常聚焦在无杂乱的环境中的材质识别问题，这种假设在实际应用中通常难以遇到。本文中，我们进行杂乱环境中的材质和可描述的纹理属性的识别的工作，使用的是是从OpenSurface纹理库中推演得到的新数据集。受到这个问题给出的挑战所推动，我们提出了一种新的纹理描述子，FV-CNN，是通过CNN滤波器组的Fisher向量池化得到的。FV-CNN显著改进了目前最好的纹理、材质和场景识别性能。我们的方法在Flickr材质数据集上得到了79.8%的准确率，在MIT室内场景中得到了81%的准确率，比现有的方法改进了超过10%的绝对值。FV-CNN在不同问题中可以很容易的迁移，不需要像CNNs的全连接层那样进行特征适应。而且，FV-CNN可以无缝纳入多尺度信息，描述任意形状和大小的区域。我们的方法特别适用于定位stuff类别，在MSRC分割数据集上得到了目前最好效果，在OpenSurface数据集的杂乱环境中识别材质和表面属性，也得到了非常不错的效果。

## 1. Introduction

Texture is ubiquitous and provides useful cues of material properties of objects and their identity, especially when shape is not useful. Hence, a significant amount of effort in the computer vision community has gone into recognizing texture via tasks such as texture perception [1, 2, 12, 13] and description [7, 11], material recognition [26, 36, 37], segmentation [20, 29], and even synthesis [9, 43].

纹理无处不在，为目标的材质属性和识别提供了有用的线索，尤其是在形状信息不可用的时候。因此，计算机视觉团体投入了相当的经历来研究纹理识别，其任务比如纹理感知，纹理描述，材质识别，材质分割，甚至是合成。

Perhaps the most studied task in texture understanding is the one of material recognition, as captured in benchmarks such as CuRET [8], KTH-TIPS [5], and, more recently, FMD [38]. However, while at least the FMD dataset contains images collected from the Internet, vividly dubbed “images in the wild”, all these datasets make the simplifying assumption that textures fill images. Thus, they are not necessarily representative of the significantly harder problem of recognising materials in natural images, where textures appear in clutter. Building on a recent dataset collected by the computer graphics community, the first contribution of this paper is a large-scale analysis of material and perceptual texture attribute recognition and segmentation in clutter (Fig. 1 and Sect. 2).

在纹理理解中研究最多的任务可能是材质识别，比如CuRET, KTH-TIPS, 以及最近的FMD数据集基准测试。但是，FMD数据集的图像是从互联网上收集的，可以称为“自然环境中的图像”，其他数据集都是将纹理充满了整幅图像。因此，它们都不能代表自然图像中识别材质的更难问题，这其中纹理是在很杂乱的环境中的。我们在一个最近的更新的数据集中构建出了这样的数据集，本文的第一个贡献就是，在杂乱环境中对材质和纹理属性感知进行了大规模分析。

Motivated by the challenge posed by recognising texture in clutter, we develop a new texture descriptor. In the simplest terms a texture is characterized by the arrangement of local patterns, as captured in early works [26, 41] by the distribution of local “filter bank” responses. These filter banks were designed to capture edges, spots and bars at different scales and orientations. Typical combinations of the filter responses, identified by vector quantisation, were used as the computational basis of the “textons” proposed by Julesz [22]. Texton distributions were the early versions of “bag-of-words” representations, a dominant approach in recognition in the early 2000s, since then improved by new pooling schemes such as soft-assignment [27, 42, 48] and Fisher Vectors (FVs) [32]. Until recently, FV with SIFT features [28] as a local representation was the state-of-the-art method for recognition, not only for textures, but for objects and scenes too.

受到在杂乱环境中识别纹理的挑战所推动，我们提出了一个新的纹理描述子。在最简单的表述中，纹理的特征是局部模式的排列，早期的滤波器组响应分布工作都反应了这一点。这些滤波器组设计用于捕获不同尺度和方向的边缘，斑点和条状物。滤波器响应的典型组合，通过矢量量化识别，用作Julesz提出的texton的计算基础。Texton分布是bag-of-words表示的早期标准，在2000s早期的主要识别方法，后来被新的池化方法改进，如soft-assignment和Fisher Vectors (FVs)。直到最近，用SIFT特征得到的FV作为局部表示，是识别的目前最好方法，不仅是对纹理，也是对目标识别和场景识别。

Later, however, Convolutional Neural Networks (CNNs) have emerged as the new state-of-the-art for recognition, exemplified by remarkable results in image classification [23], detection [14] and segmentation [16] on a number of widely used benchmarks. Key to their success is the ability to leverage large labelled datasets to learn increasingly complex transformations of the input to capture invariances. Importantly, CNNs pre-trained on such large datasets have been shown [6, 14, 30] to contain general-purpose feature extractors, transferrable to many other domains.

但是后来，CNNs成为了新的目前最好的识别方法，如在图像分类，检测和分割上在很多广泛使用的基准测试中得到了很不错的结果。其成功的关键是利用大规模标注数据集来学习输入的复杂变换，以捕获不变性的能力。重要的是，在这些大型数据集上预训练得到的CNNs，已经证明包含了通用目的的特征提取器，可以迁移到很多其他领域。

Domain transfer in CNNs is usually achieved by using as features the output of a deep, fully-connected layer of the network. From the perspective of textures, however, this choice has three drawbacks. The first one (I) is that, while the convolutional layers are akin to non-linear filter banks, the fully connected layers capture their spatial layout. While this may be useful for representing the shape of an object, it may not be as useful for representing texture. A second drawback (II) is that the input to the CNN has to be of fixed size to be compatible with the fully connected layers, which requires an expensive resizing of the input image, particularly when features are computed for many different regions [14, 15]. A third and more subtle drawback (III) is that deeper layers may be more domain-specific and therefore potentially less transferrable than shallower layers.

CNNs中的领域迁移，通常通过使用一个深度全连接层网络的输出作为特征来实现。但从纹理的角度，这个选择有三个缺陷。第一个缺陷是，卷积层与非线性滤波器组类似，全连接层捕获的是其空间分布。这对于表示目标的形状可能有用，但表示纹理可能不一定有用。第二个缺陷是，CNN的输入大小是固定的，要与全连接层相兼容，这需要对输入图像进行耗费很大的改变大小运算，尤其是特征还是从很多不同的区域中计算得到的。第三个更加微妙的缺陷是，更深的层是更加对应领域的，因此比更浅的层没有那么强的迁移能力。

The second contribution of this paper is FV-CNN (Sect. 3), a pooling method that overcomes these drawbacks. The idea is to regard the convolutional layers of a CNN as a filter bank and build an orderless representation using FV as a pooling mechanism, as is commonly done in the bag-of-words approaches. Although the suggested change is simple, the approach is remarkably flexible and effective. First, pooling is orderless and multi-scale, hence suitable for textures. Second, any image size can be processed by convolutional layers, avoiding costly resizing operations. Third, convolutional filters, pooled by FV-CNN, are shown to transfer more easily than fully-connected ones even without fine-tuning. While other authors [15, 18] have recently proposed alternative pooling strategies for CNNs, we show that our method is more natural, faster and often significantly more accurate.

本文的第二个贡献是FV-CNN，一种克服这些缺陷的池化方法。其思想是将CNN的卷积层视为一个滤波器组，使用FV作为一种池化机制来构建无序的表示，在bag-of-words方法中就是这样进行的。虽然建议的改变很简单，但这种方法非常灵活和有效。首先，池化是无序和多尺度的，因此适用于纹理。第二，卷积层可以处理任意大小的图像，避免了耗费很大的改变大小运算。第三，卷积滤波器，由FV-CNN进行池化，证明了比全连接层可以更容易的进行迁移，甚至不需要精调。其他作者最近提出了其他池化策略用于CNNs，我们证明了我们的方法更加自然，更加快速，而且会明显更加准确。

The third contribution of the paper is a thorough evaluation of these descriptors on a variety of benchmarks, from textures to objects (Sect. 4). In textures, we evaluate material and describable attributes recognition and segmentation on new datasets derived from OpenSurfaces (Sect. 2). When used with linear SVMs, FV-CNN improves the state of the art on texture recognition by a significant margin. Like textures, scenes are also weakly structured and a bag-of-words representation is effective. FV-CNN obtains 81.1% accuracy on the MIT indoor scenes dataset [34], significantly outperforming the current state-of-the-art of 70.8% [47]. What is remarkable is that, where [47] finds that CNNs trained on scene recognition data perform better than CNNs trained on an object domain (ImageNet), when used in FV-CNN not only is there an overall performance improvement, but the domain-specific advantage is entirely removed (Tab. 3). This indicates that FV-CNN are in fact better at domain transfer. Our method also matches the previous best in PASCAL VOC 2007 classification dataset providing measurable boost over CNNs and closely approaches competitor methods on CUB 2010-2011 datasets when ground-truth object bounding boxes are given.

本文的第三个贡献，是在很多基准测试中彻底评估了这些描述子，包括纹理和目标。在纹理中，我们在OpenSurfaces导出的新数据集中，评估了材质和可描述的属性识别和分割。当使用线性SVMs时，FV-CNN与目前最好的纹理识别水平相比，改进了非常多。与纹理类似，场景的结构也很弱，bag-of-words表示也是很有效的。FV-CNN在MIT室内场景数据集中得到了81.1%的准确率，显著超过了目前最好的70.8%的水平。令人更加印象深刻的是，[47]发现，在场景识别数据中训练的CNNs，比在目标领域(ImageNet)训练的CNNs性能要好，当使用FV-CNN时，不仅有总体性能改进，而且领域相关的优势也完全没有了。这说明，FV-CNN实际上更擅长领域迁移。我们的方法也达到了PASCAL VOC 2007分类数据集上之前最好的结果，比CNNs有可测量的改进，在CUB 2010-2011数据集上，当给定了真值目标边界框时，也很接近竞争者方法的结果。

FV-CNN can be used for describing regions by simply pooling across pixels within the region. Combined with a low-level segmentation algorithm this suffices to localize textures within images. This approach is similar to a recently proposed method called “R-CNN” for localizing objects [14]. However, in contrast to it we do not need repeated evaluations of the CNN since the convolutional features can be computed just once for the entire image and pooled differently. This makes FV-CNN not only faster, but also as experiments suggest, much more accurate at texture localization. We achieve state of the art results on the MSRC segmentation dataset using a simple scheme of classifying “superpixels” obtaining an accuracy of 87.0% (previous best 86.5%). The corresponding R-CNN obtains 57.7% accuracy. Segmentation results are promising in the OpenSurfaces dataset [4] as well.

FV-CNN只要简单的对区域内的像素进行池化，就可以用于描述区域。与一个底层分割算法进行结合，足以在图像中对纹理进行定位。这种方法与最近提出的R-CNN对目标进行定位的方法很类似。但是，与之相比，我们不需要重复评估CNN，因为卷积特征可以对整幅图像计算一次，然后进行不同的池化。这使得FV-CNN不仅快速，而且根据实现表明，对纹理定位也更加准确。我们在MSRC分割数据集上，使用一种简单的方法分类超像素，得到了87.0%的准确率，是目前的最好结果（之前最好的是86.5%）。对应的R-CNN得到了57.7%的准确率。分割结果在OpenSurfaces数据集上也是非常有希望的。

Finally, we analyze the utility of different network layers and architectures as filter banks, concluding that: SIFT is competitive only with the first few layers of a CNN (Fig. 4) and that significant improvement to the underlying CNN architecture, such as the ones achieved by the very deep models of Simonyan and Zisserman [39], directly translate into much better filter banks for texture recognition.

最后，我们分析了不同的网络层和架构作为滤波器组的作用，得出结论：SIFT只与CNN的前几层相似，那些对潜在的CNN架构进行了显著改进的，比如非常深的模型Simonyan and Zisserman [39]，直接成为了纹理识别的好的多的滤波器组。

## 2. Texture recognition in clutter

A contribution of this work is the analysis of materials and texture attributes in realistic imaging conditions. Earlier datasets such as KTH-TIPS were collected in controlled conditions, which makes their applicability to natural images unclear. More recent datasets such as FMD and DTD remove this limitation by building on images downloaded from the Internet, dubbed images “in the wild”. However, in these datasets texture always fill the field of view of the camera. In this paper we remove this limitation by experimenting for the first time with a large dataset of textures collected in the wild and in cluttered conditions.

本文的一个贡献是，在实际的成像条件下进行材质分析和纹理属性确定。早期的数据集，如KTH-TIPS，是在受控的环境中进行收集的，这在自然图像中的可应用性不太清楚。更新的数据集，如FMD和DTD，通过从互联网上下载图像，去除了这些限制，即是自然环境中的图像。但是，在这些数据集中，纹理充满了相机的视野。在本文中，我们在自然情况下在杂乱环境中收集的纹理图像大型数据集上进行试验，去除了这些限制。

In particular, we build on the Open Surfaces (OS) dataset that was recently introduced by Bell et al. [4] in computer graphics. OS comprises 25,357 images, each containing a number of high-quality texture/material segments. Many of these segments are annotated with additional attributes such as the material name, the viewpoint, the BRDF, and the object class. Not all segments have a complete set of annotations; the experiments in this paper focus on the 58928 that contain material names. Since material classes are highly unbalanced, only the materials that contain at least 400 examples are considered. This result in 53,915 annotated material segments in 10,422 images spanning 22 different classes. Images are split evenly into training, validation, and test subsets with 3,474 images each. Segment sizes are highly variable, with half of them being relatively small, with an area smaller than 64 × 64 pixels. While the lack of exhaustive annotations makes it impossible to define a complete background class, several less common materials (including for example segments that annotators could not assign to a material) are merged in an “other” class that acts as pseudo-background.

特别是，我们在OpenSurfaces(OS)数据集上构建了新的数据集。OS是由Bell等[4]提出的，包含25357幅图像，每幅图像都包含了几个高质量纹理/材质片段。很多这些片段是用额外的属性标注的，比如材质名称，视角，BRDF，和目标类别。并不是所有的片段都有完整的标注集；本文的试验关注的是58928个包括材质名称的。因为材质类别高度不平衡，所以我们只考虑了至少包含400个样本的材质类别。这得到了53915个标注的材质片段，10422幅图像，在22个类别中。图像平均分成训练，验证和测试集，每个集合3474幅图像。片段的大小变化很大，大约一半相对较小，小于64×64像素。缺少完全的标注使其不可能定义一个完整的背景类别，但几种不太常见的材质合并到一起，称为“其他”的类别，可以作为伪背景。

In order to study perceptual properties as well as materials, we augment the OS dataset with some of the 47 attributes from the DTD [7]. Since the OS segments do not trigger with sufficient frequency all the 47 attributes, the evaluation is restricted to eleven of them for which it was possible to identify at least 100 matching segments. The attributes were manually labelled in the 53,915 segments retained for materials. We refer to this data as OSA. The complete list of images, segments, labels, and splits are available at http://www.robots.ox.ac.uk/˜vgg/data/dtd/.

为研究感知性质以及材质，我们用DTD中的一些47个属性来扩增OS数据集。由于OS中的片段出现47个属性的频率不够，我们对其的评估限制在了其中的11个，这样才可能找到至少100个匹配的片段。这些属性手动标注到53915个片段中。我们称这个数据集为OSA。图像，片段，标注和片段的完整列表已经放在网上。

## 3. Method

This section describes the methodological contributions of this paper: region description and segmentation. 本节描述了本文的方法性贡献：区域描述和分割。

### 3.1. Region description

This section introduces a number of visual descriptors suitable to model the appearance of image regions. Texture is traditionally described by orderless pooling of filter bank responses as, unlike in objects, the overall shape information is usually unimportant. However, small under-sampled textures may benefit if recognized in the context of an object. Thus, the primacy of orderless pooling may not always hold in the recognition of textures in natural conditions.

本节提出了几个视觉描述子，可以对图像区域的外观进行建模。纹理在传统上是通过滤波器组响应的无序池化来描述，这与目标的描述不同，因为大体的形状信息是不重要的。但是，小的欠采样的纹理，如果在目标识别的环境中，会有受益。因此，在自然条件下，无序池化的在纹理识别中重要性可能不会那么高。

In order to explore the interplay between shape and orderless pooling, we evaluate two corresponding region descriptors: FC-CNN for shape and FV-CNN for texture. Both descriptors are based on the same CNN features [23] obtained from an off-the-shelf CNN pre-trained on the ImageNet ILSVRC 2012 data as suggested in [6, 21, 35]. Since the underlying CNN is the same, it is meaningful to compare FC- and FV-CNN directly.

为探索形状和无序池化之间的相互关系，我们评估了两个对应的区域描述子：形状的FC-CNN，纹理的FV-CNN。这两个描述子是基于同样的CNN特征的[23]，其特征是用ImageNet ILSVRC 2012上预训练的开箱即用的CNN得到的。由于潜在的CNN是相同的，直接比较FC-和FV-CNN是有意义的。

**Object descriptor: FC-CNN**. The FC-CNN descriptor is obtained by extracting as features the output of the penultimate Fully-Connected (FC) layer of a CNN, including the non-linear gating function, applied to the input image. This can be considered an object descriptor because the fully connected layers allow FC-CNN to capture the overall shape of the object contained in the region. FC-CNN is applied to an image region R (which may be the whole image) by warping the bounding box enclosing R (plus a 10% border) to a square of a fixed size matching the default CNN input geometry, obtaining the same R-CNN descriptor introduced by Girshick et al. [14] as a state-of-the-art object detector in the PASCAL VOC [10] data.

**目标描述子：FC-CNN**。FC-CNN描述子是通过将CNN的倒数第二层全连接层的输出提取作为特征得到的，包括非线性的门控函数，应用于输入图像。这可以认为是一个目标描述子，因为全连接层使FC-CNN可以捕获区域中包含的目标的总体形状。FC-CNN应用导一个图像区域R中（可能是整个图像），将包含R的边界框变形成一个固定大小的方形，与CNN的默认输入形状相匹配，在PASCAL VOC数据集的目前最好的目标检测器是Girshick等提出的R-CNN描述子。

**Texture descriptor: FV-CNN**. The FV-CNN descriptor is inspired by the state-of-the-art texture descriptors of [7] based on the Fisher Vector (FV). Differently from FC-CNN, FV pools local features densely within the described regions removing global spatial information, and is therefore more apt at describing textures than objects. Here FV is computed on the output of a single (last) convolutional layer of the CNN, but we compared features from other layers as well (Sect 4.4). By avoiding the computation of the fully connected layers, the input image does not need to be rescaled to a specific size; in fact, the dense convolutional features are extracted at multiple scales and pooled into a single FV just like for SIFT. The pooled convolutional features are extracted immediately after the last linear filtering operator and are not otherwise normalised.

**纹理描述子：FV-CNN**。FV-CNN描述子是由目前最好的纹理描述子[7]启发得到的，这是基于Fisher向量的。与FC-CNN不同，FV对描述区域中的局部特征进行密集池化，去除了全局的空域信息，因此更适用于描述纹理。这里FV是从CNN的最后一个卷积层的输出中计算得到的，但我们也与其他层中得到的特征进行比较。避免了全连接层的计算，因此输入图像不需要改变导特定的大小；实际上，密集卷积特征在多个层中进行提取，池化导一个FV中，这与SIFT一样。池化的卷积特征在最后的线性滤波算子后立刻提取，并没有进行其他归一化。

The FV-CNN descriptor is related to the one proposed by Gong et al. [15]. There VLAD pooling, which is similar to FV, is applied to FC-CNN-like descriptors extracted from densely sampled image windows. A key difference of FV-CNN is that dense features are extracted from the convolutional rather than fully-connected layers. This is more natural, significantly more efficient (as it does not require recomputing the network for each extracted local descriptor) and, as shown in Sect. 4, more accurate.

FV-CNN描述子与Gong等[15]提出的描述子相关。那里的VLAD池化与FV类似，应用到FC-CNN类似的描述子中，这是从密集采样的图像窗口中提取出来的。FV-CNN的一个关键差异是，密集特征是从卷积层中提取出来的，而不是从全连接层中提取出来的。这是更自然的，效率更高（因为不需要对每个提取的局部描述子重新计算网络），在第4部分我们也可以看到，更加准确。

### 3.2. Region segmentation

This section discusses our method to automatically partition an image into a number of recognisable regions. Inspired by Cimpoi et al. [7] that successfully ported object description methods to texture descriptors, here we propose a segmentation technique inspired by object detection. An increasingly popular method for object detection, followed for example by R-CNN [14], is to first propose a number of candidate object regions using low-level image cues, and then verifying a shortlist of such regions using a powerful classifier. Applied to textures, this requires a low-level mechanism to generate textured region proposals, followed by a region classifier. A key advantage of this approach is that it allows applying object- (FC-CNN) and texture-like (FV-CNN) descriptors alike. After proposal classification, each pixel can be assigned more than one label; this is solved with simple voting schemes, also inspired by object detections methods.

本节讨论我们的方法将一幅图像自动的分割成几个可识别的区域。[7]成功的将目标描述方法转化成纹理描述子，受其启发，这里我们提出一个受目标检测启发得到的分割方法。R-CNN这种受欢迎的目标检测方法，首先用底层图像线索提出几种候选目标区域，然后使用很强的分类器来验证少量这种区域。应用于纹理，这需要以中国底层机制来生成纹理区域候选，然后进行区域分类。这种方法的一个关键优势是，应用于目标FC-CNN和纹理FV-CNN是类似的。在候选分类后，每个像素可以指定多余一个标签；这可以通过简单的投票机制来解决，也是受目标检测方法启发得到的。

The paper explores two such region generation methods: the crisp regions of [19] and the multi-scale combinatorial grouping of [3]. In both cases, region proposals are generated using low-level image cues, such as colour or texture consistency, as specified by the original methods. It would of course be possible to incorporate FC-CNN and FV-CNN among these energy terms to potentially strengthen the region generation mechanism itself. However, this contradicts partially the logic of the scheme, which breaks down the problem into cheaply generating tentative segmentations and then verifying them using a more powerful (and likely expensive) model. Furthermore, and more importantly, these cues focus on separating texture instances, as presented in each particular image, whereas FC-CNN and FV-CNN are meant to identify a texture class. It is reasonable to expect instance-specific cues (say the colour of a painted wall) to be better for segmentation.

本文探索了两个这样的区域生成方法：[19]的crisp区域，和[3]的多尺度组合分组。在这两种情况下，区域建议都是使用底层图像线索来生成的，比如色彩或纹理一致性，这是由原始方法来指定的。将FC-CNN和FV-CNN加入到这些能量项中，以加强区域生成机制，当然是可能的。但是，这违反了方案的部分逻辑，即将问题分解成廉价的生成实验性的分割，然后使用更强的模型来进行验证。而且，更重要的是，这些线索聚焦在分离纹理实例，就像在每个特别的图像中所示的，而FC-CNN和FV-CNN是要识别一个纹理类别的。期待实例相关的线索对分割来说更好，这是合理的。

## 4. Results

This section evaluates the proposed region recognition methods for classifying and segmenting materials, describable texture properties, and higher-level object categories. Sect. 4.1 evaluates the classification task by assessing how well regions can be classified given that their true extent is known and Sect. 4.3 evaluates both classification and segmentation. The rest of the section introduces the evaluation benchmarks and technical details of the representations.

本节评估提出的区域识别方法，在对材质、可描述的纹理性质和更高层的目标类别进行分类和分割时的效果。4.1节评估分类任务，即在区域的真值给定时，评估区域分类的效果，4.3节评估了分类和分割。本节剩下部分介绍了评估的基准测试和表示的技术细节。

**Datasets**. The evaluation considers three texture recognition benchmarks other than OS (Sect. 2). The first one is the Flickr Material Dataset (FMD) [37], a recent benchmark containing 10 material classes. The second one is the Describable Texture Datasets (DTD) [7], which contains texture images jointly annotated with 47 describable attributes drawn from the psychological literature. Both FMD and DTD contain images “in the wild”, i.e. collected in uncontrolled conditions. However, differently from OS, these images are uncluttered. The third texture dataset is KTH-TIPS-2b [5, 17], containing a number of example images for each of four samples of 11 material categories. For each material, images of one sample are used for training and the remaining for testing.

**数据集**。除了OS，评估还考虑了三个纹理识别基准测试（第2部分）。第一个是Flickr材质数据集(FMD)[37]，包含10个材质类别。第二个是可描述的纹理数据集(DTD)[7]，用47个可描述的属性对纹理图像进行了联合标注。FMD和DTD所包含的图像都是自然条件下的，即在不受控的条件下收集到的。但是，与OS不同的是，这些图像都不是杂乱的。第三个纹理数据集是KTH-TIPS-2b，对11个材质类别都包含了很多图像样本。对每个材质，一个样本用于训练，剩下的用于测试。

Object categorisation is evaluated in the PASCAL VOC 2007 [10] dataset, containing 20 object categories, any combination of which may be associated to any of the benchmark images. Scene categorisation uses the MIT Indoor [34] dataset, containing 67 indoor scene classes. Finegrained categorisation uses the Caltech/UCSD Bird dataset (CUB) [45], containing images of 200 bird species.

目标分类在PASCAL VOC 2007数据集上进行评估，包含20个目标类别，其任意组合形成了基准测试中的图像。场景分类使用的是MIT室内数据集，包含67个室内场景类别。细粒度分类使用的是CUB，包含200个鸟的类别的图像。

Note that some of these datasets come with ground truth region/object localisation. The +R suffix will be appended to a dataset to indicate that this information is used both at training and testing time. For example, OS means that segmentation is performed automatically at test time, whereas OS+R means that ground-truth segmentations are used.

注意，这些数据集中的一些是带有真值区域/目标定位信息的。+R的后缀会加到数据集后面，以表示在训练和测试时都用到了这些信息。比如，OS意思是在测试时进行了自动分割，而OS+R意味着使用了真值分割。

**Evaluation measures**. For each dataset the corresponding standard evaluator protocols and accuracy measures are used. In particular, for FMD, DTD, MIT Indoor, CUB, and OS+R, evaluation uses average classification accuracy, per-segment/image and normalized for each class. When evaluating the quality of a segmentation algorithm, however, one must account for the fact that in most datasets, and in OS and MSRC in particular, not all pixels are labelled. In this case, accuracy is measured per-pixel rather than per-segment, ignoring all pixels that are unlabelled in the ground truth. For MSRC, furthermore, accuracy is normalised across all pixels regardless of their category. For OSA, since some segments may have more than one label, we are reporting mAP, following the standard procedure for multi-label datasets. Finally, PASCAL VOC 2007 classification uses mean average precision (mAP), computed using the TRECVID 11-point interpolation [10].

**评估度量**。对每个数据集，使用对应的标准评估协议和准确率度量。特别是，对于FMD，DTD，MIT室内，CUB和OS+R，评估使用了平均分类准确率，每个片段/图像和对每个类别的归一化。但当评估一个分割算法的质量时，必须考虑到，在多数数据集中，特别是在OS和MSRC中，并不是所有的像素都进行了标注。在这种情况下，准确率是逐个像素进行度量的，而不是逐个片段进行度量的，忽略了在真值中未标注的所有像素。而且，对于MSRC，准确率是在所有像素中归一化的，不论其类别。对OSA，由于一些片段的标签多余一个，当给出mAP时，我们是按照多标签数据集的标准过程的。最后，PASCAL VOC 2007分类使用了mAP，使用TRECVID 11点插值进行计算的。

**Descriptor details**. FC-CNN and FV-CNN build on the pre-trained VGG-M [6] model as this performs better than other popular models such as [21] while having a similar computational cost. This network results in 4096-dimensional FC-CNN features and 512-dimensional local features for FV-CNN computation. The latter are pooled into a FV representation with 64 Gaussian components, resulting in 65K-dimensional descriptors. While the FV-CNN dimensionality is much higher than the 4K dimensions of FC-CNN, the FV is known to be highly redundant and can be typically compressed by one order of magnitude without appreciable reduction in the classification performance [31], so the effective dimensionality of FC- and FV-CNN is likely comparable. We verified that by PCA-reducing FV to 4096 dimensions and observing only a marginal reduction in classification performance in the PASCAL VOC object recognition task described below. In addition to VGG-M, the recent state-of-the art VGG-VD (very deep with 19 layers) model of Simonyan and Zisserman [39] is also evaluated.

**描述子细节**。FC-CNN和FV-CNN是在预训练的VGG-M[6]模型中构建得到的，因为这个模型比其他模型要好，而且计算量类似。这个网络得到了4096维的FC-CNN特征，和512维的局部特征进行FV-CNN计算，后者池化成一个FV表示，有64个高斯组成部分，得到了一个65K维的描述子。虽然FV-CNN的维度比FC-CNN的4K维要高的多，但FV肯定是高度冗余的，一般可以压缩一个数量级，而分类性能不会有什么下降，所有FC-和FV-CNN的有效维度很可能是类似的。我们通过PCA降维将FV降到4096维进行验证，观察到分类性能在PASCAL VOC目标识别任务中只有很小的下降。除了VGG-M，最近的目前最好的VGG-VD模型也进行了评估。

Due to the similarity between FV-CNN and the dense SIFT FV descriptors used for texture recognition in [7], the latter is evaluated as well. Since SIFT descriptors are smaller (128-dimensional) than the convolutional ones (512-dimensional), a larger number of Gaussian components (256) are used to obtain FV descriptors with a comparable dimensionality. The SIFT descriptors support is 32 × 32 pixels at the base scale.

由于FV-CNN和[7]中用于纹理识别的密集SIFT FV描述子的相似性，后者也进行了评估。由于SIFT描述子（128维）比卷积的描述子（512维）更小，大量高斯组成部分(256)用于得到可比维度的FV描述子。SIFT描述子在基础尺度上支持32 × 32像素。

In order to make results comparable to [7], we use the same settings whenever possible. FV-CNN and D-SIFT compute features after rescaling the image by factors 2^s, s = −3, −2.5, ... 1.5 (but, for efficiency, discarding scales that would make the image larger than 1024^2 pixels). Before pooling descriptors with a FV, these are usually de-correlated by using PCA. Here PCA is applied to SIFT, additionally reducing its dimension to 80, as this was empirically shown to improve the overall recognition performance. However, PCA is not applied to the convolutional features in FV-CNN as in this case results were worse.

为使结果与[7]可比，我们在可能的地方都使用了相同的设置。FV-CNN和D-SIFT在用系数2^s, s = −3, −2.5, ... 1.5改变图像大小后，计算特征（但是，为了效率，会丢掉使图像大于1024^2像素的尺度）。在使用FV来池化描述子之前，通常会用PCA去相关。这里PCA应用于SIFT，可以额外将其维度降到80，因为这通过经验表明会改进总体的识别性能。但是，PCA在FV-CNN中没有应用于卷积特征，因为会使结果变差。

**Learning details**. The region descriptors (FC-CNN, FV-CNN, and D-SIFT) are classified using 1-vs-rest Support Vector Machine (SVM) classifiers. Prior to learning, descriptors are L2 normalised and the learning constant set to C = 1. This is motivated by the fact that, after data normalisation, the exact choice of C has a negligible effect on performance. Furthermore, the accuracy of the 1-vs-rest classification scheme is improved by recalibrating the SVM scores after training, by scaling the SVM weight vector and bias such that the median scores of the negative and positive training samples for each class are mapped respectively to the values −1 and 1.

**学习细节**。区域描述子(FC-CNN, FV-CNN和D-SIFT)使用1-vs-rest SVM分类器进行分类。在学习之前，描述子进行L2归一化，学习常数设为C=1。因为数据经过归一化后，C的具体选择对性能几乎没有影响了。而且，1-vs-rest分类方案的准确率，在训练后重新校准SVM分数可以得到改进。

### 4.1. Region recognition: textures

This and the following section evaluate region recognition assuming that the ground-truth region R is known (Table 1), for example because it fills the entire image. This section focuses on textures (materials and perceptual attributes), while the next on objects and scenes.

本节和下一节评估区域识别，假设真值区域R是已知的（表1），比如因为其充满了整幅图像。本节关注纹理（材质和感知属性），而下一节关注目标和场景。

**Texture recognition without clutter**. This experiment evaluates the performance of FC-CNN, FV-CNN, D-SIFT, and their combinations in standard texture recognition benchmarks such as FMD, KTH-TIPS-2, and DTD. FC-CNN is roughly equivalent to the DeCAF method used in [7] for this data as regions fill images; however, while the performance of our FC-CNN is similar in KTH (∼ 70%), it is substantially better in FMD (60.7% → 70.4% accuracy) and DTD (54.8% → 58.7%). This likely is caused by the improved underlying CNN, an advantage which is more obvious in FMD and DTD that are closer to object recognition than KTH. FV-CNN performs within ±2% in FMD and KTH but substantially better for DTD (58.7% → 66.6%). D-SIFT is comparable in performance to FC-CNN in DTD and KTH, but substantially worse (70.4% → 59.2%) in FMD. Our conclusion is that, even when textures fill the input image as in these benchmarks, orderless pooling in FV-CNN and D-SIFT can be either the same or substantially better than the pooling operated in the fully-connected layers by FC-CNN.

**没有杂乱的纹理识别**。这个试验评估了FC-CNN, FV-CNN, D-SIFT，以及在标准纹理识别基准测试中其组合的性能。FC-CNN与[7]中的DeCAF方法大致等价，因为区域充满了图像；但是，虽然FC-CNN在KTH中性能类似(~70%)，但在FMD (60.7% → 70.4% accuracy)和DTD (54.8% → 58.7%)中性能明显更好。这很可能是潜在的CNN的改进导致的，这在FMD和DTD中更加明显，更加接近于目标识别。FV-CNN在FMD和KTH中表现在±2%内，但在DTD中效果明显好很多(58.7% → 66.6%)。D-SIFT与FC-CNN在DTD和KTH中性能类似，但在FMD中明显很差(70.4% → 59.2%)。我们的结论是，即使在纹理充满输入图像中时，就像在这些基准测试中的一样，FV-CNN中的无序池化和D-SIFT，比全连接层FC-CNN中的池化比，要么类似，要么明显更好。

Combining FC- and FV-CNN improves performance in all datasets by 1 − 3%. While this combination is already significantly above the state-of-the-art in DTD and FMD (+2.6%/11.2%), the method of [7] still outperforms these descriptors in KTH. However, replacing VGG-M with VGG-VD significantly improves the performance in all cases – a testament to the power of deep features. In particular, the best method FC+FV-CNN-VD, improves the state of the art by at least 6% in all datasets. Interestingly, this is obtained by using a single low-level feature type as FC- and FV-CNN build on the same convolutional features. Adding D-SIFT results in at most ∼ 1% improvement, and in some cases it slightly degrades performance.

将FC-和FV-CNN结合到一起，在所有数据集中性能改进了1-3%。这种组合在DTD和FMD中已经明显高于目前最好的水平(+2.6%/11.2%)，但[7]中的方法早KTH中表现仍然比这些方法好。在所有情况中，将VGG-M替换为VGG-VD会显著改进性能，这是深度特征能力的证明。特别是，最佳方法FC+FV-CNN-VD，在所有数据集中，比目前最好性能至少改进了6%。有趣的是，这是通过使用单个低层特征类型得到的，因为FC-和FV-CNN都是构建在同样的卷积特征上的。加入了D-SIFT最多可以得到~1%的改进，在一些情况中会使得性能略微恶化。

**Texture recognition in clutter**. The advantage of FV-CNN over FC-CNN is much larger when textures do not fill the image but are extracted from clutter. In OS+R (Sect. 2), material recognition accuracy starts at about 46% for both FC-CNN and D-SIFT; however, FV-CNN improves this by more than 11% (46.5% → 58.1%). The combination of FC- and FV-CNN improves results further by ∼ 2%, but adding SIFT deteriorates performance. With the very deep CNN conclusions are similar; however, switching to VGG-VD barely affects the FC-CNN performance (46.5 → 48.0%), but strongly affects the one of FV-CNN (58.1% → 65.1%). This confirms that FC-CNN, while excellent in object detection, is not a very good descriptor for classifying textured regions. Results in OSA+R for texture attribute recognition (Sect. 2) and in MSRC+R for semantic segmentation are analogous; it is worth noting that, when ground-truth segments are used in this experiment, the best model achieves a nearly perfect 99.7% classification rate in MSRC.

**杂乱中的纹理识别**。在纹理没有充满图像中时，要从杂乱的背景中提取出来，FV-CNN比FC-CNN的优势更加明显。在OS+R中，材质识别准确率对于FC-CNN和D-SIFT来说开始是46%；但是，FV-CNN将其改进了超过11% (46.5% → 58.1%)。FC-和FV-CNN的这种组合，将结果改进了~2%，但加入SIFT会使得性能变坏。非常深的CNN的结论是类似的；但是，切换到VGG-VD仅仅影响了FC-CNN的性能一点点(46.5 → 48.0%)，但极大的影响FV-CNN的性能(58.1% → 65.1%)。这确认了，FC-CNN虽然在目标检测中效果很好，但对分类纹理区域效果并不太好。在OSA+R中对纹理属性识别的结果，和在MSRC+R中进行语义分割的结果是类似的；值得注意的是，当真值片段用在试验中时，最好的模型在MSRC中得到了一个近似完美的99.7%的分类准确率。

### 4.2. Region recognition: objects and scenes

This section shows that the FV-CNN descriptor, despite its orderless nature that make it an excellent texture descriptor, excels at object and scene recognition as well. In the remainder of the section, and unless otherwise noted, region descriptors are applied to images as a whole by considering these single regions.

本节展示了，FV-CNN描述子尽管有其无序的本质，但仍然是一个非常优秀的纹理描述子，在目标和场景检测上效果也非常好。在本节剩下的部分中，除非另外提出，区域描述子通过考虑这些很多单个区域而应用于图像。

**FV-CNN vs FC-CNN**. As seen in Table 1, in PASCAL VOC and MIT Indoor the FC-CNN descriptor performs very well but in line with previous results for this class of methods [6]. FV-CNN performs similarly to FC-CNN in PASCAL VOC, but substantially better (+5% for VGG-M and +13% for VGG-VD) in MIT Indoor. As further discussed below, this is an example of the ability of FV-CNN to transfer between domains better than FC-CNN. The gap between FC-CNN and FV-CNN is the highest for the very deep VGG-VD models (68.1% → 81.1%), a trend also exhibited by other datasets as seen in Tab. 1. In the CUB dataset, FV-CNN significantly outperforms FC-CNN both whether the descriptor is computed from the whole image (CUB) or from the bird bounding box (CUB+R). In the latter case, the difference is very large (+10 − 14%).

**FV-CNN vs FC-CNN**.如表1所示，在PASCAL VOC和MIT室内数据集中，FC-CNN描述子效果很好，与之前的这类方法效果类似[6]。FV-CNN在PASCAL VOC上与FC-CNN效果类似，但在MIT室内数据集上效果明显更好(+5% for VGG-M and +13% for VGG-VD)。下面会进一步讨论，这是FV-CNN在领域之前的迁移能力优于FC-CNN的一个例子。在FC-CNN和FV-CNN之间的差距，在使用VGG-VD模型时最高(68.1% → 81.1%)，这在其他数据集中时，也有同样的趋势，如表1所示。在CUB数据集中，FV-CNN明显超过了FC-CNN，这在描述子是从整幅图像计算(CUB)时，和从鸟类的边界框计算(CUB+R)时，都是这种情况。在后者的情况下，其差异非常大(+10−14%)。

**Comparison with alternative pooling methods**. FV-CNN is related to the method of [15], which uses a similar underlying CNN and VLAD instead of FV for pooling. Notably, however, FV-CNN results on MIT Indoor are markedly better than theirs for both VGG-M and VGG-VD (68.8% → 73.5%/81.1% resp.) and marginally better (68.8% → 69.1%) when the same CAFFE CNN is used (Tab. 3). The key difference is that FV-CNN pools convolutional features, whereas [15] pools fully-connected descriptors extracted from square image patches. Thus, even without spatial information as used by [15], FV-CNN is not only substantially faster, but at least as accurate. Using the same settings, that is, the same net and the same three scales, our approach results in an 8.5× speedup.

**与其他池化方法的对比**。FV-CNN与[15]的方法是有关联的，其使用了一个类似的潜在CNN和VLAD池化，而没有使用FV进行池化。但是，值得注意的是，FV-CNN在MIT室内数据集上明显比它们的要好，包括VGG-M和VGG-VD模型(68.8% → 73.5%/81.1% resp.)，当使用的是CAFFE CNN时，略好一些(68.8% → 69.1%，如表3).其关键区别是，FV-CNN是对卷积特征进行池化，而[15]对全连接描述子进行池化，这是从方形图像块计算得到的。因此，即使不用[15]所使用的空间信息，FV-CNN不仅快很多，而且也一样准确。使用相同的设置，即，相同的网络和相同的三个尺度，我们的方法加速了8.5x。

**Comparison with the state-of-the-art**. The best result obtained in PASCAL VOC is comparable to the current state-of-the-art set by the deep learning method of [44] (85.2% → 85.0%), but using a much more straightforward pipeline. In MIT Places our best performance is also substantially superior (+10%) to the current state-of-the-art using deep convolutional networks learned on the MIT Place dataset [47] (see also below). In the CUB dataset, our best performance is a little short (∼ 3%) of the state-of-the-art results of [46]. However, [46] uses a category-specific part detector and corresponding part descriptor as well as a CNN fine-tuned on the CUB data; by contrast, FV-CNN and FC-CNN are used here as global image descriptors which, furthermore, are the same for all the datasets considered. Compared to the results of [46] without part-based descriptors (but still using a part-based object detector), our global image descriptors perform substantially better (62.1% → 69.1%).

**与当前最好结果的比较**。在PASCAL VOC上得到的最好结果，与[44]的深度学习方法得到的当前最好结果类似(85.2% → 85.0%)，但使用了一个更加直接的流程。在MIT Places数据集中，我们最优的性能也比目前最好的模型要好很多(+10%)，而后者是用深度学习模型在MIT Place数据集上学习得到的。在CUB数据集上，我们最好的性能比目前最好的结果[46]略差一些(∼3%)。但是，[46]使用了一个类别专用的部位检测器和对应的部位描述子，以及一个在CUB上精调的CNN；对比起来，FV-CNN和FC-CNN在这里是作为全局图像描述子使用，而且这是对所有数据集都一样的。与[46]不使用基于部位的描述子的结果相比较（但仍然使用了一个基于部位的目标检测器），我们的全局图像描述子效果就非常好了(62.1% → 69.1%)。

We conclude that FV-CNN is a very strong descriptor. Results are usually as good, and often significantly better, than FC-CNN. In most applications, furthermore, FV-CNN is many times faster as it does not require evaluating the CNN for each target image region. Finally, FC- and FV-CNN can be combined outperforming the state-of-the-art in many benchmarks.

我们总结得到，FV-CNN是非常强的描述子。结果通常与FC-CNN一样好，但也经常好很多。而且，在多数应用中，FV-CNN通常也快很多倍，因为不需要对每个目标图像区域计算CNN。最后，FC-和FV-CNN可以结合使用，在很多基准测试中超过了目前最好的模型。

**Domain transfer**. So far, the same underlying CNN features, trained on ImageNet's ILSVCR, were used for all datasets. Here we investigate the effect of using domain-specific features. To do so, we consider the PLACES [47], trained to recognize places on a dataset of about 2.5 million labeled images. [47] showed that, applied to the task of scene recognition in MIT Indoor, these features outperform similar ones trained on ILSVCR (denoted CAFFE [21] below) – a fact explained by the similarity of domains. Below, we repeat this experiment using FC- and FV-CNN descriptors on top of VGG-M, VGG-VD, PLACES, and CAFFE.

**领域迁移**。迄今为止，在所有数据集上使用的都是在ImageNet ILSVCR上训练的相同的潜在CNN特征。这里，我们调研了使用领域专用的特征的效果。为此，我们考虑PLACES数据集[47]，在一个大约250万标注图像的数据集上训练用于识别位置。[47]表明，应用于在MIT室内数据集上的场景识别任务中，这些特征超过了在ILSVRC上训练的类似结果（下面用CAFFE[21]表示），这个结果可以由领域的相似性来进行解释。下面，我们使用VGG-M, VGG-VD, PLACES和CAFFE上的FC-和FV-CNN描述子重复这个试验。

Results are shown in Table 3. The FC-CNN results are in line with those reported in [47] – in scene recognition with FC-CNN the same CNN architecture performs better if trained on the Places dataset instead of the ImageNet data (58.6% → 65.0% accuracy). Nevertheless, stronger CNN architectures such as VGG-M and VGG-VD can approach and outperform PLACES even if trained on ImageNet data (65.0% → 63.0%/68.1%).

结果如表3所示。FC-CNN结果与[47]中的类似，在场景识别中，使用相同CNN架构的FC-CNN，如果是在Places数据集中训练，结果比在ImageNet上训练效果要好(58.6% → 65.0% accuracy)。尽管如此，更强的CNN架构，如VGG-M和VGG-VD，即使是在ImageNet数据上训练，也会接近甚至超过在Places数据集上训练的结果(65.0% → 63.0%/68.1%)。

However, when it comes to using the filter banks with FV-CNN, conclusions are very different. First, FV-CNN outperforms FC-CNN in all cases, with substantial gains up to 20% in correspondence of a domain transfer from ImageNet to MIT Indoor. Second, the advantage of using domain-specific CNNs disappears. In fact, the same CAFFE model that is 6.4% worse than PLACES with FC-CNN, is actually 1.5% better when used in FV-CNN. The conclusion is that FV-CNN appears to be immune, or at least substantially less sensitive, to domain shifts.

但是，当使用FV-CNN的滤波器组时，结论就非常不一样了。首先，FV-CNN在所有情况中都超过了FC-CNN，在从ImageNet迁移到MIT室内数据集时，性能甚至提升了20%。第二，使用领域专用CNN的优势消失了。实际上，在使用FC-CNN时，CAFFE模型比PLACES性能差了6.4%，但在使用FV-CNN时，性能好了1.5%。结论是，FV-CNN似乎对领域迁移是免疫的，或至少敏感度非常低的。

Our tentative explanation of this surprising phenomenon is that the convolutional layers are less committed to a specific dataset than the fully ones. Hence, by using those, FV-CNN tends to be a more general than FC-CNN.

对这种令人惊讶的现象，我们的实验性解释是，卷积层与全连接层相比，更加不是专用于特定数据集的。因此，通过使用这些，FV-CNN倾向于比FC-CNN更加通用。

### 4.3. Texture segmentation

The previous section considered the problem of region recognition when the region support is known at test time. This section studies the problem of recognising regions when their extent R is not known and also be estimated.

前面一节考虑的问题是区域识别，在测试时，区域的支撑是已知的。本节研究的是，当R是未知的时候，对区域的识别问题，并对R进行估计。

The first experiment (Tab. 2) investigates the simplest possible scheme: combining the region descriptors of Sect. 4.1 with a general-purpose image segmentation method, namely the crisp regions of [19]. Two datasets are evaluated: OS for material recognition and MSRC for things & stuff. Compared to OS+R, classifying crisp regions results in a drop of about 5% points for all descriptors. As this dataset is fairly challenging with best achievable performance is 55.4%, this is a satisfactory result. But it also illustrates that there is ample space for future improvements. In MSRC, the best accuracy is 87.0%, just a hair above the best published result 86.5% [25]. Remarkably, these algorithms not use any dataset-specific training, nor CRF-regularised semantic inference: they simply greedily classify regions as obtained from a general-purpose segmentation algorithms. Qualitative segmentation results (sampled at random) are given in Fig. 2 and 3.

第一个试验（表2）研究的是最简单的方案：将4.1节中的区域描述子与一个通用目标图像分割方法[19]相结合。对两个数据集进行了评估：OS进行材质识别，MSRC进行things & stuff识别。与OS+R进行比较，对crisp区域进行分类的结果，对所有描述子效果都下降了大约5%。因为这个数据集有一定挑战性，得到的最好性能是55.4%，这是一个很令人满意的结果。但也说明了，有很大的空间进行进一步改进。在MSRC中，最好的准确率是87.0%，比已经发表的最好结果86.5%略微高了一点。出乎意料的是，这些算法并没有使用任意数据集专用的训练，或CRF正则化的语义推理：它们只是贪婪的对通用分割算法得到的区域进行分类。定性的分割结果如图2和图3所示。

Unlike crisp regions, the proposals of [3] are overlapping and a typical image contains thousands of them. We propose a simple scheme to combine prediction from multiple proposals. For each proposal we set its label to the highest scoring class, and score to the highest score. We then sort the proposals in the increasing order of their score divided by their area and paste them one by one. This has the effect of considering larger regions before smaller ones and more confident regions after less ones for regions of the same area. Results using FV-CNN shown in Tab. 2 in brackets (FC-CNN was too slow for our experiments). The results are comparable to those using crisp regions, and we obtain 55.7% accuracy on the OS dataset. Our initial attempts at schemes such as non-maximum suppression of overlapping regions that are quite successful for object segmentation [16] performed rather poorly. We believe this is because unlike objects, material information is fairly localized and highly irregularly shaped in an image. However, there is room for improvement by combining evidence from multiple segmentations.

与crisp区域不同，[3]的建议是重叠的，一幅典型的图像会包含数千个建议。我们提出一种简单的方案，从多个建议中将预测结合起来。对每个建议，我们将标签设为最高的得分类别，将分数设为最高的分数。然后我们将建议按照分数除以区域面积的递增顺序进行分类，并一个一个进行粘贴。这有一个效果，首先考虑更大的区域，然后才考虑较小的区域，对同样面积的区域，首先考虑置信度较低的区域，然后是置信度较高的区域。使用FV-CNN的结果如表2中括号的结果所示(FC-CNN在这个试验中太慢)。这些结果与使用crisp区域的结果类似，在OS数据集中，我们得到了55.7%的准确率。我们对方案的初始尝试，比如重叠区域的NMS，这在目标分割中效果很好，但在我们的试验中效果较差。我们相信，这是因为与目标不同，材质信息是非常局部化的，在图像中的形状是高度不规则的。但是，通过将多个分割的证据进行结合，还有改进的空间。

### 4.4. Convolutional layer analysis

We study the performance of filter banks extracted from different layers of a CNN in the FV-CNN framework. We use the VGG-M network which has five convolutional layers. Results on various datasets, obtained as in Sect. 4.1 and 4.2, are shown in Fig. 4. In addition we also show the performance using FVs constructed from dense SIFT using a number of words such that the resulting FV is roughly the same size of FV-CNN. The CNN filter banks from layer 3 and beyond significantly outperform SIFT. The performance monotonically improves from layer one to five.

我们在FV-CNN框架中，从CNN的不同层中提取滤波器组，研究了其性能。我们使用VGG-M网络，有5个卷积层。在各种数据集上4.1节和4.2节的结果，如图4所示。除此以外，我们还展现了从密集SIFT中使用FVs的性能。第3层的CNN滤波器组的性能显著超过了SIFT。从层1到层5性能单调递增。

## 5. Conclusions

We have conducted a range of experiments on material and texture attribute recognition in a large dataset of textures in clutter. This benchmark was derived from OpenSurfaces, an earlier contribution of the computer graphics community, highlights the potential for collaboration between computer graphics and vision communities. We have also evaluated a number of state-of-the-art texture descriptors on these and many other benchmarks. Our main finding is that orderless pooling of convolutional neural network features is a remarkably good texture descriptor, versatile enough to dubbed as a scene and object descriptor, resulting in new state-of-the-art performance in several benchmarks.

我们在一个大规模杂乱中的纹理数据集上，对材质识别和纹理属性识别，进行了一系列试验。基准测试是从OpenSurfaces推导得到的。我们在这些和很多其他基准测试中，评估了几个目前最好的纹理描述子。我们的主要发现是，CNN特征的无序池化是一种非常好的纹理描述子，非常通用，可以作为场景描述子和目标描述子，在几个基准测试中都得到了目前最好的性能。