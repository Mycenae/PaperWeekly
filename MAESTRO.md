# MAESTRO: A Data-Centric Approach to Understand Reuse, Performance, and Hardware Cost of DNN Mappings

Hyoukjun Kwon et. al. @ Georgia Tech & NVIDIA Corp

## 0. Abstract

The efficiency of an accelerator depends on three factors—mapping, deep neural network (DNN) layers, and hardware—constructing extremely complicated design space of DNN accelerators. To demystify such complicated design space and guide the DNN accelerator design for better efficiency, we propose an analytical cost model, MAESTRO. MAESTRO receives DNN model description and hardware resources information as a list, and mapping described in a data-centric representation we propose as inputs. The data-centric representation consists of three directives that enable concise description of mappings in a compiler-friendly form. MAESTRO analyzes various forms of data reuse in an accelerator based on inputs quickly and generates more than 20 statistics including total latency, energy, throughput, etc., as outputs. MAESTRO’s fast analysis enables various optimization tools for DNN accelerators such as hardware design exploration tool we present as an example.

加速器的效率依赖于三个因素，映射，深度神经网络DNN的层，和硬件，构成了DNN加速器极其复杂的设计空间。为弄清楚这样复杂的设计空间，引导DNN加速器设计的效率更高，我们提出解析代价模型，MAESTRO。MAESTRO接收DNN模型描述和硬件资源信息作为列表，和我们提出的以数据为中心的表示描述的映射，作为输入。以数据为中心的表示，包括三个指示符，对映射以编译器友好的形式进行简洁的描述。MAESTRO基于输入，迅速分析加速器中各种形式的数据重用，生成超过20个统计量，包括总计的延迟，功耗，吞吐量，等，作为输入。MAESTRO的快速分析使各种用于DNN加速器的优化工具都可用，比如硬件设计空间探索工具，我们给出了一个例子。

## 1. Introduction

Deep Neural Network (DNN) inference accelerators achieve high performance by exploiting parallelism over hundreds of processing elements (PEs) and high energy efficiency by maximizing data reuse within PEs and on-chip scratchpads. The efficiency (performance and energy efficiency) of a DNN accelerator depends on three factors depicted in Figure 1: 1) the workload (DNN layers), 2) the amount and type of available hardware resources (hardware), and 3) the mapping strategy of a DNN layer on the target hardware (mapping). That is, we can predict the efficiency (latency, energy, buffer requirement, etc.) of an accelerator when we have full parameters for those three factors, which can guide the DNN accelerator design for better efficiency. One critical requirement on the efficiency estimation is that it needs to be fast since the design space (e.g., 480 million valid designs in our hardware DSE even if we fix the target mapping and layer) is huge, and we need to query the efficiency of candidate designs in the search space when we search for an optimal design. How do we implement such a fast efficiency estimation framework that thoroughly considers all the parameters of the three factors that determine the efficiency of DNN accelerators?

DNN推理加速器通过在数百个PEs上利用并行性来获得高性能，通过在PEs和片上便签内存上最大化数据复用来获得高能量效率。DNN加速器的效率（性能和功耗效率）依赖于三个因素，如图1所示：1)workloads (DNN layers)，2)可用的硬件资源的数量和类型（硬件），3)DNN层到目标硬件上的映射策略（映射）。即，当我们有这三个因素的完整参数时，我们可以预测加速器的效率（延迟，功耗，缓冲区要求，等），这可以引导DNN加速器得到更好效率的设计。对效率估计的一个关键要求是要快，因为设计空间非常大（如，如果我们固定目标映射和层，在我们的硬件DSE中有4.8亿种有效的设计），当我们搜索最优设计时，我们需要查询搜索空间中候选设计的效率。我们怎样实现这样一个快速的效率估计框架呢，要彻底考虑这三个因素的所有参数，这确定了DNN加速器的效率。

Such demands led to the development of an analytical cost model instead of cycle-accurate simulators. Analytically, modeling the complex high-dimensional DNN accelerator design space over the three factors (DNN layer, hardware, and mapping) is challenging because it requires deep understanding of complex interaction of hardware components, mapping, and DNN layers. In particular, data reuse in scratchpad memory hierarchy in DNN accelerators is one of the key behaviors, which is critical for energy efficiency, thus the prime optimization target of DNN accelerators. Data reuse pattern is dictated by dataflow, which are data/computation tile scheduling and spatial partitioning strategies without actual tile size as described in Figure 1(a). To systematically and analytically model the data reuse for DNN accelerators’ efficiency estimation, we need a precise and thorough description of mapping and a framework to analyze data reuse of a mapping on target hardware and the DNN layer.

这样的需求带来了解析代价模型的开发，而不是周期准确的仿真器。解析的来说，在三个因素上（DNN层，硬件和映射）建模复杂的高维DNN加速器设计空间是很有挑战的，因为这需要对硬件组成部分、映射和DNN层的复杂交互有深入的理解。特别是，DNN加速器中便签内存层次结构中的数据复用，是一个关键行为，对能耗效率非常关键，因此是DNN加速器的首要优化目标。数据复用模式是由数据流决定的，是数据/计算平铺的调度和空域分割策略，在没有图1a中描述的实际的平铺大小来说的。为对DNN加速器的效率估计进行系统的和解析的建模数据复用，我们需要一个精确和彻底的映射描述，和一个分析映射在目标硬件和DNN层上的数据复用的框架。

Therefore, we propose a data-centric representation of mapping that enables precise descriptions of all the possible mappings in a concise and compiler-friendly manner. Leveraging the compiler-friendly format, we develop MAESTRO, a comprehensive cost-benefit analysis framework based on systematic data reuse analysis. As shown in Figure 1(b), MAESTRO receives the three factors—DNN layer, hardware, and mapping—as inputs and generates more than 20 estimated statistics including latency, energy, the number of buffer accesses, buffer size requirement, etc. We validated the performance statistics of MAESTRO against cycle-accurate RTL simulation results and reported performance in a previous work with the accuracy of 96.1% on average. MAESTRO provides fast cost-benefit estimation based on an analytical model, which took 493 ms to analyze the entire Resent50 layers on a 256PE NVDLA-style accelerator on a laptop with i9-9980H CPU with 16 GB of memory. MAESTRO supports arbitrary layer sizes and a variety of layer operations from state-of-the-art DNN models, which includes CONV1D, CONV2D, fully connected (FC) layer, depthwise separable convolution, up-scale convolution, etc.

因此，我们提出一种映射的数据中心表示，对所有可能的映射以简洁的、编译器友好的方式进行精确描述。利用编译器友好的格式，我们提出了MAESTRO，一个全面的代价收益分析框架，基于系统的数据复用分析。如图1b所示，MAESTRO的输入为三种因素-DNN层，硬件和映射，生成超过20种估计的统计数据，包括延迟，功耗，缓冲区访问的数量，缓冲区大小的要求，等。我们用周期准确的RTL仿真结果，验证了MAESTRO的性能统计，在之前的工作中给出了性能，准确率平均为96.1%。MAESTRO基于解析模型给出了快速的代价收益估计，在256PE的NVDLA类型的加速器上，用一个i9-9980H CPU 16 GB内存的笔记本上，耗时493ms就分析完了整个ResNet50的层。MAESTRO支持目前最好的DNN模型中任意的层的大小，很多的层的运算，包括CONV1D，CONV2D，全连接层，分层可分离卷积，上采样卷积，等。

## 2. Data Reuse in DNN Accelerators

Data reuse is the key behavior in DNN accelerator that improves both latency and energy via reducing the number of remote buffer accesses (i.e., global buffer), which is determined by dataflow. Data reuse opportunities exist when the dataflow assigns the same set of data tiles over consecutive time on the same PE (i.e., reuse in time) or across multiple PEs but not over consecutive time (i.e., reuse in space). We define those opportunities as temporal and spatial reuse opportunities. For example, in the example dataflow in Figure 1, output tiles (orange tiles) remain the same in time 0 and 1, which implies the temporal reuse opportunities. Within time 1, as the spatial partitioning example in Figure 1 shows, input tile 3 is mapped on all the PEs, which implies the spatial reuse opportunities.

数据复用是DNN加速器改进延迟和功耗，减少远程缓冲区（如，全局缓冲区）访问次数的关键行为，这是由数据流决定的。数据复用，可以是数据流在连续的时间内在相同的PE上指定了同样的数据块的集合（即，时间上的复用），或在多个PEs上，但是不是连续的时间（即，空间复用）。我们定义这些为时域和空域复用。比如，在图1的数据流例子中，输出块（橘色块）在时间0和1上保持一致，这意味着时域复用。在时刻1，如图1中的空域分割例子所示，输入块3映射到所有PEs上，这意味着空域复用。

Dataflow implies data reuse opportunities, and we can categorize data reuse in DNN accelerators into four types (data reuse taxonomy), which we summarize in Table 1. Each data reuse type requires proper hardware support to exploit the data reuse opportunity as actual data reuse. We discuss those four reuse types grouped in communication type as follows:

数据流意味着数据复用的机会，我们可以将DNN加速器中的数据复用分类成4种类型（数据复用分类），总结在表1中。每种数据复用类型需要合适的硬件支持，以利用数据复用为实际的数据复用。我们将这四种复用类型按照通信类型分类如下：

**Spatial/Temporal Multicast**. When the spatial/temporal reuse opportunities are in input tensors (i.e., filter and input activation), the reused data can be multicasted to multiple PEs (spatial reuse) or over time (temporal reuse). The examples in Table 1 show such a pattern based on fanout NoC (spatial multicast), which delivers data to multiple PEs at the same time, and buffer (temporal multicast).

**空/时multicast**。当空/时复用是在输入张量中时（如，滤波和输入激活），复用的数据可以multicast到多个PEs中（空域复用）或在多个时间上（时域复用）。表1中的例子基于fanout NoC展示了这样一种模式（空域multicast），同时将数据送到多个PEs中，和缓冲区（时域multicast）。

In the spatial multicast example, tiles 1 and 2 are delivered to PE1 and PE2 at the same time leveraging the multicast capability of fanout hardware. Alternatively, store-and-forward style implementation such as systolic arrays is available with tradeoff of hardware cost and latency. In the temporal multicast example, the same data tile appears over time in the same PE (PE1). That is, we send the data to the future for reuse in the future (i.e., store the data in a buffer and read it in the future). Therefore, temporal multicast, which is reading the same stored data over time, requires a buffer, as shown in Table 1.

在空域multicast的例子中，利用fanout硬件的multicast能力，块1和2同时送到PE1和PE2中。或者，store-and-forward类型的实现，如脉动阵列是可用的，含有硬件代价和延迟的折中。在时域multicast的例子中，相同的数据块在不同的时间上出现在相同的PE中(PE1)。即，我们将数据送到未来以在未来复用（即，将数据存储到缓冲区中，在未来进行读取）。因此，时域multicast是在不同的时间读取相同的存储了的数据，需要一个缓冲区，如表1所示。

**Spatial/Temporal Reduction**. When the spatial reuse opportunities are in the output activation tensor, the reuse pattern in hardware is spatial reduction, which accumulates partial outputs (or, partial sums) for an output across multiple PEs. The example in Table 1 shows an example reuse pattern based on store-and-forward hardware. We observe that the output tiles 1 and 2 are moving to the next PE over time, which illustrates pipelined accumulation to the right direction assuming that PEs are receiving new operands from above (i.e., a row of a systolic array). Alternatively, fanin hardware such as reduction tree can support the spatial reduction.

**时域/空域缩减**。当空域复用是在输出激活张量中时，在硬件中的复用模式是空域缩减，即对一个输出在多个PEs中累加部分输出（或，部分和）。表1中的例子展示了基于store-and-forward硬件的复用模式。我们观察到，输出块1和2随着时间移动到下一个PE中，这描述了向右方的流水线式的累加，假设PEs从前面接收到新的操作数（即，一行脉动阵列）。或者，fanin硬件，如缩减树，可以支持空域缩减。

In contrast, the temporal reuse opportunities imply that we compute partial sums over time and accumulate them within the same location. This type of reuse requires a buffer since intermediate results need to be stored and read again in the future, which effectively indicates multiple read-modify-write to a buffer. The example in Table 1 shows such a reuse pattern, where the output tile 1 appears at the same PE over time.

对比起来，时域复用意味着，我们随着时间来计算部分和，在相同的位置累加。这种复用的类型需要一个缓冲区，因为中间结果需要存储，在未来再次读取，意味着对一个缓冲区的多次读-修改-写操作。表1中的例子展示这样一种复用模式，其中输出块1在同样的PE在不同的时间出现。

To identify the reuse opportunities in arbitrary mappings, we need a precise representation of mapping and systematically infer data reuse from the description. For those two goals, we present a data-centric representation of mapping, which is concise and compiler friendly.

为在任意的映射中识别复用机会，我们需要一个确切的映射表示，系统的从描述中推断数据复用。对这两个目标，我们提出映射的以数据为中心的表示，非常简洁，而且是编译器友好的。

## 3. Describing Mappings

We use a CONV1D operation described in Figure 2 as an example operation to introduce our mapping description. As described in Figure 2, CONV1D operation can be understood as a sliding window operation of a filter vector on a input vector, where individual multiplication results within a filter window are accumulated to generated one output value in the output vector. When we project the loop indices in the loop nest in Figure 2(a), we obtain computation space in Figure 2(b) where loop indices are on each axis, and partial sums are projected in the plane. We also construct data space of each vector as shown in Figure 2(b), where the corresponding data index is on the axis. Note that the data index is not the same as the loop index (e.g., the input data index x_ is computed using loop indices x'+s). Therefore, we denote data indices using underlined index in this example. Note that output and filter indices x_' and s_ are identical to the loop indices x' and s in this simple example operation.

我们使用图2中描述的CONV1D运算作为一个例子，介绍我们的映射描述。如图2所示，CONV1D运算可以理解为，滤波器向量在输入向量上的滑窗运算，在一个滤波器窗口中的相乘运算的结果累加起来，生成输出向量中的一个输出值。当我们将图2a中的循环嵌套中的循环索引投影，就得到图2b中的计算空间，其中循环索引在每个轴上，部分和投影到平面上。我们还构建了每个向量的数据空间，如图2b所示，其中对应的数据索引在轴上。注意，数据索引与循环索引不是完全相同的（如，输入数据索引x_是用使用循环索引x'+s计算的）。因此，我们在例子中使用下划线索引来表示数据索引。注意，在这个简单的例子中，输出和滤波器索引x_'和s_与循环索引x'和s是一样的。

We show an example of mapping on three-PE accelerator in computation and data space in Figure 2(b). In this example mapping, we map three partial sum computation to each PE, and each PE collaboratively compute partial outputs (accumulated partial sums) on the same set of outputs. When the PE array finishes computation in a tile (time=0 in the example), the PE array receives the next computation tile (time=1 in the example). The next computation tile is in the direction of loop index x'. We project the same mapping on the data space as shown in Figure 2, using the array subscripts in the loop nest of CONV1D operation in Figure 2(a). That is, partial sum at (x', s) requires weight at s, input at x'+s, and output at x', as shown in the loop body of in Figure 2(a). In the example, we observe that the data space explicitly shows data reuse behavior; mapped filter values do not move over time, which implies that the example mapping is based on a weight-stationary style dataflow. This implies that inferring data reuse can be significantly simplified when we describe mapping in the data space, which can facilitate a fast analysis framework of DNN accelerator’s efficiency.

我们在图2中，在计算空间和数据空间中展示一个映射的例子，加速器有3个PE。在这个例子映射中，我们将3个部分和计算映射到每个PE中，在相同的输出集合上，每个PE合作计算部分输出（累加部分和）。当PE阵列结束在一个块中的计算（例子中是t=0），PE阵列接受下一个计算块（例子中是t=1）。下一个计算块是在循环索引x'的方向上。我们将同样的映射投影到数据空间上，如图2所示，使用的图2a中CONV1D运算的循环嵌套的阵列下标。即，在(x', s)上的部分和，需要在s处的权重，在x'+s处的输入和在x'处的输出，如图2a的循环体所示。在例子中，我们观察到，数据空间显式的展示了数据复用的行为；映射的滤波器值不会随着时间运动，这意味着例子映射是基于权重静止类型的数据流。这意味着，当我们在数据空间中描述映射时，推理数据复用可以得到显著的简化，这可以帮助DNN加速器的快速分析框架的效率。

Motivated by the observation, we introduce data-centric mapping directives that directly describe the mapping in data space.

基于以上观察，我们提出以数据为中心的映射指示符，直接描述数据空间中的映射。

### 3.1. Data-centric mapping directives

We introduce three data-centric mapping directives in Figure 3(a). Temporal and spatial map directives describe data mapping that changes in time and space (PEs), respectively. That is, temporal map corresponds to a normal for loop in loop nest while spatial map corresponds to a parallel for loop. Those two mapping directives take three parameters: Mapping size, offset, and dimension. The mapping size specifies the number of data points (in tensors, mapping size in the target dimension since a mapping constructs a high-dimensional volume) mapped on each PE. The offset describes how the mapping is updated over time on temporal map and space on spatial map. Cluster directive specifies the hierarchical organization of PEs, which enables us to explore multiple parallel dimensions in a mapping.

我们提出三个数据中心的映射指示符，如图3所示。时域和空域映射指示符，描述的分别是数据映射，随着时间和空间(PEs)而变化。即，时域映射对应循环嵌套中一个正常的for循环，而空域映射对应着一个并行的for循环。这两个映射指示符有3个参数：映射大小，偏移和维度。映射大小指定了映射到每个PE上数据点的数量（以张量为单位，在目标维度中的映射大小，因为一个映射构建了一个高维体）。偏移描述了映射随着空域映射中的时间和空域映射中的空间的更新情况。集群的指示符指定了PEs的层次化组织，使我们可以在一个映射中探索多个并行维度。

To understand the syntax and semantics of data-centric mapping directives, in Figure 3, we provide an example process to determine a corresponding data mapping description of the example mapping in Figure 2(b). We omit the input tensor because input tensor data mapping can be easily inferred from the mapping of output and filter. We first determine if the mapping is in time or space by checking the mapped data are the same or different (i.e., parallelization) across PEs. Next, we check the number of data points mapped on each PE to determine the mapping size, which are three and one for output and filter, respectively, in the example. To determine the offset parameter, we check the temporal and spatial offset on temporal and spatial map, respectively. For example, for output vector in Figure 3(b), we observe that the starting index of mapping changes over time 3, which implies that the temporal offset is 3. For filter vector, we observe that the starting index of mapping for each PE changes by 1, which implies that the spatial offset is 1. Note that spatial map can also involve temporal aspect as the mapping on the filter vector in Figure 3(b); after processing all the computation that involves the first data tile on filter, the data tile will move on to the next position. This happens when the number of PEs is not sufficient to cover entire spatially mapped dimension (also known as spatial folding), and an implicit temporal offset of (spatial offset) × (number of PEs) is applied. Finally, we write the dimension on which we describe the data mapping, then we obtain the data-centric mapping description of each data mapping, as shown in the resulting data mapping description column in Figure 3(b). To specify the entire example mapping, we need to specify the order of changes in data tile between output and filter vectors. Since filter is updated in a slower manner, we place the data mapping description of filter above, and write that of output below, like we specify the update order in loop nest (outer-most loop index changes slower).

为理解以数据为中心的映射指示符的语法和语义，在图3中，我们给出了确定图2b中的例子映射的对应的数据映射描述的例子过程。我们忽略了输入张量，因为输入张量数据映射可以很容易的从输出和滤波器映射中推断出来。我们首先通过检查，映射的数据在不同的PEs中是相同的还是不同的（如，并行），来确定映射是在时间上的，还是空间上的。下一步，我们检查映射到每个PE上的数据点的数量，来决定映射大小，在例子中，对于输出和滤波器分别是3和1。为确定偏移参数，我们分别在时域和空域图中检查时域和空域偏移。比如，对于图3b中的输出向量，我们观察到映射的起始索引以3的倍数变化，这意味着时域偏移是3。对滤波器向量，我们观察到，对每个PE的映射的起始索引变化量为1，这意味着空域偏移为1。注意，空域图也可能涉及到时域方面，就像图3b中滤波器向量的映射；在处理所有涉及到滤波器上第一个数据块的计算后，数据块就移动到下一个位置上。这在PEs的数量不足以覆盖整个空域映射维度时，就会发生，使用一个隐式的时域偏移空域偏移×PEs数量。最后，我们写入我们描述数据映射的维度，然后我们得到每个数据映射的数据中心映射描述，如图3b中得到的数据映射描述列中所示。为指定整个映射例子，我们需要指定输出和滤波器向量之间的数据块的变化顺序。由滤波器的更新是更慢的，我们将滤波器的数据映射描述放在上面，将输出的放在下面，就像我们指定循环嵌套中的更新顺序一样（最外面的循环索引变化的最慢）。

### 3.2. Capability of Mapping Directives

Using the data-centric directives, we can describe a variety of mappings if it maps consecutive data points in a regular manner (i.e., affine loop subscripts when described in a loop nest representation). Figure 3(c) shows the capability of the data-centric directive by showing the changes in the resulting mapping when we update the base representation we obtained in Figure 3(b). When we change the directive order, we describe a different order of data tile update in dimensions. This effectively changes the stationary vector from weight to output, which changes the temporal data reuse opportunities. When we change the spatial dimension, then we exploit the parallelism in a different dimension, as the third example in Figure 3(b) and (c) shows. Finally, if we change the mapping size (we accordingly update the offset to keep the description legal), we change the amount of mapped filter and output, as shown in Figure 3(c) and (d).

如果将连续的数据点以一种规则的方式映射的话，使用以数据为中心的指示符，我们可以描述很多映射（即，当在循环嵌套表示描述时，是仿射循环下标的）。图3c展示了数据中心指示符的能力，展示了当我们更新在图3b中得到的基础表示时，得到的映射的变化。当我们改变指示符的顺序时，我们描述了不同的数据块更新维度。这将静态向量从权重改变到了输出，改变了时域数据复用。当我们改变了空域维度，我们在不同的维度利用并行性，如图3b和图3c的三个例子所示。最后，如果我们改变映射大小（我们相应的更新偏移，以使描述是合法的），我们改变了映射的滤波器和输出的数量，如图3c和3d所示。

Based on the fact that data reuse is explicit in data dimension and the capability of data-centric directives, we implement an analytical cost-benefit analysis framework for DNN accelerators, MAESTRO. We discuss a high-level overview of MAESTRO next and discuss insights from the case studies we performed based on MAESTRO next.

数据复用在数据维度上是显式的，基于这个事实，和数据中心指示符的能力，我们对DNN加速器实现了一个解析的代价收益分析框架，MAESTRO。我们下面讨论MAESTRO的高层概览，基于MAESTRO进行了一些案例分析，并讨论了洞见。

## 4. Analytical Cost Model

Based on the data-centric directives we discussed, we built a cost-benefit analysis framework that considers all of the three factors—DNN layers, hardware, and mapping—with precise modeling of data reuse. MAESTRO consists of five preliminary engines: Tensor, cluster, reuse, performance analysis, and cost analysis. In the article, we focus on the high-level idea without details such as edge case handling, multiple layers, and multiple level hierarchy, etc. We present implementation details in our web page and open-source repository. We validated MAESTRO’s performance model against RTL simulation and reported processing delay of two accelerators—MAERI and Eyeriss when running VGG16 and AlexNet, respectively. The latency estimated by MAESTRO are within 3.9% absolute error of the cycle-accurate RTL simulation and reported processing delay on average.

基于我们讨论的数据中心指示符，我们构建了一个代价收益分析框架，考虑了所有三个因素，DNN层，硬件和映射，和数据复用的精确建模。MAESTRO由5个基本引擎组成：张量，集群，复用，性能分析和代价分析。在本文中，我们聚焦高层思想，没有细节，比如处理边缘情况，多层，多层层次结构，等。我们在我们的网页中给出实现细节。我们与RTL仿真验证了MAESTRO的性能模型，给出了两个加速器(Eyeriss and MAERI)在运行VGG16和AlexNet时的处理延迟。MAESTRO估计的延迟，在周期准确的RTL仿真和给出的处理延迟的平均的3.9%误差内。

## 5. Case Studies

With MAESTRO, we perform deeper case studies about the costs-benefit tradeoff of various mappings when applied to different DNN operations. We evaluate five distinct mapping styles listed in Figure 4(a) in the “Case Study I: The Impact of Mapping Choices” section and the preference of each mapping to different DNN operators. For energy estimation, we multiply activity counts with base energy values from Cacti simulation (28 nm, 2 kB L1 scratchpad, and 1 MB shared L2 buffer). We also present distinct design space of an early layer (wide and shallow) and a late layer (narrow and deep) to show the dramatically different hardware preference of different DNN layers and mapping in the “Case Study II: Hardware Design-Parameters and Implementation Analysis” section.

用MAESTRO，我们对各种映射应用到不同的DNN运算上的代价-收益折中，进行了更深入的案例研究。我们评估了5种不同的映射类型，如图4a所示，在下一节讨论，还评估了每种映射对不同的DNN算子的倾向。对于能耗估计，我们将行为数与Cacti仿真的基础能量值(28nm, 2kB L1便签内存，1MB共享L2缓冲区)相乘。我们还在下面第二节中，给出了早期层（宽且浅）和晚期层（窄且深）的不同的设计空间，展示了不同的DNN层和映射对不同的硬件的倾向性。

### 5.1. Case Study I: The Impact of Mapping Choices

Figure 4(b) shows the DNN-operator granularity estimation of latency and energy of each mapping across five state-of-the-art DNN models listed in the “Case Studies” section. Note that this should be considered a comparison of mapping—not of actual designs, which can contain several low-level implementation differences, e.g., custom implementations of logic/memory blocks, process technology, etc. We observe that KC-P style mapping provides overall low latency and energy. However, the energy efficiency in VGG16 is worse than YR-P (Eyeriss style) mapping, and the latency is worse than YX-P (Shidiannao14 style) mapping in UNet. This is based on the different preference toward mapping of each DNN operator. YX-P provides short latency to segmentation networks like UNet, which has wide activation (e.g., 572 × 572 in the input layer) and recovers the original activation dimension at the end via up-scale convolution (e.g., transposed convolutions). Such a preference to the YX-P style is mainly based on its parallelization strategy: It exploits parallelism over both of row and column dimensions in activation. The energy efficiency of YR-P mapping in VGG16 is based on its high reuse factor (the number of local accesses per fetch) in early layers. The YR-P mapping has 5.8× and 15.17× higher activation and filter reuse factors, respectively, in early layers. However, in late layers, the reuse factors of YR-P and KC-P mapping are almost similar (difference < 11%), so the KC-P mapping provides similar energy efficiency as YR-P in these cases. This can also be observed in the late layer (blue) bars in Figure 4(b) bottom-row plots.

图4b展示了DNN算子颗在每种映射下对延迟和功耗，在5种目前最好的DNN模型的粒度估计。注意，这应当被考虑为映射的比较，并不是实际的设计，因为实际的设计可能会包含几种底层实现的差异，比如，逻辑/存储模块的定制设计，工艺技术，等。我们观察到，KC-P类型的映射给出了总体最低的延迟和功耗。但是，在VGG16中其功耗要比YR-P映射(Eyeriss style)要差，在UNet中延迟要比YX-P映射(Shidiannao style)要差。这是基于每个DNN算子对映射的不同倾向性的。YX-P对分割网络如U-Net可以得到短延迟，因为UNet的激活很宽（如，输入层为572 × 572），在最后会通过上采样恢复到原始激活维度（如，转置卷积）。对YX-P类型的倾向性主要是因为其并行策略：利用了激活中行和列维度的并行性。YR-P映射在VGG16中的功耗效率是基于其在早期层中很高的复用因子（每个fetch的局部访问数很高）。YR-P映射在早期层中，对激活和滤波器复用因子分别要高5.8x和15.17x。但是，在后期层中，YR-P和KC-P映射的复用因子几乎是一样的（差异小于11%），所以KC-P映射与YR-P映射在这些情况中给出了类似的功耗效率。这在图4b最下面一行的图中的后期层蓝色柱中也可以观察到。

The diverse preference to mappings of different DNN operators motivates us to employ optimal mapping for each DNN operator type. We refer such an approach as adaptive mapping and present the benefits in the right-most column of Figure 4(b), the average case analysis across entire models in the DNN operator granularity. By employing the adaptive approach, we could observe a potential 37% latency and 10% energy reduction. Such an optimization opportunity can be exploited by flexible accelerators like Flexflow and MAERI or via heterogeneous accelerators that employ multiple subaccelerators with various mapping styles in a single DNN accelerator chip.

不同的DNN算子对映射不同的倾向性，促使我们对每种DNN算子类型采用最优的映射。我们称这种方法为自适应映射，在图4b最右列给出其收益。利用自适应方法，我们可以观察到延迟和功耗降低了37%和10%。这样一种优化可以用到灵活的加速器中，如Flexflow和MAERI中，或通过异质加速器，在单个DNN加速器芯片中，采用多个子加速器，有各种不同的映射类型。

### 5.2. Case Study II: Hardware Design-Parameters and Implementation Analysis

Using MAESTRO, we implement a hardware design space exploration (DSE) tool that searches four hardware parameters (the number of PEs, L1 buffer size, L2 buffer size, and NoC bandwidth) optimized for either energy efficiency, throughput, or energy-delay-product (EDP) within given hardware area and power constraints. The DSE tool receives the same set of inputs as MAESTRO with hardware area/power constraints and the area/power of building blocks synthesized with the target technology. For the cost of building blocks, we implement float/fixed point multiplier and adder, bus, bus arbiter, and global/local scratchpad in RTL and synthesis them using 28-nm technology. For bus and arbiter cost, we fit the costs into a linear and quadratic model using regression because bus cost increases linearly and arbiter cost increases quadratically (e.g., matrix arbiter).

使用MAESTRO，我们实现了一种硬件DSE工具，搜索四种硬件参数（PEs的数量，L1 buffer大小，L2 buffer大小，和NoC带宽），对功耗效率，吞吐量，或EDP进行优化，并给定硬件面积和功耗约束。DSE工具接收的输入与MAESTRO一样，并带有面积/功耗约束，和在目标工艺下综合得到的组成部分的面积/功耗。对于组成部分的代价，我们用RTL实现了浮点/定点乘法器和加法器，总线，总线仲裁器，和全局/局部便签内存，使用28nm工艺对其进行了综合。对于总线和仲裁器的代价，我们将将其代价用回归拟合为线性和平方模型，因为总线代价线性增长，仲裁器代价平方增长（如，矩阵仲裁器）。

Using the DSE tool, we explore the design space of KC-P and YR-P mapping accelerators. We set the area and power constraint as 16 mm2 and 450 mW, which is the reported chip area and power of Eyeriss. We plot the entire design space we explored in Figure 4(c). Whether an accelerator can achieve peak throughput depends on not only the number of PEs but also NoC bandwidth. In particular, although an accelerator has sufficient number of PEs to exploit the maximum degree of parallelism a mapping allows, if the NoC does not provide sufficient bandwidth, the accelerator suffers a communication bottleneck in the NoC. Such design points can be observed in the area-throughput plot in Figure 4(c). YR-P mapping requires low NoC bandwidth so it does not show the same behavior as KC-P mapping. However, with more stringent area and power constraints, YR-P mapping will show the same behavior.

使用DSE工具，我们探索了KC-P和YR-P映射加速器的设计空间，我们设面积和功耗约束为16mm2和450mW，这是Eyeriss的芯片面积和功耗。我们在图4c中画出了探索的整个设计空间。加速器是否可以取得峰值吞吐量，不仅取决于PEs的数量，而且取决于NoC带宽。特别是，虽然加速器有充足的PEs数量，可以利用映射所允许的最大程度的并行性，如果NoC不给出足够的带宽，加速器会在NoC处收到通信瓶颈限制。这样的设计点可以在图4c中的面积-吞吐量图中观察到。YR-P映射需要低NoC带宽，所以其与KC-P映射的行为并不相同。但是，有了更严格的面积和功耗约束后，YR-P映射会展现出相同的行为。

During DSE runs, MAESTRO reports buffer requirements for each mapping and the DSE tool places the exact amount buffers MAESTRO reported. Contrary to intuition, larger buffer sizes do not always provide high throughput, as shown in buffer-throughput plots in Figure 4 (plots in the second column). The optimal points regarding the throughput per buffer size are in the top-left region of the buffer-throughput plots. The existence of such points indicates that the tiling strategy of the mapping (mapping sizes in our directive representation) significantly affects the efficiency of buffer use. We observe that the throughput-optimized designs have a moderate number of PEs and buffer sizes, implying that hardware resources need to be distributed not only to PEs but also to NoC and buffers for high PE utilization. Likewise, we observe that the buffer amount does not directly increase throughput and energy efficiency. These results imply that all the components are intertwined, and they need to be well-balanced to obtain a highly efficient accelerator.

在DSE运行过程中，MAESTRO给出每个映射的buffer需求，DSE工具给出的buffer数量，与MAESTRO给出的一样。与直觉不同的是，更大的buffer大小并不会一直给出高吞吐量，如图4c中的buffer-吞吐量图所示。单位buffer大小的吞吐量的最优点，是在buffer-吞吐量图的上左区域。这样点的存在，说明映射的tiling策略显著影响了buffer使用的效率。我们观察到，吞吐量优化的设计，其PEs数量和buffer大小都比较中等，说明要得到高PE利用率，需要分布的硬件资源不仅对PEs，而且还包括NoC和buffers。类似的，我们观察到buffer数量也不会直接增加吞吐量和功耗效率。这些结果说明，所有的组成部分是纠缠在一起的，需要很好的均衡，以得到高效的加速器。

We also observe the impact of hardware support for each data reuse type, discussed in Table 1. Figure 4(d) shows such design points found in the design space of KC-P mapping on VGG16-conv2 layer presented in the first row of Figure 4(c). The reference design point is the throughput-optimized design represented as a star in the first row of Figure 4(c). When bandwidth gets smaller, the throughput significantly drops, but energy remains similar. However, the lack of spatial multicast or reduction support resulted in approximately 47% energy increase, as the third and fourth design points shows.

我们还观察了，对每种数据复用类型，硬件支持的影响，如表I所示。图4d展示了在KC-P映射下在VGG16-conv2层的设计空间中找到的设计点。参考设计点是图4c中第一行中对吞吐量优化的设计，表示为一个星号。当带宽变小，吞吐量显著下降，但功耗保持类似。但是，缺少空域multicast或缩减支持，功耗大约增加了47%，如第3和第4个设计点所示。

## 6. Conclusion

Fast modeling of cost-benefit space of DNN accelerators is critical for automated optimization tools since the design space is huge and high dimensional based on hundreds of DNN model, hardware, and mapping parameters. In this article, we presented a methodology to enable fast cost-benefit estimation of a DNN accelerator on a given DNN model and mapping, which consists of a compiler-friendly data-centric representation of mappings and an analytical cost-benefit estimation framework that exploits the explicit data reuse in data space in data-centric representations. To analytically estimate the costs and benefits, we demystify data reuse in hardware and required hardware support and apply the observation into the analytical cost-benefit estimation framework, MAESTRO.

DNN加速器的代价-收益空间的快速建模，对自动优化工具非常关键，因为设计空间是巨大的，高维的，其中DNN模型，硬件和映射参数有几百个。在本文中，我们给出了一种方法论，可以快速估计DNN加速器在给定DNN模型和映射下的代价-收益，由一个编译器友好的数据中心映射表示，和一个解析的代价-收益估计框架组成，以数据中心表示利用了显式的数据空间的数据复用。为解析的估计代价和收益，我们弄清楚了硬件中的数据复用，和需要的硬件支持，将这些观察应用到解析代价-收益估计框架MAESTRO中。

Using MAESTRO, we show that no single mapping and no single hardware is ideal for all the DNN layers, which implies the complexity of the DNN accelerator design space. Using hardware design space exploration framework we implemented using MAESTRO, we also show that hardware features can significantly impact the throughput and energy. Those cases show that the capability of MAESTRO for various analysis problems on DNN accelerator design space. In addition to the case studies we performed, MAESTRO also facilitates many other optimization (e.g., neural architecture search specialized for a target accelerator, mapping search for a target accelerator, etc.) frameworks based on its speed and accuracy, which will lead to broad impact on various areas (DNN model design, compiler, architecture, etc.) in the DNN accelerator domain.

使用MAESTRO，我们证明了，没有对所有DNN层都理想的单个映射和一种硬件，这说明DNN加速器设计空间的复杂性。使用我们用MAESTRO实现的硬件DSE框架，我们还证明了，硬件特征会显著影响吞吐量和功耗。这些案例表明，MAESTRO可以在DNN加速器设计空间进行各种问题分析。除了我们进行的各种案例研究，MAESTRO还促进了很多其他优化框架（如，专用于目标加速器的神经架构搜索，对目标加速器的映射搜索，等），基于其速度和准确率，会带来对DNN加速器领域其他领域的广泛影响（DNN模型设计，编译器，架构等）。