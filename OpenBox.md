# OpenBox: A Generalized Black-box Optimization Service

Yang Li, et. al. @ Peking University

## 0. Abstract

Black-box optimization (BBO) has a broad range of applications, including automatic machine learning, engineering, physics, and experimental design. However, it remains a challenge for users to apply BBO methods to their problems at hand with existing software packages, in terms of applicability, performance, and efficiency. In this paper, we build OpenBox, an open-source and general-purpose BBO service with improved usability. The modular design behind OpenBox also facilitates flexible abstraction and optimization of basic BBO components that are common in other existing systems. OpenBox is distributed, fault-tolerant, and scalable. To improve efficiency, OpenBox further utilizes â€œalgorithm agnosticâ€ parallelization and transfer learning. Our experimental results demonstrate the effectiveness and efficiency of OpenBox compared to existing systems.

é»‘ç®±ä¼˜åŒ–(BBO)åº”ç”¨éå¸¸å¹¿æ³›ï¼ŒåŒ…æ‹¬è‡ªåŠ¨æœºå™¨å­¦ä¹ ï¼Œå·¥ç¨‹ï¼Œç‰©ç†å’Œè¯•éªŒè®¾è®¡ã€‚ä½†æ˜¯ï¼Œç”¨æˆ·ç”¨ç°æœ‰çš„è½¯ä»¶åŒ…ï¼Œå°†BBOåº”ç”¨åˆ°å…¶æ‰‹å¤´ä¸Šçš„é—®é¢˜ä¸­ï¼Œåœ¨åº”ç”¨æ€§ï¼Œæ€§èƒ½ï¼Œå’Œæ•ˆç‡ä¸Šï¼Œè¿™ä»ç„¶æ˜¯ä¸€ä¸ªé—®é¢˜ã€‚æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ„å»ºäº†OpenBoxï¼Œä¸€ä¸ªå¼€æºï¼Œé€šç”¨ç›®æ ‡çš„BBOæœåŠ¡ï¼Œå¯ç”¨æ€§å¾—åˆ°äº†æ”¹è¿›ã€‚OpenBoxçš„æ¨¡å—åŒ–è®¾è®¡ä¹Ÿä½¿å¾—åœ¨å…¶ä»–å·²æœ‰çš„ç³»ç»Ÿä¸­é€šç”¨çš„åŸºæœ¬BBOç»„æˆéƒ¨åˆ†çš„çµæ´»çš„æŠ½è±¡å’Œä¼˜åŒ–éå¸¸æ–¹ä¾¿ã€‚OpenBoxæ˜¯åˆ†å¸ƒå¼çš„ï¼Œå¯å®¹å¿é”™è¯¯çš„ï¼Œå¯æ‰©å±•çš„ã€‚ä¸ºæ”¹è¿›æ•ˆç‡ï¼ŒOpenBoxè¿›ä¸€æ­¥åˆ©ç”¨äº†ä¸ç®—æ³•æ— å…³çš„å¹¶è¡ŒåŒ–å’Œè¿ç§»å­¦ä¹ ã€‚æˆ‘ä»¬çš„è¯•éªŒç»“æœè¡¨æ˜äº†OpenBoxä¸å…¶ä»–ç³»ç»Ÿç›¸æ¯”ï¼Œæ˜¯æœ‰æ•ˆçš„ï¼Œè€Œä¸”æ˜¯é«˜æ•ˆçš„ã€‚

**Keywords** Bayesian Optimization, Black-box Optimization

## 1. Introduction

Blackâ€“box optimization (BBO) is the task of optimizing an objective function within a limited budget for function evaluations. â€œBlack-boxâ€ means that the objective function has no analytical form so that information such as the derivative of the objective function is unavailable. Since the evaluation of objective functions is often expensive, the goal of black-box optimization is to find a configuration that approaches the global optimum as rapidly as possible.

é»‘ç®±ä¼˜åŒ–(BBO)æ˜¯åœ¨æœ‰é™çš„å‡½æ•°è¯„ä¼°çš„é¢„ç®—ä¸‹ï¼Œä¼˜åŒ–ä¸€ä¸ªç›®æ ‡å‡½æ•°çš„ä»»åŠ¡ã€‚é»‘ç®±çš„æ„æ€æ˜¯ï¼Œç›®æ ‡å‡½æ•°å¹¶æ²¡æœ‰è§£æçš„å½¢å¼ï¼Œæ‰€ä»¥ç›®æ ‡å‡½æ•°çš„æ¢¯åº¦è¿™æ ·çš„ä¿¡æ¯æ˜¯ä¸å¯ç”¨çš„ã€‚å› ä¸ºç›®æ ‡å‡½æ•°çš„è¯„ä¼°é€šå¸¸æ˜¯æ˜‚è´µçš„ï¼Œé»‘ç›’ä¼˜åŒ–çš„ç›®æ ‡æ˜¯ï¼Œæ‰¾åˆ°ä¸€ç§é…ç½®ï¼Œå¯ä»¥å°½å¿«çš„æ‰¾åˆ°å…¨å±€æœ€ä¼˜å€¼ã€‚

Traditional BBO with a single objective has many applications: 1) automatic A/B testing, 2) experimental design [15], 3) knobs tuning in database [46, 48, 49], and 4) automatic hyper-parameter tuning [6, 27, 32, 44], one of the most indispensable components in AutoML systems [1, 34] such as Microsoftâ€™s Azure Machine Learning, Googleâ€™s Cloud Machine Learning, Amazon Machine Learning [35], and IBMâ€™s Watson Studio AutoAI, where the task is to minimize the validation error of a machine learning algorithm as a function of its hyper-parameters. Recently, generalized BBO emerges and has been applied to many areas such as 1) processor architecture and circuit design [2], 2) resource allocation [18], and 3) automatic chemical design [22], which requires more general functionalities that may not be supported by traditional BBO, such as multiple objectives and constraints. As examples of applications of generalized BBO in the software industry, Microsoftâ€™s Smart Buildings project [36] searches for the best smart building designs by minimizing both energy consumption and construction costs (i.e., BBO with multiple objectives); Amazon Web Service aims to optimize the performance of machine learning models while enforcing fairness constraints [39] (i.e., BBO with constraints).

ä¼ ç»Ÿçš„å•ç›®æ ‡BBOæœ‰å¾ˆå¤šåº”ç”¨ï¼š1)è‡ªåŠ¨A/Bæµ‹è¯•ï¼Œ2)è¯•éªŒè®¾è®¡ï¼Œ3)æ•°æ®åº“ä¸­çš„knobsè°ƒèŠ‚ï¼Œ4)è‡ªåŠ¨è¶…å‚æ•°è°ƒèŠ‚ï¼Œåœ¨ä¸€äº›AutoMLç³»ç»Ÿä¸­ï¼Œå¦‚å¾®è½¯çš„Azureæœºå™¨å­¦ä¹ ç³»ç»Ÿï¼ŒGoogleäº‘æœºå™¨å­¦ä¹ ç³»ç»Ÿï¼ŒAmazonçš„æœºå™¨å­¦ä¹ ç³»ç»Ÿï¼Œå’ŒIBMçš„Watson Studio AutoAIç³»ç»Ÿï¼Œè¿™æ˜¯æœ€ä¸å¯æˆ–ç¼ºçš„ä¸€éƒ¨åˆ†ï¼Œå…¶ä»»åŠ¡æ˜¯æœ€å°åŒ–æœºå™¨å­¦ä¹ ç®—æ³•ä½œä¸ºå…¶è¶…å‚æ•°çš„å‡½æ•°çš„éªŒè¯è¯¯å·®ã€‚æœ€è¿‘ï¼Œé€šç”¨BBOå‡ºç°äº†ï¼Œåº”ç”¨åˆ°äº†å¾ˆå¤šé¢†åŸŸï¼Œæ¯”å¦‚ï¼Œ1)å¤„ç†å™¨æ¶æ„å’Œç”µè·¯è®¾è®¡ï¼Œ2)èµ„æºåˆ†é…ï¼Œ3)è‡ªåŠ¨åŒ–å­¦ç‰©è´¨è®¾è®¡ï¼Œè¿™éœ€è¦æ›´é€šç”¨çš„åŠŸèƒ½ï¼Œä¼ ç»Ÿçš„BBOå¯èƒ½å¹¶ä¸æ”¯æŒï¼Œæ¯”å¦‚å¤šç›®æ ‡å’Œçº¦æŸã€‚é€šç”¨BBOåœ¨è½¯ä»¶äº§ä¸šä¸­çš„åº”ç”¨çš„ä¸€ä¸ªä¾‹å­æ˜¯ï¼Œå¾®è½¯çš„æ™ºæ…§å»ºç­‘é¡¹ç›®ï¼Œå…¶æœç´¢æœ€æ™ºæ…§çš„å»ºç­‘è®¾è®¡çš„æ–¹æ³•æ˜¯ï¼Œæœ€å°åŒ–èƒ½è€—å’Œå»ºç­‘ä»·æ ¼ï¼ˆå³ï¼Œå¤šç›®æ ‡çš„BBOï¼‰ï¼›Amazonçš„ç½‘é¡µæœåŠ¡çš„ç›®æ ‡æ˜¯ï¼Œä¼˜åŒ–æœºå™¨å­¦ä¹ æ¨¡å‹çš„æ€§èƒ½ï¼ŒåŒæ—¶åŠ å…¥ä¸€äº›åˆç†çš„çº¦æŸï¼ˆå³ï¼Œå¸¦æœ‰çº¦æŸçš„BBOï¼‰ã€‚

Many software packages and platforms have been developed for traditional BBO (see Table 1). Yet, to the best of our knowledge, so far there is no platform that is designed to target generalized BBO. The existing BBO packages have the following three limitations when applied to general BBO scenarios:

å¯¹ä¼ ç»ŸBBOï¼Œå·²ç»å¼€å‘äº†å¾ˆå¤šè½¯ä»¶åŒ…å’Œå¹³å°ï¼ˆè§è¡¨1ï¼‰ã€‚ä½†æ˜¯ï¼Œæ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œç›®å‰å¹¶æ²¡æœ‰ä¸ºé€šç”¨BBOè®¾è®¡ä»»ä½•å¹³å°ã€‚ç°æœ‰çš„BBOåŒ…åœ¨åº”ç”¨åˆ°é€šç”¨BBOåœºæ™¯æ—¶ï¼Œæœ‰ä¸‰ä¸ªå±€é™ï¼š

(1) Restricted scope and applicability. Restricted by the underlying algorithms, most existing BBO implementations cannot handle diverse optimization problems in a unified manner (see Table 1). For example, Hyperopt [6], SMAC3 [27], and HpBandSter [13] can only deal with single-objective problems without constraints. Though BoTorch [3] and GPflowOpt [30] can be used, as a framework, for developers to implement new optimization problems with multi-objectives and constraints; nevertheless, their current off-the-shelf supports are also limited (e.g., the support for non-continuous parameters).

èŒƒå›´å’Œå¯åº”ç”¨æ€§æœ‰é™ã€‚å—åˆ°æ½œåœ¨çš„ç®—æ³•çš„é™åˆ¶ï¼Œå¤šæ•°ç°æœ‰BBOçš„å®ç°ä¸èƒ½ä»¥ç»Ÿä¸€çš„æ–¹å¼å¤„ç†å„ç§ä¼˜åŒ–é—®é¢˜ã€‚æ¯”å¦‚ï¼ŒHyperoptï¼ŒSMAC3å’ŒHpBandSteråªèƒ½å¤„ç†æ²¡æœ‰çº¦æŸçš„å•ç›®æ ‡ä¼˜åŒ–é—®é¢˜ã€‚è™½ç„¶å¯ä»¥å°†BoTorchå’ŒGPflowOptç”¨ä½œä¸€ä¸ªæ¡†æ¶ï¼Œå¼€å‘è€…å¯ä»¥å®ç°æ–°çš„å¸¦æœ‰å¤šç›®æ ‡å’Œçº¦æŸçš„ä¼˜åŒ–ç®—æ³•ï¼›å°½ç®¡å¦‚æ­¤ï¼Œä»–ä»¬å½“å‰å¼€ç®±å³ç”¨çš„æ”¯æŒä¹Ÿæ˜¯æœ‰é™çš„ï¼ˆå¦‚ï¼Œæ”¯æŒéè¿ç»­å‚æ•°ï¼‰ã€‚

(2) Unstable performance across problems. Most existing software packages only implement one or very few BBO algorithms. According to the â€œno free lunchâ€ theorem [26], no single algorithm can achieve the best performance for all BBO problems. Therefore, existing packages would inevitably suffer from unstable performance when applied to different problems. Figure 1 presents a brief example of hyper-parameter tuning across 25 AutoML tasks, where for each problem we rank the packages according to their performances. We can observe that all packages exhibit unstable performance, and no one consistently outperforms the others. This poses challenges on practitioners to select the best package for a specific problem, which usually requires deep domain knowledge/- expertise and is typically very time-consuming.

åœ¨ä¸åŒé—®é¢˜ä¸­çš„æ€§èƒ½ä¸ç¨³å®šã€‚å¤šæ•°ç°æœ‰çš„è½¯ä»¶åŒ…åªå®ç°äº†ä¸€ä¸ªæˆ–å¾ˆå°‘å‡ ä¸ªBBOç®—æ³•ã€‚æ ¹æ®æ²¡æœ‰å…è´¹åˆé¤çš„å®šå¾‹ï¼Œæ²¡æœ‰é‚£ä¸ªå•ç‹¬çš„ç®—æ³•èƒ½å¤Ÿåœ¨æ‰€æœ‰BBOé—®é¢˜ä¸­è·å¾—æœ€å¥½çš„æ€§èƒ½ã€‚å› æ­¤ï¼Œç°æœ‰çš„åŒ…åœ¨åº”ç”¨åˆ°ä¸åŒçš„é—®é¢˜æ—¶ï¼Œä¼šä¸å¯é¿å…çš„é‡åˆ°ä¸ç¨³å®šçš„æ€§èƒ½ã€‚å›¾1ç»™å‡ºäº†è¶…å‚æ•°è°ƒèŠ‚åœ¨25ä¸ªAutoMLä»»åŠ¡ä¸­çš„ç®€å•ä¾‹å­ï¼Œå¯¹æ¯ä¸ªé—®é¢˜æˆ‘ä»¬æ ¹æ®æ€§èƒ½æ¥å¯¹è½¯ä»¶åŒ…è¿›è¡Œæ’åºã€‚æˆ‘ä»¬å¯ä»¥è§‚å¯Ÿåˆ°ï¼Œæ‰€æœ‰çš„åŒ…éƒ½è¡¨ç°å‡ºäº†ä¸ç¨³å®šçš„æ€§èƒ½ï¼Œæ²¡æœ‰å“ªä¸ªä¼šä¸€ç›´æ¯”å…¶ä»–çš„æ›´å¥½ã€‚è¿™ä¼šä½¿å®è·µè€…åœ¨ä¸€ä¸ªå…·ä½“é—®é¢˜ä¸Šé€‰æ‹©æœ€å¥½çš„åŒ…æ—¶ï¼Œä¼šé‡åˆ°æŒ‘æˆ˜ï¼Œè¿™é€šå¸¸éœ€è¦å¾ˆæ·±çš„é¢†åŸŸçŸ¥è¯†æˆ–ä¸“ä¸šçŸ¥è¯†ï¼Œä¸€èˆ¬æ˜¯éå¸¸è€—æ—¶çš„ã€‚

(3) Limited scalability and efficiency. Most existing packages execute optimization in a sequential manner, which is inherently inefficient and unscalable. However, extending the sequential algorithm to make it parallelizable is nontrivial and requires significant engineering efforts. Moreover, most existing systems cannot support transfer learning to accelerate the optimization on a similar task.

æœ‰é™çš„å¯æ‰©å±•æ€§å’Œæ•ˆç‡ã€‚å¤šæ•°ç°æœ‰çš„åŒ…éƒ½ä»¥é¡ºåºçš„æ–¹å¼è¿›è¡Œä¼˜åŒ–ï¼Œè¿™åœ¨å†…åœ¨ä¸Šå°±ä¸æ˜¯é«˜æ•ˆçš„ï¼Œä¹Ÿæ˜¯ä¸å¯æ‰©å±•çš„ã€‚ä½†æ˜¯ï¼Œå°†é¡ºåºçš„ç®—æ³•æ‰©å±•æˆå¯å¹¶è¡Œçš„ï¼Œè¿™æœ‰å¾ˆå¤§çš„å·¥ä½œé‡ï¼Œéœ€è¦å¾ˆå¤šå·¥ç¨‹ä¸Šçš„åŠªåŠ›ã€‚è€Œä¸”ï¼Œå¤šæ•°ç°æœ‰çš„ç³»ç»Ÿä¸æ”¯æŒè¿ç§»å­¦ä¹ ï¼Œå°±ä¸èƒ½åœ¨ç›¸ä¼¼çš„ä»»åŠ¡ä¸ŠåŠ é€Ÿä¼˜åŒ–è¿‡ç¨‹ã€‚

With these challenges, in this paper we propose OpenBox, a system for generalized black-box optimization. The design of OpenBox follows the philosophy of providing â€œBBO as a serviceâ€ â€” instead of developing another software package, we opt to implement OpenBox as a distributed, fault-tolerant, scalable, and efficient service, which addresses the aforementioned challenges in a uniform manner and brings additional advantages such as ease of use, portability, and zero maintenance. In this regard, Googleâ€™s Vizier [19] is perhaps the only existing BBO service as far as we know that follows the same design philosophy. Nevertheless, Vizier only supports traditional BBO, and cannot be applied to general scenarios with multiple objectives and constraints that OpenBox aims for. Moreover, unlike Vizier, which remains Googleâ€™s internal service as of today, we have open-sourced OpenBox that is available at https://github.com/PKU-DAIR/open-box.

æœ‰äº†è¿™äº›æŒ‘æˆ˜ï¼Œæœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†OpenBoxï¼Œè¿™æ˜¯ä¸€ç§é€šç”¨é»‘ç›’ä¼˜åŒ–çš„ç³»ç»Ÿã€‚OpenBoxçš„è®¾è®¡å“²å­¦ï¼Œæ˜¯å°†BBOä½œä¸ºä¸€ç§æœåŠ¡æ¥æä¾›ï¼Œè€Œä¸æ˜¯å¼€å‘å¦ä¸€ä¸ªè½¯ä»¶åŒ…ï¼Œæˆ‘ä»¬å°†OpenBoxå®ç°ä¸ºä¸€ä¸ªåˆ†å¸ƒå¼çš„ï¼Œèƒ½å®¹é”™çš„ï¼Œå¯æ‰©å±•çš„å’Œé«˜æ•ˆçš„æœåŠ¡ï¼Œç»Ÿä¸€å¤„ç†å‰é¢æåˆ°çš„æŒ‘æˆ˜ï¼Œå¸¦æ¥äº†é¢å¤–çš„ä¼˜åŠ¿ï¼Œæ¯”å¦‚å®¹æ˜“ä½¿ç”¨ï¼Œå¯ç§»æ¤æ€§ï¼Œé›¶ç»´æŠ¤ã€‚è¿™æ–¹é¢ï¼ŒGoogleçš„Vizieræ˜¯å”¯ä¸€ç°æœ‰çš„BBOæœåŠ¡ï¼Œæœ‰ç€ç›¸åŒçš„è®¾è®¡å“²å­¦ã€‚å°½ç®¡å¦‚æ­¤ï¼ŒVizieråªæ”¯æŒä¼ ç»Ÿçš„BBOï¼Œä¸èƒ½åº”ç”¨åˆ°é€šç”¨çš„åœºæ™¯ï¼Œå¦‚å¤šç›®æ ‡ï¼Œå¸¦æœ‰çº¦æŸï¼Œè¿™æ˜¯OpenBoxçš„ç›®æ ‡ã€‚è€Œä¸”ï¼Œä¸Vizierä¸åŒçš„æ˜¯ï¼Œæˆ‘ä»¬å°†OpenBoxå¼€æºäº†ã€‚

The key novelty of OpenBox lies in both the system implementation and algorithm design. In terms of system implementation, OpenBox allows users to define their tasks and access the generalized BBO service conveniently via a task description language (TDL) along with customized interfaces. OpenBox also introduces a high-level parallel mechanism by decoupling basic components in common optimization algorithms, which is â€œalgorithm agnosticâ€ and enables parallel execution in both synchronous and asynchronous settings. Moreover, OpenBox also provides a general transfer-learning framework for generalized BBO, which can leverage the prior knowledge acquired from previous tasks to improve the efficiency of the current optimization task. In terms of algorithm design, OpenBox can host most of the state-of-the-art optimization algorithms and make their performances more stable via an automatic algorithm selection module, which can choose proper optimization algorithm for a given problem automatically. Furthermore, OpenBox also supports multi-fidelity and early-stopping algorithms for further optimization of algorithm efficiency.

OpenBoxçš„å…³é”®åˆ›æ–°æ˜¯ç³»ç»Ÿå®ç°å’Œç®—æ³•è®¾è®¡ã€‚åœ¨ç³»ç»Ÿå®ç°ä¸Šï¼ŒOpenBoxä½¿ç”¨æˆ·å¯ä»¥é€šè¿‡ä»»åŠ¡æè¿°è¯­è¨€(TDL)ç”¨å®šåˆ¶çš„ç•Œé¢æ¥å®šä¹‰ä»–ä»¬çš„ä»»åŠ¡ï¼Œæ–¹ä¾¿çš„è®¿é—®é€šç”¨BBOæœåŠ¡ã€‚OpenBoxè¿˜å¼•å…¥äº†ä¸€ç§é«˜å±‚æ¬¡å¹¶è¡Œæœºåˆ¶ï¼Œå°†å¸¸è§çš„ä¼˜åŒ–ç®—æ³•ä¸­çš„åŸºæœ¬ç»„æˆéƒ¨åˆ†è¿›è¡Œè§£è€¦ï¼Œè¿™æ˜¯ç®—æ³•æ— å…³çš„ï¼Œå¯ä»¥åœ¨åŒæ­¥å’Œå¼‚æ­¥çš„è®¾ç½®ä¸­è¿›è¡Œå¹¶è¡Œæ‰§è¡Œã€‚è€Œä¸”ï¼ŒOpenBoxè¿˜å¯¹é€šç”¨BBOç»™å‡ºäº†ä¸€ç§é€šç”¨çš„è¿ç§»å­¦ä¹ æ¡†æ¶ï¼Œå¯ä»¥åˆ©ç”¨ä»ä¹‹å‰çš„ä»»åŠ¡è·å¾—çš„å…ˆéªŒçŸ¥è¯†ï¼Œæ¥æ”¹è¿›ç›®å‰ä¼˜åŒ–ä»»åŠ¡çš„æ•ˆç‡ã€‚åœ¨ä»»åŠ¡è®¾è®¡ä¸Šï¼ŒOpenBoxåŒ…å«äº†å¤šæ•°ç›®å‰æœ€å¥½çš„ä¼˜åŒ–ç®—æ³•ï¼Œé€šè¿‡è‡ªåŠ¨ç®—æ³•é€‰æ‹©æ¨¡å—ï¼Œå¯¹ç»™å®šé—®é¢˜è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„ä¼˜åŒ–ç®—æ³•ï¼Œä½¿æ€§èƒ½ä¼šéå¸¸ç¨³å®šã€‚è€Œä¸”ï¼ŒOpenBoxè¿˜æ”¯æŒå¤šä¿çœŸåº¦å’Œæ—©åœç®—æ³•ï¼Œä»¥è¿›ä¸€æ­¥ä¼˜åŒ–ç®—æ³•æ•ˆç‡ã€‚

Contributions. In summary, our main contributions are:

C1. An open-sourced service for generalized BBO. To the best of our knowledge, OpenBox is the first open-sourced service for efficient and general black-box optimization.

é€šç”¨BBOçš„å¼€æºæœåŠ¡ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼ŒOpenBoxæ˜¯ç¬¬ä¸€ä¸ªé«˜æ•ˆé€šç”¨é»‘ç®±ä¼˜åŒ–ç®—æ³•çš„å¼€æºæœåŠ¡ã€‚

C2. Ease of use. OpenBox provides user-friendly interfaces, visualization, resource-aware management, and automatic algorithm selection for consistent performance.

å®¹æ˜“ä½¿ç”¨ã€‚OpenBoxç»™å‡ºäº†ç”¨æˆ·å‹å¥½çš„ç•Œé¢ï¼Œå¯è§†åŒ–ï¼Œèµ„æºç®¡ç†ï¼Œå’Œè‡ªåŠ¨ç®—æ³•é€‰æ‹©æ¨¡å—ï¼Œä»¥å¾—åˆ°ä¸€è‡´çš„æ€§èƒ½ã€‚

C3. High efficiency and scalability. We develop scalable and general frameworks for transfer-learning and distributed parallel execution in OpenBox. These building blocks are properly integrated to handle diverse optimization scenarios efficiently.

å¾ˆé«˜çš„æ•ˆç‡å’Œå¯æ‰©å±•æ€§ã€‚åœ¨OpenBoxä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†å¯æ‰©å±•çš„é€šç”¨æ¡†æ¶è¿›è¡Œè¿ç§»å­¦ä¹ ï¼Œå’Œåˆ†å¸ƒå¼å¹¶è¡Œæ‰§è¡Œã€‚è¿™äº›æ„å»ºæ¨¡å—å¾—åˆ°äº†åˆé€‚çš„é›†æˆï¼Œä»¥å¤„ç†å„ç§ä¼˜åŒ–åœºæ™¯ã€‚

C4. State-of-the-art performance. Our empirical evaluation demonstrates that OpenBox achieves state-of-the-art performance compared to existing systems over a wide range of BBO tasks.

ç›®å‰æœ€å¥½çš„æ€§èƒ½ã€‚æˆ‘ä»¬çš„ç»éªŒè¯„ä¼°è¡¨æ˜ï¼Œä¸ç°æœ‰çš„ç³»ç»Ÿåœ¨å¾ˆå¤šBBOä»»åŠ¡ä¸Šæ¯”è¾ƒï¼ŒOpenBoxæ˜¯ç›®å‰æœ€å¥½çš„æ€§èƒ½ã€‚

Moving Forward. With the above advantages and features, OpenBox can be used for optimizing a wide variety of different applications in an industrial setting. We are currently conducting an initial deployment of OpenBox in Kuaishou, one of the most popular â€œshort videoâ€ platforms in China, to automate the tedious process of hyperparameter tuning. Initial results have suggested we can outperform human experts.

æœ‰äº†ä¸Šé¢çš„ä¼˜åŠ¿å’Œç‰¹ç‚¹ï¼ŒOpenBoxå¯ä»¥ç”¨äºä¼˜åŒ–å¾ˆå¤šä¸åŒçš„åº”ç”¨ã€‚æˆ‘ä»¬ç›®å‰æ­£åœ¨å°†OpenBoxéƒ¨ç½²åˆ°å¿«æ‰‹ä¸­ï¼Œä»¥å°†è¶…å‚æ•°ä¼˜åŒ–çš„è¿‡ç¨‹è‡ªåŠ¨åŒ–ã€‚åˆæ­¥çš„ç»“æœè¡¨æ˜ï¼Œè¿™å¯ä»¥è¶…è¿‡äººç±»ä¸“å®¶çš„ç»“æœã€‚

## 2. Background and Related Work

**Generalized Black-box Optimization (BBO)**. Black-box optimization makes few assumptions about the problem, and is thus applicable in a wide range of scenarios. We define the generalized BBO problem as follows. The objective function of generalized BBO is a vector-valued black-box function ğ’‡(ğ’™) : X â†’ R^ğ‘, where X is the search space of interest. The goal is to identify the set of Pareto optimal solutions Pâˆ— = {ğ’‡(ğ’™) s.t. \nexists ğ’™â€² âˆˆ X : ğ’‡(ğ’™â€²) â‰º ğ’‡(ğ’™)}, such that any improvement in one objective means deteriorating another. To approximate Pâˆ—, we compute the finite Pareto set P from observed data {(ğ’™_ğ’Š,ğ’š_ğ’Š)}^ğ‘›_{ğ‘–=1}. When ğ‘ = 1, the problem becomes single-objective BBO, as P = {ğ‘¦_best} where ğ‘¦_best is defined as the best objective value observed. We also consider the case with black-box inequality constraints. Denote the set of feasible points by C = {ğ’™ : ğ‘_1(ğ’™) â‰¤ 0, . . . , ğ‘_ğ‘(ğ’™) â‰¤ 0}. Under this setting, we aim to identify the feasible Pareto set P_feas = {ğ’‡(ğ’™) s.t. ğ’™ âˆˆ C, \nexists ğ’™â€² âˆˆ X : ğ’‡(ğ’™â€²) â‰º ğ’‡(ğ’™), ğ’™â€² âˆˆ C}.

é€šç”¨BBOã€‚é»‘ç®±ä¼˜åŒ–å¯¹é—®é¢˜åšçš„å‡è®¾å¾ˆå°‘ï¼Œå› æ­¤åœ¨å¾ˆå¹¿æ³›çš„åœºæ™¯ä¸­éƒ½å¯ä»¥ä½¿ç”¨ã€‚æˆ‘ä»¬å°†é€šç”¨BBOé—®é¢˜å®šä¹‰å¦‚ä¸‹ã€‚é€šç”¨BBOçš„ç›®æ ‡å‡½æ•°æ˜¯ä¸€ä¸ªå‘é‡å€¼çš„é»‘ç®±å‡½æ•°ğ’‡(ğ’™) : X â†’ R^ğ‘ï¼Œå…¶ä¸­Xæ˜¯æ„Ÿå…´è¶£çš„æœç´¢ç©ºé—´ã€‚ç›®æ ‡æ˜¯æ‰¾åˆ°Paretoæœ€ä¼˜è§£çš„é›†åˆPâˆ— = {ğ’‡(ğ’™) s.t. \nexists ğ’™â€² âˆˆ X : ğ’‡(ğ’™â€²) â‰º ğ’‡(ğ’™)}ï¼Œè¿™æ ·ä¸€ä¸ªç›®æ ‡çš„ä»»ä½•æ”¹è¿›æ„å‘³ç€å¦ä¸€ä¸ªç›®æ ‡çš„æ¶åŒ–ã€‚ä¸ºè¿‘ä¼¼P*ï¼Œæˆ‘ä»¬ä»è§‚å¯Ÿåˆ°çš„æ•°æ®{(ğ’™_ğ’Š,ğ’š_ğ’Š)}^ğ‘›_{ğ‘–=1}ä¸­è®¡ç®—æœ‰é™Paretoé›†åˆPã€‚å½“p=1æ—¶ï¼Œé—®é¢˜å°±å˜æˆäº†å•ç›®æ ‡BBOï¼Œå…¶ä¸­P={ğ‘¦_best}ï¼Œè¿™é‡Œ{ğ‘¦_best}å®šä¹‰ä¸ºè§‚å¯Ÿåˆ°çš„æœ€å¥½çš„ç›®æ ‡å€¼ã€‚æˆ‘ä»¬è¿˜è€ƒè™‘æœ‰é»‘ç®±ä¸ç­‰å¼çº¦æŸçš„æƒ…å†µã€‚å°†å¯è¡Œé›†çš„ç‚¹è¡¨ç¤ºä¸ºC = {ğ’™ : ğ‘_1(ğ’™) â‰¤ 0, . . . , ğ‘_ğ‘(ğ’™) â‰¤ 0}ã€‚åœ¨è¿™ç§è®¾ç½®ä¸‹ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯æ‰¾åˆ°å¯è¡Œçš„Paretoé›†P_feas = {ğ’‡(ğ’™) s.t. ğ’™ âˆˆ C, \nexists ğ’™â€² âˆˆ X : ğ’‡(ğ’™â€²) â‰º ğ’‡(ğ’™), ğ’™â€² âˆˆ C}ã€‚

**Black-box Optimization Methods**. Black-box optimization has been studied extensively in many fields, including derivative-free optimization [42], Bayesian optimization (BO) [43], evolutionaray algorithms [23], multi-armed bandit algorithms [31, 45], etc. To optimize expensive-to-evaluate black-box functions with as few evaluations as possible, OpenBox adopts BO, one of the most prevailing frameworks in BBO, as the basic optimization framework. BO iterates between fitting probabilistic surrogate models and determining which configuration to evaluate next by maximizing an acquisition function. With different choices of acquisition functions, BO can be applied to generalized BBO problems.

é»‘ç®±ä¼˜åŒ–æ–¹æ³•ã€‚é»‘ç®±ä¼˜åŒ–åœ¨å¾ˆå¤šé¢†åŸŸä¸­å¾—åˆ°äº†å¹¿æ³›çš„ç ”ç©¶ï¼ŒåŒ…æ‹¬æ— æ¢¯åº¦ä¼˜åŒ–ï¼Œè´å¶æ–¯ä¼˜åŒ–ï¼Œæ¼”åŒ–ç®—æ³•ï¼Œå¤šè‡‚è€è™æœºç®—æ³•ï¼Œç­‰ã€‚ä¸ºç”¨å°½å¯èƒ½å°‘çš„è¯„ä¼°æ¬¡æ•°ï¼Œæ¥ä¼˜åŒ–è¿™äº›è¯„ä¼°èµ·æ¥å¾ˆè€—æ—¶çš„é»‘ç®±å‡½æ•°ï¼ŒOpenBoxé‡‡ç”¨äº†BOä½œä¸ºåŸºæœ¬çš„ä¼˜åŒ–æ¡†æ¶ï¼Œè¿™æ˜¯BBOä¸­æœ€æµè¡Œçš„æ¡†æ¶ä¹‹ä¸€ã€‚BOä¸æ–­çš„æ‹Ÿåˆæ¦‚ç‡ä»£ç†æ¨¡å‹ï¼Œé€šè¿‡æœ€å¤§åŒ–é‡‡é›†å‡½æ•°æ¥ç¡®å®šå“ªç§é…ç½®åœ¨ä¸‹ä¸€æ­¥è¿›è¡Œè¯„ä¼°ã€‚é‡‡é›†å‡½æ•°çš„é€‰æ‹©ä¸åŒï¼ŒBOå¯ä»¥åº”ç”¨åˆ°é€šç”¨BBOé—®é¢˜ä¸­ã€‚

BBO with Multiple Objectives. Many multi-objective BBO algorithms have been proposed [4, 5, 25, 29, 38]. Couckuyt et. al. [7] propose the Hypervolume Probability of Improvement (HVPOI); Yang et. al. [47] and Daulton et. al. [8] use the Expected Hypervolume Improvement (EHVI) metrics.

å¤šç›®æ ‡BBOã€‚æå‡ºäº†å¾ˆå¤šå¤šç›®æ ‡BBOç®—æ³•ã€‚Couckuytç­‰[7]æå‡ºäº†Hypervolume Probability of Improvement (HVPOI)ï¼›Yangç­‰[47]å’ŒDaultonç­‰[8]ä½¿ç”¨äº†Expected Hypervolume Improvement (EHVI)åº¦é‡ã€‚

BBO with Black-box Constraints. Gardner et.al. [16] present Probability of Feasibility (PoF), which uses GP surrogates to model the constraints. In general, multiplying PoF with the unconstrained acquisition function produces the constrained version of it. SCBO [12] employs the trust region method and scales to large batches by extending Thompson sampling to constrained optimization. Other methods handle constraints in different ways [21, 24, 40]. For multiobjective optimization with constraints, PESMOC [17] and MESMOC [5] support constraints by adding the entropy of the conditioned predictive distribution.

å¸¦æœ‰é»‘ç®±çº¦æŸçš„BBOã€‚Gardnerç­‰[16]æå‡ºäº†Probability of Feasibility (PoF)ï¼Œä½¿ç”¨GPä»£ç†æ¥å¯¹çº¦æŸè¿›è¡Œå»ºæ¨¡ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå°†PoFä¸æ— çº¦æŸé‡‡é›†å‡½æ•°ç›¸ä¹˜ï¼Œç”Ÿæˆæœ‰çº¦æŸçš„ç‰ˆæœ¬ã€‚SCBO[12]é‡‡ç”¨ä¿¡ä»»åŒºåŸŸæ–¹æ³•ï¼Œé€šè¿‡å°†Thompsoné‡‡æ ·æ‹“å±•åˆ°çº¦æŸä¼˜åŒ–ä¸­ï¼Œä»è€Œç¼©æ”¾åˆ°å¤§çš„æ‰¹æ¬¡ä¸­ã€‚å…¶ä»–æ–¹æ³•ä»¥ä¸åŒçš„æ–¹å¼å¤„ç†çº¦æŸã€‚å¯¹äºå¸¦æœ‰çº¦æŸçš„å¤šç›®æ ‡ä¼˜åŒ–ï¼ŒPESMOCå’ŒMESMOCæ”¯æŒçº¦æŸï¼ŒåŠ å…¥äº†æœ‰æ¡ä»¶é¢„æµ‹åˆ†å¸ƒçš„ç†µã€‚

**BBO Systems and Packages**. Many of these algorithms have available open-source implementations. BoTorch, GPflowOpt and HyperMapper implement several BO algorithms to solve mathematical problems in different settings. Within the machine learning community, Hyperopt, Spearmint, SMAC3 and HpBandSter aim to optimize the hyper-parameters of machine learning models. Googleâ€™s Vizier is one of the early attempts in building service for BBO. We also note that Facebook Ax provides high-level API for BBO with BoTorch as its Bayesian optimization engine.

BBOç³»ç»Ÿå’ŒåŒ…ã€‚å¾ˆå¤šç®—æ³•éƒ½æœ‰å¼€æºçš„å®ç°ã€‚BoTorchï¼ŒGPflowOptå’ŒHyperMapperå®ç°äº†å‡ ç§BOç®—æ³•ï¼Œæ¥åœ¨ä¸åŒçš„è®¾ç½®ä¸­æ±‚è§£æ•°å­¦é—®é¢˜ã€‚åœ¨æœºå™¨å­¦ä¹ å›¢ä½“ä¸­ï¼ŒHyperoptï¼ŒSpearmintï¼ŒSMAC3å’ŒHpBandSterçš„ç›®æ ‡æ˜¯ä¼˜åŒ–æœºå™¨å­¦ä¹ æ¨¡å‹ä¸­çš„è¶…å‚æ•°ã€‚Googleçš„Vizieræ˜¯æ„å»ºBBOæœåŠ¡çš„æ—©æœŸå°è¯•ã€‚æˆ‘ä»¬è¿˜æŒ‡å‡ºï¼ŒFacebook Axä¸ºBBOæä¾›äº†é«˜å±‚æ¬¡APIï¼ŒBoTorchæ˜¯å…¶è´å¶æ–¯ä¼˜åŒ–å¼•æ“ã€‚

## 3. System Overview

In this section, we provide the basic concepts in the paper, explore the design principles in implementing black-box optimization (BBO) as a service, and describe the system architecture.

æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ç»™å‡ºäº†æœ¬æ–‡ä¸­çš„åŸºæœ¬æ¦‚å¿µï¼Œæ¢ç´¢åœ¨å®ç°BBOæœåŠ¡ä¸­çš„è®¾è®¡åŸåˆ™ï¼Œæè¿°äº†ç³»ç»Ÿæ¶æ„ã€‚

### 3.1 Definitions

Throughout the paper, we use the following terms to describe the semantics of the system:

åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨ä¸‹åˆ—é¡¹æ¥æè¿°ç³»ç»Ÿçš„è¯­ä¹‰ï¼š

Configuration. Also called suggestion, a vector ğ’™ sampled from the given search space X; each element in ğ’™ is an assignment of a parameter from its domain.

é…ç½®ã€‚ä¹Ÿç§°ä¸ºå»ºè®®ï¼Œä»ç»™å®šçš„æœç´¢ç©ºé—´Xä¸­é‡‡æ ·å¾—åˆ°çš„å‘é‡ğ’™ï¼›ğ’™ä¸­çš„æ¯ä¸ªå…ƒç´ ï¼Œæ˜¯åœ¨å…¶é¢†åŸŸä¸­ä¸€ä¸ªå‚æ•°çš„æŒ‡å®šå€¼ã€‚

Trial. Corresponds to an evaluation of a configuration ğ’™, which has three status: Completed, Running, Ready. Once a trial is completed, we can obtain the evaluation result ğ’‡(ğ’™).

å°è¯•ã€‚å¯¹åº”ä¸€ä¸ªé…ç½®ğ’™çš„ä¸€æ¬¡è¯„ä¼°ï¼Œæœ‰ä¸‰ç§çŠ¶æ€ï¼šå®Œæˆï¼Œè¿è¡Œï¼Œå‡†å¤‡å¥½ã€‚ä¸€æ—¦ä¸€æ¬¡å°è¯•å®Œæˆäº†ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¾—åˆ°è¯„ä¼°ç»“æœğ’‡(ğ’™)ã€‚

Task. A BBO problem over a search space X. The task type is identified by the number of objectives and constraints.

ä»»åŠ¡ã€‚åœ¨ä¸€ä¸ªæœç´¢ç©ºé—´Xä¸­çš„ä¸€ä¸ªBBOé—®é¢˜ã€‚ä»»åŠ¡ç±»å‹æŒ‰ç…§ç›®æ ‡å’Œçº¦æŸçš„æ•°é‡æ¥ç¡®å®šã€‚

Worker. Refers to a process responsible for executing a trial. è´Ÿè´£æ‰§è¡Œä¸€æ¬¡å°è¯•trialçš„è¿‡ç¨‹ã€‚

### 3.2 Goals and Principles

3.2.1 Design Goal. As mentioned before, OpenBoxâ€™s design satisfies the following desiderata: è®¾è®¡ç›®æ ‡ã€‚å‰é¢æåˆ°è¿‡ï¼ŒOpenBoxçš„è®¾è®¡æ»¡è¶³ä¸‹é¢çš„è€ƒè™‘ï¼š

â€¢ Ease of use. Minimal user effort, and user-friendly visualization for tracking and managing BBO tasks.

å®¹æ˜“ä½¿ç”¨ã€‚ç”¨æˆ·çš„å·¥ä½œé‡æœ€å°åŒ–ï¼Œç”¨æˆ·å‹å¥½çš„å¯è§†åŒ–ï¼Œå¯ä»¥è¿½è¸ªå¹¶ç®¡ç†BBOä»»åŠ¡ã€‚

â€¢ Consistent performance. Host state-of-the-art optimization algorithms; choose the proper algorithm automatically.

ä¸€è‡´çš„æ€§èƒ½ã€‚åŒ…å«ç›®å‰æœ€å¥½çš„ä¼˜åŒ–ç®—æ³•ï¼›è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„ç®—æ³•ã€‚

â€¢ Resource-aware management. Give cost-model based advice to users, e.g., minimal workers or time-budget.

èµ„æºæ•æ„Ÿçš„ç®¡ç†ã€‚ç»™ç”¨æˆ·åŸºäºä»·æ ¼æ¨¡å‹çš„å»ºè®®ï¼Œå¦‚ï¼Œæœ€å°‘çš„workersæˆ–æ—¶é—´é¢„ç®—ã€‚

â€¢ Scalability. Scale to dimensions on the number of input variables, objectives, tasks, trials, and parallel evaluations.

å¯æ‰©å±•æ€§ã€‚è¾“å…¥å˜é‡çš„æ•°é‡ï¼Œç›®æ ‡çš„æ•°é‡ï¼Œä»»åŠ¡çš„æ•°é‡ï¼Œtrialsçš„æ•°é‡ï¼Œå’Œå¹¶è¡Œè¯„ä¼°çš„æ•°é‡ï¼Œéƒ½è¦å¯ä»¥ç¼©æ”¾ã€‚

â€¢ High efficiency. Effective use of parallel resources, system optimization with transfer-learning and multi-fidelities, etc.

é«˜æ•ˆã€‚å¹¶è¡Œèµ„æºçš„é«˜æ•ˆä½¿ç”¨ï¼Œå¸¦æœ‰è¿ç§»å­¦ä¹ å’Œå¤šä¿çœŸåº¦çš„ç³»ç»Ÿä¼˜åŒ–ï¼Œç­‰ã€‚

â€¢ Fault tolerance, extensibility, and data privacy protection.

å®¹é”™æ€§ï¼Œå¯æ‹“å±•æ€§ï¼Œå’Œæ•°æ®éšç§ä¿æŠ¤ã€‚

3.2.2 Design Principles. We present the key principles underlying the design of OpenBox.

è®¾è®¡åŸåˆ™ã€‚æˆ‘ä»¬ç»™å‡ºOpenBoxè®¾è®¡çš„å…³é”®åŸåˆ™ã€‚

**P1: Provide convenient service API that abstracts the implementation and execution complexity away from the user**. For ease of use, we adopt the â€œBBO as a serviceâ€ paradigm and implement OpenBox as a managed general service for black-box optimization. Users can access this service via REST API conveniently (see Figure 2), and do not need to worry about other issues such as environment setup, software maintenance, programming, and optimization of the execution. Moreover, we also provide a Web UI, through which users can easily track and manage the tasks.

åŸåˆ™1ï¼šç»™å‡ºæ–¹ä¾¿çš„æœåŠ¡APIï¼Œå°†å®ç°å’Œæ‰§è¡Œçš„å¤æ‚åº¦æŠ½è±¡æ‰ï¼Œä¸ç»™ç”¨æˆ·çœ‹åˆ°ã€‚ä¸ºå®¹æ˜“ä½¿ç”¨ï¼Œæˆ‘ä»¬é‡‡ç”¨äº†å°†BBOåšæˆæœåŠ¡çš„æ¨¡å¼ï¼Œå°†OpenBoxå®ç°ä¸ºé»‘ç®±ä¼˜åŒ–çš„é€šç”¨æœåŠ¡ã€‚ç”¨æˆ·å¯ä»¥æ–¹ä¾¿çš„é€šè¿‡REST APIè®¿é—®è¿™äº›æœåŠ¡ï¼ˆè§å›¾2ï¼‰ï¼Œä¸éœ€è¦æ‹…å¿ƒå…¶ä»–çš„é—®é¢˜ï¼Œå¦‚ç¯å¢ƒè®¾ç½®ï¼Œè½¯ä»¶ç»´æŠ¤æ€§ï¼Œç¼–ç¨‹ï¼Œå’Œæ‰§è¡Œçš„ä¼˜åŒ–ã€‚è€Œä¸”ï¼Œæˆ‘ä»¬è¿˜æä¾›äº†ä¸€ä¸ªç½‘é¡µUIï¼Œé€šè¿‡è¿™äº›ç”¨æˆ·å¯ä»¥å¾ˆå®¹æ˜“çš„è¿½è¸ªå’Œç®¡ç†è¿™äº›ä»»åŠ¡ã€‚

**P2: Separate optimization algorithm selection complexity away from the user**. Users do not need to disturb themselves with choosing the proper algorithm to solve a specific problem via the automatic algorithm selection module. Furthermore, an important decision is to keep our service stateless (see Figure 2), so that we can seamlessly switch algorithms during a task, i.e., dynamically choose the algorithm that is likely to perform the best for a particular task. This enables OpenBox to achieve satisfactory performance once the BBO algorithm is selected properly.

åŸåˆ™2ï¼šç”¨æˆ·æ— éœ€æ‹…å¿ƒä¼˜åŒ–ç®—æ³•é€‰æ‹©çš„å¤æ‚æ€§ã€‚ç”¨æˆ·åœ¨æ±‚è§£ç‰¹å®šé—®é¢˜æ—¶ï¼Œå¯ä»¥é€šè¿‡è‡ªåŠ¨ç®—æ³•é€‰æ‹©æ¨¡å—æ¥é€‰æ‹©åˆé€‚çš„ç®—æ³•ã€‚è€Œä¸”ï¼Œä¸€ä¸ªé‡è¦çš„å†³å®šæ˜¯ï¼Œä¿æŒæˆ‘ä»¬çš„æœåŠ¡æ˜¯æ— çŠ¶æ€çš„ï¼ˆè§å›¾2ï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥æ— ç¼çš„åœ¨ä»»åŠ¡ä¹‹é—´åˆ‡æ¢ç®—æ³•ï¼Œå³ï¼ŒåŠ¨æ€çš„é€‰æ‹©å¯¹ç‰¹å®šçš„ä»»åŠ¡çœ‹èµ·æ¥æ˜¯æœ€å¥½çš„ç®—æ³•ã€‚ä¸€æ—¦é€‰æ‹©äº†åˆé€‚çš„BBOç®—æ³•ï¼ŒOpenBoxå°±å¯ä»¥è·å¾—ä»¤äººæ»¡æ„çš„æ€§èƒ½ã€‚

**P3: Support general distributed parallelization and transfer learning**. We aim to provide users with full potential to improve the efficiency of the BBO service. We design an â€œalgorithm agnosticâ€ mechanism that can parallelize the BBO algorithms (Sec. 5.1), through which we do not need to re-design the parallel version for each algorithm individually. Moreover, if the optimization history over similar tasks is provided, our transfer learning framework can leverage the history to accelerate the current task (Sec. 5.2).

åŸåˆ™3ï¼šæ”¯æŒä¸€èˆ¬çš„åˆ†å¸ƒå¼å¹¶è¡ŒåŒ–å’Œè¿ç§»å­¦ä¹ ã€‚æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ç»™ç”¨æˆ·æä¾›æ”¹è¿›BBOæœåŠ¡çš„æ•ˆç‡çš„å®Œæ•´å¯èƒ½æ€§ã€‚æˆ‘ä»¬è®¾è®¡ä¸€ä¸ªç®—æ³•æ— å…³çš„æœºåˆ¶ï¼Œä½¿BBOç®—æ³•å¹¶è¡ŒåŒ–ï¼Œé€šè¿‡è¿™ä¸ªæœºåˆ¶ï¼Œæˆ‘ä»¬ä¸éœ€è¦é‡æ–°è®¾è®¡æ¯ä¸ªç®—æ³•çš„å¹¶è¡ŒåŒ–ç‰ˆæœ¬ã€‚è€Œä¸”ï¼Œå¦‚æœç»™å‡ºäº†ç±»ä¼¼ä»»åŠ¡çš„ä¼˜åŒ–å†å²ï¼Œæˆ‘ä»¬çš„è¿ç§»å­¦ä¹ æ¡†æ¶å¯ä»¥åˆ©ç”¨è¿™ä¸ªå†å²æ¥åŠ é€Ÿå½“å‰çš„ä»»åŠ¡ã€‚

**P4: Offer resource-aware management that saves user expense**. OpenBox implements a resource-aware module and offers advice to users, which can save expense or resources for users especially in the cloud environment. Using performance-resource extrapolation (Sec. 4.4), OpenBox can estimate 1) the minimal number of workers users need to complete the current task within the given time budget, or 2) the minimal time budget to finish the current task given a fixed number of workers. For tasks that involve expensive-to-evaluate functions, low-fidelity or early-stopped evaluations with less cost could help accelerate the convergence of the optimization process (Sec. 5.3).

åŸåˆ™4ï¼šç»™å‡ºæ„è¯†åˆ°èµ„æºçš„ç®¡ç†ï¼ŒèŠ‚çœç”¨æˆ·çš„ä»£ä»·ã€‚OpenBoxå®ç°äº†ä¸€ä¸ªæ„è¯†åˆ°èµ„æºçš„æ¨¡å—ï¼Œå¯¹ç”¨æˆ·ç»™å‡ºå»ºè®®ï¼Œå¯ä»¥èŠ‚çœç”¨æˆ·çš„ä»£ä»·æˆ–èµ„æºï¼Œå°¤å…¶æ˜¯åœ¨äº‘ç¯å¢ƒä¸­ã€‚ä½¿ç”¨æ€§èƒ½-èµ„æºå¤–æ’ï¼ŒOpenBoxå¯ä»¥ä¼°è®¡ 1)åœ¨ç»™å®šçš„æ—¶é—´é¢„ç®—ä¸‹ï¼Œå®Œæˆå½“å‰ä»»åŠ¡æ‰€éœ€çš„æœ€å°‘çš„workeræ•°é‡ï¼›2)åœ¨ç»™å®šæ•°é‡çš„workersä¸‹ï¼Œå®Œæˆå½“å‰ä»»åŠ¡æ‰€éœ€çš„æœ€å°‘æ—¶é—´é¢„ç®—ã€‚å¯¹äºæ¶‰åŠåˆ°è¯„ä¼°èµ·æ¥å¾ˆè€—æ—¶è€—èµ„æºçš„å‡½æ•°çš„ä»»åŠ¡ï¼Œä½ä¿çœŸåº¦æˆ–æ—©åœçš„è¯„ä¼°ï¼Œæœ‰è¾ƒå°‘çš„ä»£ä»·ï¼Œä¼šå¸®åŠ©åŠ é€Ÿä¼˜åŒ–è¿‡ç¨‹çš„æ”¶æ•›ã€‚

### 3.3 System Architecture

Based on these design principles, we build OpenBox as depicted in Figure 2, which includes five main components. Service Master is responsible for node management, load balance, and fault tolerance. Task Database holds the states of all tasks. Suggestion Service creates new configurations for each task. REST API establishes the bridge between users/workers and suggestion service. Evaluation workers are provided and owned by the users.

åŸºäºè¿™äº›è®¾è®¡åŸåˆ™ï¼Œæˆ‘ä»¬æ„å»ºäº†OpenBoxï¼Œå¦‚å›¾2æ‰€ç¤ºï¼Œè¿™åŒ…æ‹¬5ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†ã€‚Service Masterè´Ÿè´£èŠ‚ç‚¹ç®¡ç†ï¼Œè´Ÿè½½å‡è¡¡ï¼Œå’Œå®¹é”™ã€‚ä»»åŠ¡æ•°æ®åº“ä¿å­˜æ‰€æœ‰ä»»åŠ¡çš„çŠ¶æ€ã€‚å»ºè®®æœåŠ¡å¯¹æ¯ä¸ªä»»åŠ¡åˆ›å»ºæ–°çš„é…ç½®ã€‚REST APIç¡®å®šç”¨æˆ·/workerså’Œå»ºè®®æœåŠ¡ä¹‹é—´çš„æ¡¥æ¢ã€‚ç”¨æˆ·æ‹¥æœ‰å¹¶æä¾›è¯„ä¼°workersã€‚

## 4. System Design

In this section, we elaborate on the main features and components of OpenBox from a service perspective.

æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ç»™å‡ºOpenBoxçš„ä¸»è¦ç‰¹å¾å’Œç»„æˆéƒ¨åˆ†ã€‚

### 4.1 Service Interfaces

4.1.1 Task Description Language. For ease of usage, we design a Task Description Language (TDL) to define the optimization task. The essential part of TDL is to define the search space, which includes the type and bound for each parameter and the relationships among them. The parameter types â€” FLOAT, INTEGER, ORDINAL and CATEGORICAL are supported in OpenBox. In addition, users can add conditions of the parameters to further restrict the search space. Users can also specify the time budget, task type, number of workers, parallel strategy and use of history in TDL. Figure 3 gives an example of TDL. It defines four parameters x1-4 of different types and a condition cdn1, which indicates that x1 is active only if x3 = â€œa3â€. The time budget is three hours, the parallel strategy is async, and transfer learning is enabled.

ä»»åŠ¡æè¿°è¯­è¨€ã€‚ä¸ºå®¹æ˜“ä½¿ç”¨ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ä»»åŠ¡æè¿°è¯­è¨€(TDL)æ¥å®šä¹‰ä¼˜åŒ–ä»»åŠ¡ã€‚TDLçš„åŸºæœ¬éƒ¨åˆ†æ˜¯å®šä¹‰æœç´¢ç©ºé—´ï¼Œè¿™åŒ…æ‹¬æ¯ä¸ªå‚æ•°çš„ç±»å‹å’Œç•Œé™ï¼Œå’Œå…¶ä¹‹é—´çš„å…³ç³»ã€‚å‚æ•°ç±»å‹æœ‰ï¼Œæµ®ç‚¹ï¼Œæ•´æ•°ï¼Œåºæ•°å’Œç±»åˆ«ï¼Œåœ¨OpenBoxä¸­éƒ½æ”¯æŒã€‚å¦å¤–ï¼Œç”¨æˆ·å¯ä»¥å¯¹å‚æ•°åŠ å…¥æ¡ä»¶ï¼Œä»¥è¿›ä¸€æ­¥é™åˆ¶æœç´¢ç©ºé—´ã€‚ç”¨æˆ·è¿˜å¯ä»¥åœ¨TDLä¸­æŒ‡å®šæ—¶é—´é¢„ç®—ï¼Œä»»åŠ¡ç±»å‹ï¼Œworkersæ•°é‡ï¼Œå¹¶è¡Œç­–ç•¥å’Œä½¿ç”¨å†å²ã€‚å›¾3ç»™å‡ºäº†TDLçš„ä¸€ä¸ªä¾‹å­ï¼Œå®šä¹‰äº†4ä¸ªå‚æ•°x1-4ï¼Œç±»å‹ä¸åŒï¼Œå’Œä¸€ä¸ªæ¡ä»¶cdn1ï¼Œè¡¨æ˜åªæœ‰åœ¨x3="a3"çš„æƒ…å†µï¼Œx1æ˜¯æ¿€æ´»çš„ã€‚æ—¶é—´é¢„ç®—æ˜¯3ä¸ªå°æ—¶ï¼Œå¹¶è¡Œç­–ç•¥æ˜¯éåŒæ­¥çš„ï¼Œè¿ç§»å­¦ä¹ æ˜¯ä½¿èƒ½çš„ã€‚

4.1.2 Basic Workflow. Given the TDL for a task, the basic workflow of OpenBox is implemented as follows: ç»™å®šäº†ä¸€ä¸ªä»»åŠ¡çš„TDLï¼ŒOpenBoxçš„åŸºæœ¬å·¥ä½œæµå®ç°å¦‚ä¸‹ï¼š

```

# Register the worker with a task.

global_task_id = worker. CreateTask(task_tdl)

worker. BindTask(global_task_id)

while not worker. TaskFinished ():

# Obtain a configuration to evaluate.

config = worker. GetSuggestions ()

# Evaluate the objective function.

result = Evaluate (config)

# Report the evaluated results to the server.

worker. UpdateObservations (config, result)

```

Here Evaluate is the evaluation procedure of objective function provided by users. By calling CreateTask, the worker obtains a globally unique identifier global_task_id. All workers registered with the same global_task_id are guaranteed to link with the same task, which enables parallel evaluations. While the task is not finished, the worker continues to call GetSuggestions and UpdateObservations to pull suggestions from the suggestion service and update their corresponding observations.

è¿™é‡ŒEvaluateæ˜¯ç›®æ ‡å‡½æ•°çš„è¯„ä¼°è¿‡ç¨‹ï¼Œç”±ç”¨æˆ·ç»™å‡ºã€‚é€šè¿‡è°ƒç”¨CreateTaskï¼Œworkerå¾—åˆ°äº†ä¸€ä¸ªå…¨å±€å”¯ä¸€çš„æ ‡å¿—ç¬¦global_task_idã€‚æ‰€æœ‰æ³¨å†Œä¸ºç›¸åŒçš„global_task_idçš„workerä¿è¯éƒ½è¿æ¥åˆ°ç›¸åŒçš„ä»»åŠ¡ï¼Œè¿™ç¡®ä¿äº†å¹¶è¡Œè¯„ä¼°ã€‚åœ¨ä»»åŠ¡æ²¡æœ‰å®Œæˆæ—¶ï¼ŒworkeræŒç»­è°ƒç”¨GetSuggestionså’ŒUpdateObservationsï¼Œä»¥ä»å»ºè®®æœåŠ¡ä¸­æ‹‰å–å»ºè®®ï¼Œæ›´æ–°å…¶å¯¹åº”çš„è§‚å¯Ÿã€‚

4.1.3 Interfaces. Users can interact with the OpenBox service via a REST API. We list the most important service calls as follows:

æ¥å£ã€‚ç”¨æˆ·å¯ä»¥é€šè¿‡REST APIæ¥ä¸OpenBoxæœåŠ¡è¿›è¡Œäº¤äº’ã€‚æˆ‘ä»¬åˆ—å‡ºäº†æœ€é‡è¦çš„æœåŠ¡è°ƒç”¨å¦‚ä¸‹ï¼š

â€¢ Register: It takes as input the global_task_id, which is created when calling CreateTask from workers, and binds the current worker with the corresponding task. This allows for sharing the optimization history across multiple workers.

æ³¨å†Œï¼šä»¥global_task_idä¸ºè¾“å…¥ï¼Œè¿™æ˜¯å½“ä»workersè°ƒç”¨CreateTaskæ—¶åˆ›å»ºçš„ï¼Œå°†å½“å‰çš„workerä¸å¯¹åº”çš„ä»»åŠ¡ç»‘å®šã€‚è¿™å¯ä»¥åœ¨å¤šä¸ªworkerä¸­å…±äº«ä¼˜åŒ–å†å²ã€‚

â€¢ Suggest: It suggests the next configurations to evaluate, given the historical observations of the current task.

å»ºè®®ï¼šåœ¨ç»™å®šå½“å‰ä»»åŠ¡çš„å†å²è§‚å¯Ÿä¸‹ï¼Œè¿™å»ºè®®ä¸‹ä¸€ä¸ªé…ç½®æ¥è¿›è¡Œè¯„ä¼°ã€‚

â€¢ Update: This method updates the optimization history with the observations obtained from workers. The observations include three parts: the values of the objectives, the results of constraints, and the evaluation information.

æ›´æ–°ï¼šè¿™ä¸ªæ–¹æ³•ç”¨workerså¾—åˆ°çš„è§‚å¯Ÿæ›´æ–°ä¼˜åŒ–å†å²ã€‚è§‚å¯ŸåŒ…æ‹¬ä¸‰éƒ¨åˆ†ï¼šç›®æ ‡çš„å€¼ï¼Œçº¦æŸçš„ç»“æœï¼Œè¯„ä¼°ä¿¡æ¯ã€‚

â€¢ StopEarly: It returns a boolean value that indicates whether the current evaluation should be stopped early.

æ—©åœï¼šè¿”å›ä¸€ä¸ªboolå€¼ï¼Œè¡¨ç¤ºå½“å‰çš„è¯„ä¼°æ˜¯å¦åº”å½“æ—©åœã€‚

â€¢ Extrapolate: It uses performance-resource extrapolation, and interactively gives resource-aware advice to users.

å¤–æ’ï¼šä½¿ç”¨æ€§èƒ½èµ„æºå¤–æ’ï¼Œäº’åŠ¨çš„å‘ç”¨æˆ·ç»™å‡ºèµ„æºæ•æ„Ÿçš„å»ºè®®ã€‚

### 4.2 Automatic Algorithm Selection

OpenBox implements a wide range of optimization algorithms to achieve high performance in various BBO problems. Unlike the existing software packages that use the same algorithm for each task and the same setting for each algorithm, OpenBox chooses the proper algorithm and setting according to the characteristic of the incoming task. We use the classic EI [37] for single-objective optimization task. For multi-objective problems, we select EHVI [11] when the number of objectives is less than 5; we use MESMO [4] algorithm for problems with a larger number of objectives, since EHVIâ€™s complexity increases exponentially as the number of objectives increases, which not only incurs a large computational overhead but also accumulates floating-point errors. We select the surrogate models in BO depending on the configuration space and the number of trials: If the input space has conditions, such as one parameter must be less than another parameter, or there are over 50 parameters in the input space, or the number of trials exceeds 500, we choose the Probabilistic Random Forest proposed in [27] instead of Gaussian Process (GP) as the surrogate to avoid incompatibility or high computational complexity of GP. Otherwise, we use GP [10]. In addition, OpenBox will use the L-BFGS-B algorithm to optimize the acquisition function if the search space only contains FLOAT and INTEGER parameters; it applies an interleaved local and random search when some of the parameters are not numerical. More details about the algorithms implemented in OpenBox are discussed in Appendix A.2.

OpenBoxå®ç°äº†å¾ˆå¤šä¼˜åŒ–ç®—æ³•ï¼Œä»¥åœ¨å„ç§BBOé—®é¢˜ä¸­è·å¾—å¾ˆå¥½çš„æ€§èƒ½ã€‚ç°æœ‰çš„è½¯ä»¶åŒ…å¯¹æ¯ä¸ªä»»åŠ¡ä½¿ç”¨ç›¸åŒçš„ç®—æ³•ï¼Œå¯¹æ¯ä¸ªç®—æ³•ä½¿ç”¨ç›¸åŒçš„è®¾ç½®ï¼Œä¸æ­¤ä¸åŒçš„æ˜¯ï¼ŒOpenBoxæ ¹æ®ä»»åŠ¡çš„ç‰¹ç‚¹é€‰æ‹©åˆé€‚çš„ç®—æ³•å’Œé…ç½®ã€‚æˆ‘ä»¬å¯¹å•ç›®æ ‡ä¼˜åŒ–ä»»åŠ¡ä½¿ç”¨ç»å…¸çš„EI[37]ã€‚å¯¹å¤šç›®æ ‡é—®é¢˜ï¼Œåœ¨ç›®æ ‡æ•°é‡å°‘äº5æ—¶ï¼Œæˆ‘ä»¬é€‰æ‹©EHVI[11]ï¼›å¯¹äºç›®æ ‡æ•°é‡æ›´å¤§çš„é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨MESMO[4]ç®—æ³•ï¼Œå› ä¸ºéšç€ç›®æ ‡æ•°é‡çš„å¢é•¿ï¼ŒEHVIçš„å¤æ‚åº¦å‘ˆæŒ‡æ•°çº§å¢é•¿ï¼Œä¸ä»…å¸¦æ¥å¤§é‡è®¡ç®—ä»£ä»·ï¼Œè€Œä¸”ä¼šç´¯ç§¯æµ®ç‚¹è¯¯å·®ã€‚æˆ‘ä»¬æ ¹æ®é…ç½®ç©ºé—´å’Œtrialsçš„æ•°é‡ï¼Œåœ¨BOä¸­é€‰æ‹©ä»£ç†æ¨¡å‹ï¼šå¦‚æœè¾“å…¥ç©ºé—´æœ‰æ¡ä»¶ï¼Œæ¯”å¦‚ä¸€ä¸ªå‚æ•°å¿…é¡»å°äºå¦ä¸€ä¸ªå‚æ•°ï¼Œæˆ–åœ¨è¾“å…¥ç©ºé—´ä¸­æœ‰è¶…è¿‡50ä¸ªå‚æ•°ï¼Œæˆ–trialsçš„æ•°é‡è¶…è¿‡äº†500ï¼Œæˆ‘ä»¬é€‰æ‹©[27]ä¸­æå‡ºçš„æ¦‚ç‡éšæœºæ£®æ—ï¼Œæ›¿æ¢é«˜æ–¯è¿‡ç¨‹ä½œä¸ºä»£ç†ï¼Œä»¥é¿å…ä¸GPä¸å…¼å®¹ï¼Œæˆ–å¤æ‚åº¦å¤ªé«˜ã€‚å¦åˆ™ï¼Œæˆ‘ä»¬ä½¿ç”¨GPã€‚å¦å¤–ï¼Œå¦‚æœæœç´¢ç©ºé—´åŒ…å«æµ®ç‚¹å’Œæ•´æ•°å‚æ•°ï¼ŒOpenBoxä¼šä½¿ç”¨L-BFGS-Bç®—æ³•æ¥ä¼˜åŒ–é‡‡é›†å‡½æ•°ï¼›åœ¨ä¸€äº›å‚æ•°ä¸æ˜¯æ•°å€¼å‚æ•°æ—¶ï¼Œä¼šå®è¡Œä¸€ä¸ªäº¤å‰çš„å±€éƒ¨å’Œéšæœºæœç´¢ã€‚å…³äºåœ¨OpenBoxä¸­å®ç°çš„ç®—æ³•çš„æ›´å¤šçš„ç»†èŠ‚ï¼Œæˆ‘ä»¬åœ¨é™„å½•A.2ä¸­è®¨è®ºã€‚

### 4.3 Parallel Infrastructure

OpenBox is designed to generate suggestions for a large number of tasks concurrently, and a single machine would be insufficient to handle the workload. Our suggestion service is therefore deployed across several machines, called suggestion servers. Each suggestion server generates suggestions for several tasks in parallel, giving us a massively scalable suggestion infrastructure. Another main component is service master, which is responsible for managing the suggestion servers and balancing the workload. It serves as the unified endpoint, and accepts the requests from workers; in this way, each worker does not need to know the dispatching details. The worker requests new configurations from the suggestion server and the suggestion server generates these configurations based on an algorithm determined by the automatic algorithm selection module. Concretely, in this process, the suggestion server utilizes the local penalization based parallelization mechanism (Sec. 5.1) and transfer-learning framework (Sec. 5.2) to improve the sample efficiency.

OpenBoxè®¾è®¡ç”¨äºåŒæ—¶å¯¹å¤§é‡ä»»åŠ¡æ¥ç”Ÿæˆå»ºè®®ï¼Œä¸€å°æœºå™¨ä¸è¶³ä»¥ç”¨äºå¤„ç†workloadã€‚æˆ‘ä»¬çš„å»ºè®®æœåŠ¡å› æ­¤æ˜¯åœ¨å‡ å°æœºå™¨ä¸­éƒ¨ç½²çš„ï¼Œç§°ä¸ºå»ºè®®æœåŠ¡å™¨ã€‚æ¯ä¸ªå»ºè®®æœåŠ¡å™¨å¯¹å‡ ä¸ªä»»åŠ¡å¹¶è¡Œç”Ÿæˆå»ºè®®ï¼Œç»™äº†æˆ‘ä»¬ä¸€ä¸ªå·¨å¤§çš„å¯æ‰©å±•çš„å»ºè®®åŸºç¡€è®¾æ–½ã€‚å¦ä¸€ä¸ªä¸»è¦ç»„æˆéƒ¨åˆ†æ˜¯ï¼ŒæœåŠ¡masterï¼Œè´Ÿè´£ç®¡ç†å»ºè®®æœåŠ¡å™¨ï¼Œå‡è¡¡workloadã€‚å®ƒçš„ä½œç”¨æ˜¯ç»Ÿä¸€çš„endpointï¼Œæ¥å—workersçš„è¯·æ±‚ï¼›è¿™æ ·ï¼Œæ¯ä¸ªworkerä¸éœ€è¦çŸ¥é“è°ƒåº¦ç»†èŠ‚ã€‚workerä»å»ºè®®æœåŠ¡å™¨ä¸Šè¯·æ±‚æ–°çš„é…ç½®ï¼Œå»ºè®®æœåŠ¡å™¨ç”Ÿæˆè¿™äº›é…ç½®åŸºäºè‡ªåŠ¨ç®—æ³•é€‰æ‹©æ¨¡å—ç¡®å®šçš„ç®—æ³•ã€‚å…·ä½“çš„ï¼Œåœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œå»ºè®®æœåŠ¡å™¨åˆ©ç”¨åŸºäºå±€éƒ¨æƒ©ç½šçš„å¹¶è¡Œæœºåˆ¶å’Œè¿ç§»å­¦ä¹ æ¡†æ¶ï¼Œæ¥æ”¹è¿›é‡‡æ ·æ•ˆç‡ã€‚

One main design consideration is to maintain a fault-tolerant production system, as machine crash happens inevitably. In OpenBox, the service master monitors the status of each server and preserves a table of active servers. When a new task comes, the service master will assign it to an active server and record this binding information. If one server is down, its tasks will be dispatched to a new server by the master, along with the related optimization history stored in the task database. Load balance is one of the most important guidelines to make such task assignments. In addition, the snapshot of service master is stored in the remote database service; if the master is down, we can recover it by restarting the node and fetching the snapshot from the database.

ä¸€ä¸ªä¸»è¦çš„è®¾è®¡è€ƒè™‘æ˜¯ï¼Œç»´æŠ¤ä¸€ä¸ªå®¹é”™ç”Ÿäº§ç³»ç»Ÿï¼Œå› ä¸ºæœºå™¨ä¸å¯é¿å…çš„ä¼šå´©æºƒã€‚åœ¨OpenBoxä¸­ï¼ŒæœåŠ¡å™¨masterç›‘æ§æ¯ä¸ªæœåŠ¡å™¨çš„çŠ¶æ€ï¼Œä¿ç•™æ´»è·ƒæœåŠ¡å™¨çš„è¡¨æ ¼ã€‚å½“æ–°ä»»åŠ¡åˆ°æ¥æ—¶ï¼ŒæœåŠ¡masterä¼šå°†å…¶æŒ‡å®šåˆ°ä¸€ä¸ªæ´»è·ƒçš„æœåŠ¡å™¨ï¼Œè®°å½•è¿™ä¸ªç»‘å®šä¿¡æ¯ã€‚å¦‚æœä¸€ä¸ªæœåŠ¡å™¨downæœºï¼Œå…¶ä»»åŠ¡ä¼šç”±masteræŒ‡æ´¾åˆ°ä¸€ä¸ªæ–°çš„æœåŠ¡å™¨ï¼Œä»¥åŠç›¸å…³çš„ä¼˜åŒ–å†å²ï¼Œå­˜å‚¨åœ¨ä»»åŠ¡æ•°æ®åº“ä¸­ã€‚è´Ÿè½½å‡è¡¡æ˜¯ä¸€ä¸ªæœ€é‡è¦çš„æŒ‡å¼•æ¥è¿›è¡Œè¿™æ ·çš„ä»»åŠ¡æŒ‡å®šã€‚å¦å¤–ï¼ŒæœåŠ¡masterçš„å¿«ç…§å­˜å‚¨åœ¨è¿œç¨‹æ•°æ®åº“æœåŠ¡ä¸­ï¼›å¦‚æœmaster downæœºï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡é‡å¯è¿™ä¸ªèŠ‚ç‚¹ï¼Œä»æ•°æ®åº“ä¸­å–å›å¿«ç…§ï¼Œæ¥è¿›è¡Œæ¢å¤ã€‚

### 4.4 Performance-Resource Extrapolation

In the setting of parallel infrastructure with cloud computing, saving expense is one of the most important concerns from users. OpenBox can guide users to configure their resources, e.g., the minimal number of workers or time budget, which further saves expense for users. Concretely, we use a weighted cost model to extrapolate the performance vs. trial curve. It uses several parametric decreasing saturating function families as base models, and we apply MCMC inference to estimate the parameters of the model. Given the existing observations, OpenBox trains a cost model as above and uses it to predict the number of trials at which the curve approaches the optimum. Based on this prediction and the cost of each evaluation, OpenBox estimates the minimal resource needed to reach satisfactory performance (more details in Appendix A.1).

åœ¨äº‘è®¡ç®—çš„å¹¶è¡ŒåŸºç¡€è®¾æ–½çš„è®¾ç½®ä¸‹ï¼ŒèŠ‚çº¦èŠ±è´¹æ˜¯ç”¨æˆ·ä¸€ä¸ªæœ€é‡è¦çš„å…³åˆ‡ã€‚OpenBoxå¯ä»¥å¼•å¯¼ç”¨æˆ·æ¥é…ç½®å…¶èµ„æºï¼Œå¦‚ï¼Œworkersæˆ–æ—¶é—´é¢„ç®—çš„æœ€å°å€¼ï¼Œè¿™è¿›ä¸€æ­¥ä¸ºç”¨æˆ·èŠ‚çº¦äº†ä»£ä»·ã€‚å…·ä½“çš„ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ä¸ªåŠ æƒä»£ä»·æ¨¡å‹ï¼Œæ¥å¤–æ’æ€§èƒ½vs trialæ›²çº¿ã€‚å®ƒä½¿ç”¨å‡ ä¸ªå‚æ•°åŒ–çš„ä¸‹é™çš„é¥±å’Œå‡½æ•°æ—ï¼Œä½œä¸ºåŸºç¡€æ¨¡å‹ï¼Œæˆ‘ä»¬ä½¿ç”¨MCMCæ¨ç†æ¥ä¼°è®¡æ¨¡å‹çš„å‚æ•°ã€‚ç»™å®šç°æœ‰çš„è§‚å¯Ÿï¼ŒOpenBoxè®­ç»ƒäº†ä¸€ä¸ªä»£ä»·æ¨¡å‹ï¼Œç”¨äºé¢„æµ‹æ›²çº¿è¾¾åˆ°æœ€ä¼˜çš„æ•°é‡æ—¶trialsçš„æ•°é‡ã€‚åŸºäºè¿™äº›é¢„æµ‹å’Œæ¯æ¬¡è¯„ä¼°çš„ä»£ä»·ï¼ŒOpenBoxä¼°è®¡éœ€è¦çš„æœ€å°èµ„æºæ¥è¾¾åˆ°å¦ä¸€æ»¡æ„çš„æ€§èƒ½ã€‚

Application Example. Two interesting applications that save expense for users are listed as follows:

åº”ç”¨ä¾‹å­ã€‚ä¸¤ä¸ªæœ‰è¶£çš„ä¸ºç”¨æˆ·èŠ‚çº¦ä»£ä»·çš„åº”ç”¨å¦‚ä¸‹ï¼š

Case 1. Given a fixed number of workers, OpenBox outputs a minimal time budget ğµmin to finish this task based on the estimated evaluation cost of workers. With this estimation, users can stop the task in advance if the given time budget ğµtask > ğµmin; otherwise, users should increase the time budget to ğµmin.

æ¡ˆä¾‹1ã€‚ç»™å®šå›ºå®šæ•°é‡çš„workersï¼ŒOpenBoxè¾“å‡ºäº†ä¸€ä¸ªæœ€å°æ—¶é—´é¢„ç®—Bminï¼Œæ¥å®Œæˆè¿™ä¸ªä»»åŠ¡ï¼ŒåŸºäºä¼°è®¡çš„workersçš„ä»£ä»·è¯„ä¼°ã€‚æœ‰äº†è¿™ä¸ªä¼°è®¡ï¼Œç”¨æˆ·å¯ä»¥æå‰åœæ­¢è¿™ä¸ªä»»åŠ¡ï¼Œå¦‚æœç»™å®šçš„æ—¶é—´é¢„ç®—Btask>Bminï¼›å¦åˆ™ï¼Œç”¨æˆ·åº”å½“å¢åŠ æ—¶é—´é¢„ç®—åˆ°Bminã€‚

Case 2. Given a fixed time budget ğµtask and initial number of workers, OpenBox can suggest the minimal number of workers ğ‘min to finish the current task within ğµtask by adjusting the number of workers to ğ‘min dynamically.

æ¡ˆä¾‹2ã€‚ç»™å®šå›ºå®šçš„æ—¶é—´é¢„ç®—Btaskå’Œåˆå§‹æ•°é‡çš„workersï¼ŒOpenBoxå¯ä»¥å»ºè®®workersçš„æœ€å°æ•°é‡Nminæ¥åœ¨Btaskå†…å®Œæˆç›®å‰çš„ä»»åŠ¡ï¼ŒåŠ¨æ€çš„è°ƒæ•´workersçš„æ•°é‡åˆ°Nminã€‚

### 4.5 Augmented Components in OpenBox

Extensibility and Benchmark Support. OpenBoxâ€™s modular design allows users to define their suggestion algorithms easily by inheriting and implementing an abstract Advisor. The key abstraction method of Advisor is GetSuggestions, which receives the observations of the current task and suggests the next configurations to evaluate based on the user-defined policy. In addition, OpenBox provides a benchmark suite of various BBO problems to benchmark the optimization algorithms.

å¯æ‹“å±•æ€§å’ŒåŸºå‡†æµ‹è¯•æ”¯æŒã€‚OpenBoxçš„æ¨¡å—åŒ–è®¾è®¡ï¼Œä½¿ç”¨æˆ·å¯ä»¥å¾ˆå®¹æ˜“çš„å®šä¹‰å…¶å»ºè®®ï¼Œåªè¦ç»§æ‰¿å’Œå®ç°ä¸€ä¸ªæŠ½è±¡çš„Advisorã€‚Advisorçš„å…³é”®æŠ½è±¡æ–¹æ³•æ˜¯GetSuggestionsï¼Œæ¥æ”¶å½“å‰ä»»åŠ¡çš„è§‚å¯Ÿï¼Œæ ¹æ®ç”¨æˆ·å®šä¹‰çš„ç­–ç•¥æ¥å»ºè®®è¦è¯„ä¼°çš„ä¸‹ä¸€ä¸ªé…ç½®ã€‚æ­¤å¤–ï¼ŒOpenBoxç»™å‡ºäº†å„ç§BBOé—®é¢˜çš„ä¸€ä¸ªåŸºå‡†æµ‹è¯•åŒ…ï¼Œä»¥æµ‹è¯•ä¼˜åŒ–ç®—æ³•ã€‚

Data Privacy Protection. In some scenarios, the names and ranges of parameters are sensitive, e.g., in hyper-parameter tuning, the parameter names may reveal the architecture details of neural networks. To protect data privacy, the REST API applies a transformation to anonymize the parameter-related information before sending it to the service. This transformation involves 1) converting the parameter names to some regular ones like â€œparam1â€ and 2) rescaling each parameter to a default range that has no semantic. The workers can perform an inverse transformation when receiving an anonymous configuration from the service.

æ•°æ®éšç§ä¿æŠ¤ã€‚åœ¨ä¸€äº›åœºæ™¯ä¸­ï¼Œå‚æ•°çš„åç§°å’ŒèŒƒå›´æ˜¯æ•æ„Ÿçš„ï¼Œå¦‚ï¼Œåœ¨è¶…å‚æ•°è°ƒèŠ‚ä¸­ï¼Œå‚æ•°åç§°å¯èƒ½ä¼šæš´éœ²ç¥ç»ç½‘ç»œçš„æ¶æ„ç»†èŠ‚ã€‚ä¸ºä¿æŠ¤æ•°æ®éšç§ï¼ŒREST APIåº”ç”¨äº†ä¸€ä¸ªå˜æ¢ï¼Œæ¥å°†å‚æ•°ç›¸å…³çš„ä¿¡æ¯åŒ¿ååŒ–ï¼Œç„¶åå†é€åˆ°æœåŠ¡ä¸­å»ã€‚è¿™ç§å˜æ¢åŒ…æ¢åŒ…æ‹¬ï¼Œ1)å°†å‚æ•°åç§°å˜æ¢åˆ°ä¸€äº›å¸¸è§„åç§°ï¼Œå¦‚param1ï¼Œ2)å°†æ¯ä¸ªå‚æ•°é‡æ–°ç¼©æ”¾åˆ°ä¸€ä¸ªé»˜è®¤çš„èŒƒå›´ï¼Œæ²¡æœ‰ä»»ä½•è¯­ä¹‰ã€‚workersåœ¨ä»æœåŠ¡ä¸­æ¥æ”¶åˆ°åŒ¿åé…ç½®åï¼Œè¿›è¡Œä¸€ä¸ªé€†å˜æ¢ã€‚

Visualization. OpenBox provides an online dashboard based on TensorBoardX which enables users to monitor the optimization process and check the evaluation info of the current task. Figure 4 visualizes the evaluation results in a hyper-parameter tuning task.

å¯è§†åŒ–ã€‚OpenBoxç»™å‡ºäº†ä¸€ä¸ªåŸºäºTensorFlowBoardXçš„åœ¨çº¿çš„ä»ªè¡¨ç›˜ï¼Œä½¿ç”¨æˆ·å¯ä»¥ç›‘æ§ä¼˜åŒ–è¿‡ç¨‹ï¼Œæ£€æŸ¥å½“å‰ä»»åŠ¡çš„è¯„ä¼°ä¿¡æ¯ã€‚å›¾4å¯¹è¶…å‚æ•°è°ƒèŠ‚ä»»åŠ¡çš„è¯„ä¼°ç»“æœè¿›è¡Œäº†å¯è§†åŒ–ã€‚

## 5. System Optimizations

### 5.1 Local Penalization based Parallelization

Most proposed Bayesian optimization (BO) approaches only allow the exploration of the parameter space to occur sequentially. To fully utilize the computing resources in a parallel infrastructure, we provide a mechanism for distributed parallelization, where multiple configurations can be evaluated concurrently across workers. Two parallel settings are considered (see Figure 5):

å¤šæ•°æå‡ºçš„è´å¶æ–¯ä¼˜åŒ–æ–¹æ³•åªå…è®¸é¡ºåºæ¢ç´¢å‚æ•°ç©ºé—´ã€‚ä¸ºå®Œå…¨åˆ©ç”¨è®¡ç®—èµ„æºçš„å¹¶è¡ŒåŸºç¡€è®¾æ–½ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ç§æœºåˆ¶è¿›è¡Œåˆ†å¸ƒå¼å¹¶è¡ŒåŒ–ï¼Œå…¶ä¸­å¯ä»¥åœ¨å¤šä¸ªworkersä¸­åŒæ—¶è¯„ä¼°å¤šä¸ªé…ç½®ã€‚è€ƒè™‘äº†ä¸¤ç§å¹¶è¡Œçš„è®¾ç½®ï¼ˆè§å›¾5ï¼‰ï¼š

1) Synchronous parallel setting. The worker pulls new configuration from suggestion server to evaluate until all the workers have finished their last evaluations.

åŒæ­¥å¹¶è¡Œè®¾ç½®ã€‚workersä»å»ºè®®æœåŠ¡å™¨ä¸Šæ‹‰ä¸‹æ¥æ–°çš„é…ç½®è¿›è¡Œè¯„ä¼°ï¼Œç›´åˆ°æ‰€æœ‰workerså®Œæˆå…¶æœ€åçš„è¯„ä¼°ã€‚

2) Asynchronous parallel setting. The worker pulls a new configuration when the previous evaluation is completed.

å¼‚æ­¥å¹¶è¡Œè®¾ç½®ã€‚workeråœ¨å‰ä¸€ä¸ªè¯„ä¼°å®Œæˆåï¼Œå°±æ‹‰ä¸‹æ¥æ–°çš„é…ç½®ã€‚

Our main concern is to design an algorithm-agnostic mechanism that can parallelize the optimization algorithms under the sync and async settings easily, so we do not need to implement the parallel version for each algorithm individually. To this end, we propose a local penalization based parallelization mechanism, the goal of which is to sample new configurations that are promising and far enough from the configurations being evaluated by other workers. This mechanism can handle the well-celebrated exploration vs. exploitation trade-off, and meanwhile prevent workers from exploring similar configurations. Algorithm 1 gives the pseudo-code of sampling a new configuration under the sync/async settings. More discussion about this is provided in Appendix A.4.

æˆ‘ä»¬çš„ä¸»è¦è€ƒè™‘æ˜¯ï¼Œè®¾è®¡ä¸€ä¸ªç®—æ³•æ— å…³çš„æœºåˆ¶ï¼Œå¯ä»¥åœ¨syncå’Œasyncçš„è®¾ç½®ä¸‹ï¼Œå¾ˆå®¹æ˜“çš„å¹¶è¡ŒåŒ–ä¼˜åŒ–ç®—æ³•ï¼Œè¿™æ ·æˆ‘ä»¬å°±ä¸éœ€è¦å•ç‹¬å®ç°æ¯ä¸ªç®—æ³•çš„å¹¶è¡Œç‰ˆæœ¬ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§åŸºäºå±€éƒ¨æƒ©ç½šçš„å¹¶è¡Œæœºåˆ¶ï¼Œå…¶ç›®çš„æ˜¯ï¼Œé‡‡æ ·æ–°çš„æœ‰å¸Œæœ›çš„é…ç½®ï¼Œä¸å…¶ä»–workersè¯„ä¼°çš„é…ç½®è·ç¦»å¾ˆè¿œã€‚è¿™ç§æœºåˆ¶å¯ä»¥å¤„ç†è‘—åçš„æ¢ç´¢vsåˆ©ç”¨æŠ˜ä¸­ï¼ŒåŒæ—¶é˜²æ­¢workersæ¢ç´¢ç±»ä¼¼çš„é…ç½®ã€‚ç®—æ³•1ç»™å‡ºäº†åœ¨sync/asyncçš„é…ç½®ä¸‹é‡‡æ ·ä¸€ä¸ªæ–°é…ç½®çš„ä¼ªä»£ç ã€‚æ›´å¤šçš„è®¨è®ºåœ¨é™„å½•A.4ä¸­ç»™å‡ºã€‚

### 5.2 General Transfer-Learning Framework

When performing BBO, users often run tasks that are similar to previous ones. This fact can be used to speed up the current task. Compared with Vizier, which only provides limited transfer learning functionality for single-objective BBO problems, OpenBox employs a general transfer learning framework with the following advantages: 1) support for the generalized black-box optimization problems, and 2) compatibility with most BO methods.

å½“è¿›è¡ŒBBOæ—¶ï¼Œç”¨æˆ·è¿è¡Œçš„ä»»åŠ¡é€šå¸¸ä¼šä¸ä¹‹å‰çš„ä»»åŠ¡ç±»ä¼¼ã€‚è¿™ä¸ªäº‹å®å¯ä»¥ç”¨äºåŠ é€Ÿå½“å‰çš„ä»»åŠ¡ã€‚Vizierå¯¹å•ç›®æ ‡BBOé—®é¢˜æä¾›çš„è¿ç§»å­¦ä¹ åŠŸèƒ½å¾ˆæœ‰é™ï¼ŒOpenBoxé‡‡ç”¨äº†ä¸€ç§é€šç”¨çš„è¿ç§»å­¦ä¹ æ¡†æ¶ï¼Œæœ‰ä»¥ä¸‹ä¼˜åŠ¿ï¼š1)æ”¯æŒé€šç”¨BBOé—®é¢˜ï¼Œ2)ä¸å¤šæ•°BOæ–¹æ³•å…¼å®¹ã€‚

OpenBox takes as input observations from ğ¾ + 1 tasks: ğ·^1, ..., ğ·^ğ¾ for ğ¾ previous tasks and ğ·^ğ‘‡ for the current task. Each ğ·^ğ‘– ={(ğ’™^ğ‘–_ğ‘—, ğ’š^ğ‘–_ğ‘—)}^{ğ‘›_ğ‘–}_{ğ‘—=1}, ğ‘– = 1, ..., ğ¾, includes a set of observations. Note that, ğ’š is an array, including multiple objectives for configuration ğ’™.

OpenBoxä»¥K+1ä¸ªä»»åŠ¡çš„è§‚å¯Ÿä¸ºè¾“å…¥ï¼šğ·^1, ..., ğ·^ğ¾ï¼Œè¿™æ˜¯Kä¸ªä¹‹å‰çš„ä»»åŠ¡ï¼Œğ·^ğ‘‡æ˜¯å½“å‰çš„ä»»åŠ¡ã€‚æ¯ä¸ªğ·^ğ‘– ={(ğ’™^ğ‘–_ğ‘—, ğ’š^ğ‘–_ğ‘—)}^{ğ‘›_ğ‘–}_{ğ‘—=1}, ğ‘– = 1, ..., ğ¾ï¼ŒåŒ…å«ä¸€ä¸ªè§‚å¯Ÿé›†åˆã€‚æ³¨æ„ï¼Œğ’šæ˜¯ä¸€ä¸ªé˜µåˆ—ï¼ŒåŒ…å«é…ç½®ğ’™çš„å¤šä¸ªç›®æ ‡ã€‚

For multi-objective problems with ğ‘ objectives, we propose to transfer the knowledge about ğ‘ objectives individually. Thus, the transfer learning of multiple objectives is turned into ğ‘ single-objective transfer learning processes. For each dimension of the objectives, we take RGPE [14] as the base method. 1) We first train a surrogate model ğ‘€^ğ‘– on ğ·^ğ‘– for the ğ‘–-ğ‘¡â„ prior task and ğ‘€^ğ‘‡ on ğ·^ğ‘‡; based on ğ‘€^{1:ğ¾} and ğ‘€^ğ‘‡, 2) we then build a transfer learning surrogate by combining all base surrogates:

å¯¹ğ‘ä¸ªç›®æ ‡çš„å¤šç›®æ ‡é—®é¢˜ï¼Œæˆ‘ä»¬æå‡ºé€ä¸ªè¿ç§»pä¸ªç›®æ ‡çš„çŸ¥è¯†ã€‚å› æ­¤ï¼Œå¤šç›®æ ‡è¿ç§»å­¦ä¹ å°±è½¬å˜æˆäº†pä¸ªå•ç›®æ ‡çš„è¿ç§»å­¦ä¹ è¿‡ç¨‹ã€‚å¯¹ç›®æ ‡çš„æ¯ä¸ªç»´åº¦ï¼Œæˆ‘ä»¬ä»¥RGPE[14]ä¸ºåŸºå‡†æ–¹æ³•ã€‚1)æˆ‘ä»¬é¦–å…ˆåœ¨ğ·^ğ‘–ä¸Šå¯¹ç¬¬iä¸ªå…ˆéªŒä»»åŠ¡è®­ç»ƒä¸€ä¸ªä»£ç†æ¨¡å‹ğ‘€^ğ‘–ï¼Œå¹¶åœ¨ğ·^ğ‘‡ä¸Šè®­ç»ƒğ‘€^ğ‘‡ï¼ŒåŸºäºğ‘€^{1:ğ¾}å’Œğ‘€^ğ‘‡ï¼Œ2)æˆ‘ä»¬ç„¶åæ„å»ºä¸€ä¸ªè¿ç§»å­¦ä¹ ä»£ç†ï¼Œå°†æ‰€æœ‰è¿™äº›ä»£ç†ç»“åˆåˆ°ä¸€èµ·

$$ğ‘€^{TL} = agg({ğ‘€^1, ..., ğ‘€^ğ¾, ğ‘€^ğ‘‡}; w)$$

3) the surrogate ğ‘€^{TL} is used to guide the configuration search, instead of the original ğ‘€^ğ‘‡. Concretely, we combine the multiple base surrogates (agg) linearly, and the parameters w are calculated based on the ranking of configurations, which reflects the similarity between the source and target task (see details in Appendix A.3).

ä»£ç†ğ‘€^{TL}ç”¨äºæ›¿ä»£åŸå§‹çš„ğ‘€^ğ‘‡å¼•å¯¼é…ç½®æœç´¢ã€‚å…·ä½“çš„ï¼Œæˆ‘ä»¬å°†å¤šä¸ªåŸºç¡€ä»£ç†aggçº¿æ€§çš„ç»“åˆèµ·æ¥ï¼Œå‚æ•°wæ˜¯åŸºäºé…ç½®çš„rankingæ¥è®¡ç®—å¾—åˆ°çš„ï¼Œååº”äº†æºä»»åŠ¡å’Œç›®æ ‡ä»»åŠ¡çš„ç›¸ä¼¼æ€§ã€‚

Scalability discussion A more intuitive alternative is to obtain a transfer learning surrogate by using all observations from ğ¾ + 1 tasks, and this incurs a complexity of O(ğ‘˜^3ğ‘›^3) for ğ‘˜ tasks with ğ‘› trials each (since GP has O(ğ‘›^3) complexity). Therefore, it is hard to scale to a larger number of source tasks (a large ğ‘˜). By training base surrogates individually, the proposed framework is a more computation-efficient solution that has O(ğ‘˜ğ‘›^3) complexity.

å¯æ‰©å±•æ€§è®¨è®ºã€‚ä¸€ä¸ªæ›´ç›´è§‚çš„æ›¿ä»£æ˜¯ï¼Œä½¿ç”¨K+1ä¸ªä»»åŠ¡çš„æ‰€æœ‰è§‚å¯Ÿï¼Œå¾—åˆ°ä¸€ä¸ªè¿ç§»å­¦ä¹ ä»£ç†ï¼Œå¯¹äºkä¸ªä»»åŠ¡ï¼Œæ¯ä¸ªä»»åŠ¡næ¬¡trialï¼Œè¿™å¸¦æ¥çš„å¤æ‚åº¦ä¸ºO(ğ‘˜^3ğ‘›^3)ã€‚å› æ­¤ï¼Œå¾ˆéš¾å¯¹å¤§é‡çš„æºä»»åŠ¡è¿›è¡Œç¼©æ”¾ã€‚é€šè¿‡å•ç‹¬è®­ç»ƒåŸºç¡€ä»£ç†ï¼Œæå‡ºçš„æ¡†æ¶åœ¨è®¡ç®—ä¸Šæ›´åŠ é«˜æ•ˆï¼Œå¤æ‚åº¦ä¸ºO(ğ‘˜ğ‘›^3)ã€‚

### 5.3 Additional Optimizations

OpenBox also includes two additional optimizations that can be applied to improve the efficiency of black-box optimizations.

OpenBoxè¿˜æœ‰ä¸¤ä¸ªé¢å¤–çš„ä¼˜åŒ–ï¼Œå¯ä»¥ç”¨äºæ”¹è¿›BBOçš„æ•ˆç‡ã€‚

5.3.1 Multi-Fidelity Support and Applications. During each evaluation in the multi-fidelity setting [33, 41], the worker receives an additional parameter, indicating how many resources are used to evaluate this configuration. The resource type needs to be specified by users. For example, in hyper-parameter tuning, it can be the number of iterations for an iterative algorithm and the size of dataset subset. The trial with partial resource returns a low-fidelity result with a cheap evaluation cost. Though not as precise as high-fidelity results, the low-fidelity results can provide some useful information to guide the configuration search. In OpenBox, we have implemented several multi-fidelity algorithms, such as MFES-HB [33].

å¤šä¿çœŸåº¦æ”¯æŒå’Œåº”ç”¨ã€‚åœ¨å¤šä¿çœŸåº¦è®¾ç½®ä¸‹ï¼Œåœ¨æ¯æ¬¡è¯„ä¼°è¿‡ç¨‹ä¸­ï¼Œworkeræ”¶åˆ°ä¸€ä¸ªé¢å¤–çš„å‚æ•°ï¼Œè¡¨æ˜ç”¨äº†å¤šå°‘èµ„æºæ¥è¯„ä¼°è¿™ä¸ªé…ç½®ã€‚èµ„æºç±»å‹è¦ç”±ç”¨æˆ·æŒ‡å®šã€‚æ¯”å¦‚ï¼Œåœ¨è¶…å‚æ•°è°ƒèŠ‚ä¸­ï¼Œå¯ä»¥æ˜¯è¿­ä»£ç®—æ³•çš„è¿­ä»£æ¬¡æ•°ï¼Œå’Œæ•°æ®é›†å­é›†çš„å¤§å°ã€‚ç”¨éƒ¨åˆ†èµ„æºçš„trialï¼Œä¼šè¿”å›ä¸€ä¸ªä½ä¿çœŸåº¦çš„ç»“æœï¼Œè¯„ä¼°ä»£ä»·ä¹Ÿæ˜¯è¾ƒå»‰ä»·çš„ã€‚è™½ç„¶ä¸åƒé«˜ä¿çœŸåº¦çš„ç»“æœé‚£ä¹ˆç²¾ç¡®ï¼Œä½ä¿çœŸåº¦çš„ç»“æœä¼šæä¾›ä¸€äº›æœ‰ç”¨çš„ä¿¡æ¯ï¼Œæ¥å¼•å¯¼é…ç½®æœç´¢ã€‚åœ¨OpenBoxä¸­ï¼Œæˆ‘ä»¬å®ç°äº†å‡ ä¸ªå¤šä¿çœŸåº¦ç®—æ³•ï¼Œå¦‚MFES-HBã€‚

5.3.2 Early-Stopping Strategy. Orthogonal to the above optimization, early-stopping strategies aim to stop a poor trial in advance based on its intermediate results. In practice, a worker can periodically ask suggestion service whether it should terminate the current evaluation early. In OpenBox, we provide two early-stopping strategies: 1) learning curve extrapolation based methods [9, 28] that stop the poor configurations by estimating the future performance, and 2) mean or median termination rules based on comparing the current result with previous ones.

æ—©åœç­–ç•¥ã€‚ä¸ä¸Šé¢çš„ä¼˜åŒ–åŒæ—¶ï¼Œæ—©åœçš„ç­–ç•¥çš„ç›®æ ‡æ˜¯ï¼ŒåŸºäºå…¶ä¸­é—´ç»“æœï¼Œæå‰åœæ­¢ä¸€ä¸ªå¾ˆå·®çš„trialã€‚åœ¨å®è·µä¸­ï¼Œworkerä¼šå‘¨æœŸæ€§çš„å‘å»ºè®®æœåŠ¡ä¸­æŸ¥è¯¢ï¼Œæ˜¯å¦åº”å½“æ—©åœå½“å‰çš„è¯„ä¼°ã€‚åœ¨OpenBoxä¸­ï¼Œæˆ‘ä»¬æä¾›äº†ä¸¤ä¸ªæ—©åœç­–ç•¥ï¼š1)å­¦ä¹ åŸºäºæ›²çº¿å¤–æ’æ–¹æ³•ï¼Œé€šè¿‡ä¼°è®¡æœªæ¥çš„æ€§èƒ½ï¼Œæ¥åœæ­¢å¾ˆå·®çš„é…ç½®ï¼›2)å°†å½“å‰çš„ç»“æœä¸ä¹‹å‰çš„è¿›è¡Œå¯¹æ¯”ï¼Œå¹³å‡æˆ–ä¸­å€¼åœæ­¢å‡†åˆ™ã€‚

## 6. Experimental Evaluation

In this section, we compare the performance and efficiency of OpenBox against existing software packages on multiple kinds of blackbox optimization tasks, including tuning tasks in AutoML.

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬ä¸ç°æœ‰çš„è½¯ä»¶åŒ…è¿›è¡Œå¯¹æ¯”ï¼Œåœ¨å¤šç§é»‘ç®±ä¼˜åŒ–ä»»åŠ¡ä¸Šï¼Œæ¯”è¾ƒäº†OpenBoxçš„æ€§èƒ½å’Œæ•ˆç‡ï¼ŒåŒ…æ‹¬AutoMLä¸­çš„è°ƒèŠ‚ä»»åŠ¡ã€‚

### 6.1 Experimental Setup

6.1.1 Baselines. Besides the systems mentioned in Table 1, we also use CMA-ES [23], Random Search and 2Ã—Random Search (Random Search with double budgets) as baselines. To evaluate transfer learning, we compare OpenBox with Google Vizier. For multi-fidelity experiments, we compare OpenBox against HpBandSter and BOHB, the details of which are in Appendix A.5.

åŸºå‡†ã€‚é™¤äº†è¡¨1ä¸­æè¿°çš„ç³»ç»Ÿï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨CMA-ESï¼Œéšæœºæœç´¢å’Œ2Ã—éšæœºæœç´¢ï¼ˆåŒå€é¢„ç®—çš„éšæœºæœç´¢ï¼‰ä½œä¸ºåŸºå‡†ã€‚ä¸ºè¯„ä¼°è¿ç§»å­¦ä¹ ï¼Œæˆ‘ä»¬å°†OpenBoxä¸Google Vizierè¿›è¡Œäº†æ¯”è¾ƒã€‚å¯¹å¤šä¿çœŸåº¦è¯•éªŒï¼Œæˆ‘ä»¬æ¯”è¾ƒå°†OpenBoxä¸HpBandSterå’ŒBOHBè¿›è¡Œäº†æ¯”è¾ƒï¼Œè¯¦è§é™„å½•A.5ã€‚

6.1.2 Problems. We use 12 black-box problems (mathematical functions) from [50] and two AutoML optimization problems on 25 OpenML datasets. In particular, 2d-Branin, 2d-Beale, 6d-Hartmann and (2d, 4d, 8d, 16d, 32d)-Ackley are used for single-objective optimization; 2d-Townsend, 2d-Mishra, 4d-Ackley and 10d-Keane are used for constrained single-objective optimization; 3d-ZDT2 with two objectives and 6d-DTLZ1 with five objectives are used for multi-objective optimization; 2d-CONSTR and 2d-SRN with two objectives are used for constrained multi-objective optimization. All the parameters for mathematical problems are of the FLOAT type and the maximum trials of each problem depend on its difficulty, which ranges from 80 to 500. For AutoML problems on 25 datasets, we split each dataset and search for the configuration with the best validation performance. Specifically, we tune LightGBM and LibSVM with the linear kernel, where the parameters of LightGBM are of the FLOAT type while LibSVM contains CATEGORICAL and conditioned parameters.

é—®é¢˜ã€‚æˆ‘ä»¬ä½¿ç”¨[50]ä¸­çš„12ä¸ªé»‘ç®±é—®é¢˜ï¼ˆæ•°å­¦å‡½æ•°ï¼‰ï¼Œå’Œåœ¨25ä¸ªOpenMLæ•°æ®é›†ä¸­çš„ä¸¤ä¸ªAutoMLä¼˜åŒ–é—®é¢˜ã€‚ç‰¹åˆ«æ˜¯ï¼Œ2d-Branin, 2d-Beale, 6d-Hartmannå’Œ(2d, 4d, 8d, 16d, 32d)-Ackleyç”¨äºå•ç›®æ ‡ä¼˜åŒ–ï¼›2d-Townsend, 2d-Mishra, 4d-Ackleyå’Œ10d-Keaneç”¨äºçº¦æŸå•ç›®æ ‡ä¼˜åŒ–ï¼›å¸¦æœ‰2ä¸ªç›®æ ‡çš„3d-ZDT2ï¼Œå’Œ5ä¸ªç›®æ ‡çš„6d-DTLZ1ï¼Œç”¨äºå¤šç›®æ ‡ä¼˜åŒ–ï¼›ä¸¤ä¸ªç›®æ ‡çš„2d-CONSTRå’Œ2d-SRNç”¨äºçº¦æŸå¤šç›®æ ‡ä¼˜åŒ–ã€‚æ•°å­¦é—®é¢˜çš„æ‰€æœ‰å‚æ•°éƒ½æ˜¯FLOATç±»å‹ï¼Œæ¯ä¸ªé—®é¢˜çš„æœ€å¤§trialsæ¬¡æ•°ä¾èµ–äºå…¶éš¾åº¦ï¼ŒèŒƒå›´åœ¨80åˆ°500ã€‚å¯¹äºåœ¨25ä¸ªæ•°æ®é›†ä¸Šçš„AutoMLé—®é¢˜ï¼Œæˆ‘ä»¬å°†æ¯ä¸ªæ•°æ®é›†åˆ†å‰²ï¼Œæœç´¢æœ€ä½³éªŒè¯æ€§èƒ½çš„é…ç½®ã€‚å…·ä½“çš„ï¼Œæˆ‘ä»¬ç”¨çº¿æ€§æ ¸æ¥è°ƒèŠ‚LightGBMå’ŒLibSVMï¼Œå…¶ä¸­LightGBMçš„å‚æ•°æ˜¯FLOATç±»å‹çš„ï¼Œè€ŒLibSVMåŒ…å«CATEGORICALå’Œæœ‰æ¡ä»¶çš„å‚æ•°ã€‚

6.1.3 Metrics. We employ the three metrics as follows. åº¦é‡ã€‚æˆ‘ä»¬é‡‡ç”¨ä¸‹é¢ä¸‰ç§åº¦é‡ï¼š

1. Optimality gap is used for single-objective mathematical problem. That is, if ğ‘¥âˆ— optimizes ğ‘“, and \hat ğ‘¥ is the best configuration found by the method, then |ğ‘“(\hat ğ‘¥) âˆ’ ğ‘“(ğ‘¥âˆ—)| measures the success of the method on that function. In rare cases, we report the objective value if the ground-truth optimal ğ‘¥âˆ— is extremely hard to obtain.

ä¼˜åŒ–æ€§ç©ºç™½ï¼Œç”¨äºå•ç›®æ ‡æ•°å­¦é—®é¢˜ã€‚å³ï¼Œå¦‚æœğ‘¥âˆ—ä½¿fæœ€ä¼˜åŒ–ï¼Œ\hat xæ˜¯ç®—æ³•å‘ç°çš„æœ€ä¼˜é…ç½®ï¼Œé‚£ä¹ˆ|ğ‘“(\hat ğ‘¥) âˆ’ ğ‘“(ğ‘¥âˆ—)|åº¦é‡çš„æ˜¯è¯¥ç®—æ³•åœ¨è¿™ä¸ªå‡½æ•°ä¸Šçš„æˆåŠŸç¨‹åº¦ã€‚åœ¨å¾ˆå°‘çš„æƒ…å†µä¸­ï¼Œå¦‚æœçœŸå€¼æœ€ä¼˜ğ‘¥âˆ—æå…¶éš¾ä»¥å¾—åˆ°ï¼Œæˆ‘ä»¬ç»™å‡ºç›®æ ‡å€¼ã€‚

2. Hypervolume indicator given a reference point ğ’“ measures the quality of a Pareto front in multi-objective problems. We report the difference between the hypervolume of the ideal Pareto front Pâˆ— and that of the estimated Pareto front P by a given algorithm, which is ğ»ğ‘‰ (Pâˆ—,ğ’“) âˆ’ ğ»ğ‘‰ (P,ğ’“).

Hypervolume indicatorï¼Œåœ¨ç»™å®šä¸€ä¸ªå‚è€ƒç‚¹ğ’“ï¼Œåº¦é‡çš„æ˜¯å¤šç›®æ ‡é—®é¢˜ä¸­çš„Pareto frontçš„è´¨é‡ã€‚æˆ‘ä»¬ç”¨ä¸€ä¸ªç»™å®šçš„ç®—æ³•ï¼Œè¡¡é‡å‡ºç†æƒ³Pareto front P*å’Œä¼°è®¡çš„Pareto front Pä¹‹é—´çš„hypervolumeçš„å·®å¼‚ï¼Œå³ğ»ğ‘‰ (Pâˆ—,ğ’“) âˆ’ ğ»ğ‘‰ (P,ğ’“)ã€‚

3. Metric for AutoML. For single-objective AutoML problems, we report the validation error. To measure the results across different datasets, we use Rank as the metric.

AutoMLçš„åº¦é‡ã€‚å¯¹äºå•ç›®æ ‡AutoMLé—®é¢˜ï¼Œæˆ‘ä»¬ç»™å‡ºéªŒè¯è¯¯å·®ã€‚ä¸ºåº¦é‡ä¸åŒæ•°æ®é›†ä¹‹é—´çš„ç»“æœï¼Œæˆ‘ä»¬ä½¿ç”¨Randä½œä¸ºåº¦é‡ã€‚

6.1.4 Parameter Settings. For both OpenBox and the considered baselines, we use the default setting. Each experiment is repeated 10 times, and we compute the mean and variance for visualization.

å‚æ•°è®¾ç½®ã€‚å¯¹äºOpenBoxå’Œè€ƒè™‘çš„åŸºå‡†ï¼Œæˆ‘ä»¬ä½¿ç”¨é»˜è®¤è®¾ç½®ã€‚æ¯ä¸ªè¯•éªŒé‡å¤10æ¬¡ï¼Œæˆ‘ä»¬è®¡ç®—å‡ºå‡å€¼å’Œæ–¹å·®ï¼Œä»¥è¿›è¡Œå¯è§†åŒ–ã€‚

### 6.2 Results and Analysis

6.2.1 Single-Objective Problems without Constraints. Figure 6 illustrates the results of OpenBox on different single-objective problems compared with competitive baselines while Figure 7 displays the performance with the growth of input dimensions. In particular, Figure 6 shows that OpenBox, HyperMapper and BoTorch are capable of optimizing these low-dimensional functions stably. However, when the dimensions of the parameter space grow larger, as shown in Figure 7, only OpenBox achieves consistent and excellent results while the other baselines fail, which demonstrates its scalability on input dimensions. Note that, OpenBox achieves more than 10-fold speedups over the baselines when solving Ackley with 16 and 32-dimensional inputs.

æ²¡æœ‰çº¦æŸçš„å•ç›®æ ‡é—®é¢˜ã€‚å›¾6ç»™å‡ºäº†OpenBoxåœ¨ä¸åŒçš„å•ç›®æ ‡é—®é¢˜ä¸Šçš„ç»“æœï¼Œå¹¶ä¸ä¸€äº›åŸºå‡†è¿›è¡Œäº†å¯¹æ¯”ï¼›å›¾7ç»™å‡ºäº†éšç€è¾“å…¥ç»´åº¦å¢é•¿ï¼Œæ€§èƒ½çš„å˜åŒ–ã€‚ç‰¹åˆ«æ˜¯ï¼Œå›¾6å±•ç¤ºäº†OpenBoxï¼ŒHyperMapperï¼Œå’ŒBoTorchå¯ä»¥å¾ˆç¨³å®šçš„ä¼˜åŒ–ä½ç»´é—®é¢˜ã€‚ä½†æ˜¯ï¼Œå½“å‚æ•°ç©ºé—´çš„ç»´åº¦å˜å¾—è¶Šæ¥è¶Šå¤§ï¼Œå¦‚å›¾7æ‰€ç¤ºï¼Œåªæœ‰OpenBoxè·å¾—äº†ä¸€è‡´çš„ä¼˜ç§€ç»“æœï¼Œè€Œå…¶ä»–åŸºå‡†åˆ™æ²¡æœ‰ï¼Œè¿™è¯´æ˜äº†OpenBoxå¯¹è¾“å…¥ç»´åº¦çš„å¯æ‰©å±•æ€§ã€‚æ³¨æ„ï¼ŒOpenBoxåœ¨å¤„ç†16ç»´å’Œ32ç»´çš„Ackleyé—®é¢˜æ—¶ï¼Œæ¯”åŸºå‡†è·å¾—äº†è¶…è¿‡10å€çš„åŠ é€Ÿã€‚

6.2.2 Single-Objective Problems with Constraints. Figure 8 shows the results of OpenBox along with the baselines on four constrained single-objective problems. Besides Random Search, we compare OpenBox with three of the software packages that support constraints. OpenBox surpasses all the considered baselines on the convergence result. Note that on the 10-dimensional Keane problem in which the ground-truth optimal value is hard to locate, OpenBox is the only method that successfully optimizes this function while the other methods fail to suggest sufficient feasible configurations.

æœ‰çº¦æŸçš„å•ç›®æ ‡é—®é¢˜ã€‚å›¾8å±•ç¤ºäº†OpenBoxä¸åŸºå‡†åœ¨4ä¸ªæœ‰çº¦æŸå•ç›®æ ‡é—®é¢˜ä¸Šçš„ç»“æœã€‚é™¤äº†éšæœºæœç´¢ï¼Œæˆ‘ä»¬æ¯”è¾ƒOpenBoxä¸å…¶ä»–ä¸‰ä¸ªæ”¯æŒçº¦æŸçš„è½¯ä»¶åŒ…ã€‚OpenBoxåœ¨æ”¶æ•›ç»“æœä¸Šè¶…è¿‡äº†æ‰€æœ‰è€ƒè™‘çš„åŸºå‡†ã€‚æ³¨æ„åœ¨10ç»´Keaneé—®é¢˜ä¸Šï¼Œå…¶ä¸­çœŸå€¼æœ€ä¼˜å€¼å¾ˆéš¾å®šä½ï¼ŒOpenBoxæ˜¯å”¯ä¸€çš„ä¸€ç§æˆåŠŸçš„æœ€ä¼˜åŒ–äº†è¿™ä¸ªå‡½æ•°çš„æ–¹æ³•ï¼Œè€Œå…¶ä»–æ–¹æ³•æ²¡æœ‰å¾—åˆ°è¶³å¤Ÿå¯è¡Œçš„é…ç½®ã€‚

6.2.3 Multi-Objective Problems without Constraints. We compare OpenBox with three baselines that support multiple objectives and the results are depicted in Figure 9(a) and 9(b). In Figure 9(a), the hypervolume difference of GPflowOpt and Hypermapper decreases slowly as the number of trials grow, while BoTorch and OpenBox obtain a satisfactory Pareto Front quickly within 50 trials. In Figure 9(b) where the number of objectives is 5, BoTorch meets the bottleneck of optimizing the Pareto front while OpenBox tackles this problem easily by switching its inner algorithm from EHVI to MESMO; GPflowOpt is missing due to runtime errors.

æ²¡æœ‰çº¦æŸçš„å¤šç›®æ ‡é—®é¢˜ã€‚æˆ‘ä»¬æ¯”è¾ƒOpenBoxä¸ä¸‰ä¸ªæ”¯æŒå¤šç›®æ ‡çš„åŸºå‡†ï¼Œç»“æœå¦‚å›¾9aå’Œ9bæ‰€ç¤ºã€‚åœ¨å›¾9aä¸­ï¼ŒGPflowOptå’ŒHyperMapperçš„hypervolumeå·®å¼‚éšç€trialsçš„æ•°é‡å¢åŠ ï¼Œé€æ­¥ç¼“æ…¢é™ä½ï¼Œè€ŒBoTorchå’ŒOpenBoxåˆ™å¾ˆå¿«çš„åœ¨50ä¸ªtrialsä»¥å†…å¾—åˆ°äº†ä¸€ä¸ªæ»¡æ„çš„Pareto Frontã€‚åœ¨å›¾9bä¸­ï¼Œç›®æ ‡çš„æ•°é‡ä¸º5ï¼ŒBoTorché‡åˆ°äº†ä¼˜åŒ–Pareto frontçš„ç“¶é¢ˆï¼Œè€ŒOpenBoxåˆ™å¾ˆå®¹æ˜“çš„å¤„ç†è¿™ä¸ªé—®é¢˜ï¼Œå°†å…¶å†…éƒ¨ç®—æ³•ä»EVHIåˆ‡æ¢åˆ°äº†MESMOï¼›GPflowOptç”±äºæœ‰è¿è¡Œæ—¶é”™è¯¯ï¼Œæ²¡æœ‰ç”»å‡ºã€‚

6.2.4 Multi-Objective Problems with Constraints. We compare OpenBox with Hypermapper and BoTorch on constrained multi-objective problems (See Figure 9(c) and 9(d)). Figure 9(c) demonstrates the performance on a simple problem, in which the convergence result of OpenBox is slightly better than the other two baselines. However, in Figure 9(d) where the constraints are strict, BoTorch and Hypermapper fail to suggest sufficient feasible configurations to update the Pareto Front. Compared with BoTorch and Hypermapper, OpenBox has more stable performance when solving multi-objective problems with constraints.

å¸¦æœ‰çº¦æŸçš„å¤šç›®æ ‡é—®é¢˜ã€‚æˆ‘ä»¬åœ¨å¸¦çº¦æŸçš„å¤šç›®æ ‡é—®é¢˜ä¸Šæ¯”è¾ƒOpenBoxä¸HyperMapperå’ŒBoTorchï¼Œå¦‚å›¾9cå’Œ9dæ‰€ç¤ºã€‚å›¾9cåœ¨ç®€å•é—®é¢˜ä¸Šå±•ç¤ºäº†æ€§èƒ½ï¼Œå…¶ä¸­OpenBoxçš„æ”¶æ•›ç»“æœæ¯”å…¶ä»–ä¸¤ä¸ªåŸºå‡†è¦ç•¥å¾®å¥½ä¸€äº›ã€‚ä½†æ˜¯ï¼Œåœ¨å›¾9dä¸­ï¼Œå…¶ä¸­çº¦æŸæ›´åŠ ä¸¥æ ¼ï¼ŒBoTorchå’ŒHyperMapperæ²¡æœ‰ç»™å‡ºè¶³å¤Ÿå¯è¡Œçš„é…ç½®ä»¥æ›´æ–°Pareto frontã€‚ä¸BoTorchå’ŒHyperMapperç›¸æ¯”ï¼ŒOpenBoxåœ¨æ±‚è§£å¸¦çº¦æŸçš„å¤šç›®æ ‡é—®é¢˜ä¸­æ—¶ï¼Œæœ‰æ›´ç¨³å®šçš„æ€§èƒ½ã€‚

### 6.3 Results on AutoML Tuning Tasks

6.3.1 AutoML Tuning on 25 OpenML datasets. Figure 11 demonstrates the universality and stability of OpenBox in 25 AutoML tuning tasks. We compare OpenBox with SMAC3 and Hyperopt on LibSVM since only these two baselines support CATEGORICAL parameters with conditions. In general, OpenBox is capable of handling different types of input parameters while achieving the best median performance among the baselines considered.

åœ¨25ä¸ªOpenMLæ•°æ®é›†ä¸Šçš„AutoML Tuningã€‚å›¾11å±•ç¤ºäº†OpenBoxåœ¨25ä¸ªAutoML tuningä»»åŠ¡ä¸Šçš„ç»Ÿä¸€æ€§å’Œç¨³å®šæ€§ã€‚æˆ‘ä»¬å°†OpenBoxä¸SMAC3å’ŒHyperoptåœ¨LibSVMä¸Šè¿›è¡Œæ¯”è¾ƒï¼Œå› ä¸ºåªæœ‰è¿™ä¸¤ä¸ªåŸºå‡†æ”¯æŒå¸¦æ¡ä»¶çš„CATEGORICALå‚æ•°ã€‚ä¸€èˆ¬æ¥è¯´ï¼ŒOpenBoxå¯ä»¥å¤„ç†ä¸åŒçš„è¾“å…¥å‚æ•°ç±»å‹ï¼ŒåŒæ—¶åœ¨è€ƒè™‘çš„åŸºå‡†ä¸Šè·å¾—æœ€å¥½çš„ä¸­å€¼æ€§èƒ½ã€‚

6.3.2 Parallel Experiments. To evaluate OpenBox with parallel settings, we conduct an experiment to tune the hyper-parameters of LightGBM on Optdigits with a budget of 600 seconds. Figure 11(a) shows the average validation error with different parallel settings. We observe that the asynchronous mode with 8 workers achieves the best results and outperforms Random Search with 8 workers by a wide margin. It brings a speedup of 8Ã— over the sequential mode, which is close to the ideal speedup. In addition, although the synchronous mode brings a certain improvement over the sequential mode in the beginning, the convergence result is usually worse than the asynchronous mode due to stragglers.

å¹¶è¡Œè¯•éªŒã€‚ä¸ºåœ¨å¹¶è¡Œçš„è®¾ç½®ä¸‹è¯„ä¼°OpenBoxï¼Œæˆ‘ä»¬è¿›è¡Œäº†ä¸€ä¸ªè¯•éªŒï¼Œåœ¨Optdigitsä¸Šç”¨600sçš„é¢„ç®—ä¸Šå¯¹LightGBMè¿›è¡Œè¶…å‚æ•°è°ƒèŠ‚ã€‚å›¾11aæ˜¯åœ¨ä¸åŒçš„å¹¶è¡Œè®¾ç½®ä¸‹çš„å¹³å‡éªŒè¯è¯¯å·®ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œæœ‰8ä¸ªworkersçš„å¼‚æ­¥æ¨¡å¼è·å¾—äº†æœ€å¥½çš„ç»“æœï¼Œæ¯”8ä¸ªworkersçš„éšæœºæœç´¢å¥½äº†å¾ˆå¤šã€‚æ¯”é¡ºåºæ¨¡å¼å¸¦æ¥äº†8xçš„åŠ é€Ÿï¼Œä¸ç†æƒ³çš„åŠ é€Ÿæ¥è¿‘ã€‚æ­¤å¤–ï¼Œè™½ç„¶åŒæ­¥æ¨¡å¼åœ¨å¼€å§‹æ—¶æ¯”é¡ºåºæ¨¡å¼æœ‰ä¸€å®šçš„æ”¹è¿›ï¼Œæ”¶æ•›ç»“æœé€šå¸¸æ¯”å¼‚æ­¥æ¨¡å¼è¦å·®ï¼Œå› ä¸ºæœ‰stragglersã€‚

6.3.3 Transfer Learning Experiment. In this experiment, we remove all baselines except Vizier, which provides the transfer learning functionality for the traditional black-box optimization. We also add SMAC3 that provides a non-transfer reference. In addition, this experiment involves tuning LightGBM on 25 OpenML datasets, and it is performed in a leave-one-out fashion, i.e, we tune the hyperparameters of LightGBM on a dataset (target problem), while taking the tuning history on the remaining datasets as prior observations. Figure 11(b) shows the average rank for each baseline. We observe that 1) Vizier and OpenBox show improved sample efficiency relative to SMAC3 that cannot use prior knowledge from source problems, and 2) the proposed transfer learning framework in OpenBox performs better than the transfer learning algorithm used in Vizier. Furthermore, it is worth mentioning that OpenBox also supports transfer learning for the generalized black-box optimization, while Vizier does not.

è¿ç§»å­¦ä¹ çš„è¯•éªŒã€‚åœ¨è¿™ä¸ªè¯•éªŒä¸­ï¼Œæˆ‘ä»¬å»é™¤äº†æ‰€æœ‰çš„åŸºå‡†ï¼Œåªç•™ä¸‹äº†Vizierï¼Œå› ä¸ºå®ƒå¯¹ä¼ ç»Ÿçš„BBOæä¾›äº†è¿ç§»å­¦ä¹ çš„åŠŸèƒ½ã€‚æˆ‘ä»¬è¿˜åŠ å…¥äº†SMAC3ï¼Œç»™å‡ºäº†ä¸€ä¸ªéè¿ç§»çš„å‚è€ƒã€‚æ­¤å¤–ï¼Œè¿™ä¸ªè¯•éªŒæ˜¯åœ¨25ä¸ªOpenMLæ•°æ®é›†ä¸Šè°ƒèŠ‚LightGBMï¼Œè€Œä¸”æ˜¯ä»¥leave-one-outçš„æ–¹å¼è¿›è¡Œçš„è¯•éªŒï¼Œå³ï¼Œæˆ‘ä»¬è°ƒèŠ‚LightGBMåœ¨ä¸€ä¸ªæ•°æ®é›†ä¸Šçš„è¶…å‚æ•°ï¼ˆç›®æ ‡é—®é¢˜ï¼‰ï¼Œè€Œä»¥åœ¨å…¶ä½™æ•°æ®é›†ä¸Šçš„è°ƒèŠ‚å†å²ä½œä¸ºå…ˆéªŒè§‚å¯Ÿã€‚å›¾11bå±•ç¤ºäº†æ¯ä¸ªbaselineçš„å¹³å‡rankã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œ1)Vizierå’ŒOpenBoxï¼Œæ¯”SMAC3ï¼Œé‡‡æ ·æ•ˆç‡å¾—åˆ°äº†æ”¹è¿›ï¼Œå› ä¸ºSMAC3ä¸èƒ½åˆ©ç”¨å…ˆéªŒçŸ¥è¯†ï¼Œ2)æå‡ºçš„åœ¨OpenBoxä¸­çš„è¿ç§»å­¦ä¹ æ¡†æ¶ï¼Œæ¯”Vizierä¸­çš„è¿ç§»å­¦ä¹ ç®—æ³•æ•ˆæœè¦å¥½ã€‚è€Œä¸”ï¼Œå€¼å¾—æ³¨æ„çš„æ˜¯ï¼ŒOpenBoxè¿˜æ”¯æŒé€šç”¨BBOä¸­çš„è¿ç§»å­¦ä¹ ï¼Œè€ŒVizieråˆ™ä¸æ”¯æŒã€‚

## 7 Conclusion

In this paper, we have introduced a service that aims for solving generalized BBO problems â€“ OpenBox, which is open-sourced and highly efficient. We have presented new principles from a service perspective that drive the system design, and we have proposed efficient frameworks for accelerating BBO tasks by leveraging local-penalization based parallelization and transfer learning. OpenBox hosts lots of state-of-the-art optimization algorithms with consistent performance, via adaptive algorithm selection. It also offers a set of advanced features, such as performance-resource extrapolation, multi-fidelity optimization, automatic early stopping, and data privacy protection. Our experimental evaluations have also showcased the performance and efficiency of OpenBox on a wide range of BBO tasks.

æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æå‡ºäº†ä¸€ç§æœåŠ¡OpenBoxï¼Œæ±‚è§£é€šç”¨BBOé—®é¢˜ï¼Œå¼€æºè€Œä¸”é«˜æ•ˆã€‚æˆ‘ä»¬ä»æœåŠ¡çš„è§’åº¦æå‡ºäº†æ–°çš„åŸåˆ™ï¼Œé©±åŠ¨äº†ç³»ç»Ÿçš„è®¾è®¡ï¼Œæˆ‘ä»¬æå‡ºäº†é«˜æ•ˆçš„æ¡†æ¶ä»¥åŠ é€ŸBBOä»»åŠ¡ï¼Œåˆ©ç”¨äº†åŸºäºlocal-penalizationçš„å¹¶è¡ŒåŒ–å’Œè¿ç§»å­¦ä¹ ã€‚OpenBoxåŒ…å«äº†å¤§é‡ç›®å‰æœ€å¥½çš„ä¼˜åŒ–ç®—æ³•ï¼Œé€šè¿‡è‡ªåŠ¨ç®—æ³•é€‰æ‹©ï¼Œè¾¾åˆ°ä¸€ç›´å¾ˆå¥½çš„æ€§èƒ½ã€‚å®ƒè¿˜æ”¯æŒå¾ˆå¤šé«˜çº§ç‰¹å¾ï¼Œæ¯”å¦‚æ€§èƒ½èµ„æºå¤–æ’ï¼Œå¤šå¯ä¿¡åº¦ä¼˜åŒ–ï¼Œè‡ªåŠ¨æ—©åœï¼Œæ•°æ®éšç§ä¿æŠ¤ã€‚æˆ‘ä»¬çš„è¯•éªŒè¯„ä¼°åœ¨å¾ˆå¤šBBOä»»åŠ¡ä¸Šå±•ç¤ºäº†OpenBoxçš„æ€§èƒ½å’Œæ•ˆç‡ã€‚