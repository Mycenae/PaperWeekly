# A Discriminatively Trained, Multiscale, Deformable Part Model

Pedro Felzenszwalb, David McAllester, Deva Ramanan 

University of Chicago, Toyota Technological Institute at Chicago, UC Irvine

## 0. Abstract

This paper describes a discriminatively trained, multi-scale, deformable part model for object detection. Our system achieves a two-fold improvement in average precision over the best performance in the 2006 PASCAL person detection challenge. It also outperforms the best results in the 2007 challenge in ten out of twenty categories. The system relies heavily on deformable parts. While deformable part models have become quite popular, their value had not been demonstrated on difficult benchmarks such as the PASCAL challenge. Our system also relies heavily on new methods for discriminative training. We combine a margin-sensitive approach for data mining hard negative examples with a formalism we call latent SVM. A latent SVM, like a hidden CRF, leads to a non-convex training problem. However, a latent SVM is semi-convex and the training problem becomes convex once latent information is specified for the positive examples. We believe that our training methods will eventually make possible the effective use of more latent information such as hierarchical (grammar) models and models involving latent three dimensional pose.

本文描述了一个分别训练的，多尺度的，形变部位模型，进行目标检测。我们的系统与2006 PASCAL人体检测挑战的最佳性能相比，在AP上获得了两倍的改进。与2007挑战相比，在20类中的10类中，也超过了之前最好的结果。这个系统重度依赖于形变部位。虽然形变部位模型已经非常流行了，但在PASCAL挑战这样的困难基准测试中，其性能尚未得到完整展示。我们的系统还严重依赖于分别训练的新方法。我们将一种数据挖掘难分负样本的边距敏感方法与隐SVM结合到了一起。隐SVM，与隐CRF类似，都会得到一个非凸训练问题。但是，隐SVM是半凸的，一旦隐信息指定了正样本，训练问题就变得凸了。我们相信，我们的训练方法最后会可以有效的利用更多的隐信息，比如层次化语法模型，和涉及到隐三维姿态的模型。

## 1. Introduction

We consider the problem of detecting and localizing objects of a generic category, such as people or cars, in static images. We have developed a new multiscale deformable part model for solving this problem. The models are trained using a discriminative procedure that only requires bounding box labels for the positive examples. Using these models we implemented a detection system that is both highly efficient and accurate, processing an image in about 2 seconds and achieving recognition rates that are significantly better than previous systems.

我们考虑的是在静态图像中，检测并定位通用类别的目标，比如人体或车辆。我们提出了一种新的多尺度形变部位模型，解决这类问题。模型的训练使用了一种有区别的过程，只需要正样本的边界框标签。使用这些模型，我们实现了一个检测系统，既高效又准确，处理一幅图像需要大约2秒，获得的识别率比之前的系统要高出好多。

Our system achieves a two-fold improvement in average precision over the winning system [5] in the 2006 PASCAL person detection challenge. The system also outperforms the best results in the 2007 challenge in ten out of twenty object categories. Figure 1 shows an example detection obtained with our person model.

我们的系统与2006 PASCAL人体检测挑战的获胜系统相比，AP提升了2倍。与2007年的挑战相比，在20类中的10类中，也超过了之前的最好结果。图1展示了用我们的人体模型获得的检测结果例子。

The notion that objects can be modeled by parts in a deformable configuration provides an elegant framework for representing object categories [1–3,6,10,12,13,15,16,22]. While these models are appealing from a conceptual point of view, it has been difficult to establish their value in practice. On difficult datasets, deformable models are often outperformed by “conceptually weaker” models such as rigid templates [5] or bag-of-features [23]. One of our main goals is to address this performance gap.

目标可以用形变配置的部位进行建模，这种概念为表示目标类别提供了一个优雅的框架。这些模型从概念的视角非常吸引人，但在实际中确定其价值还很困难。在困难的数据集中，形变模型通常比不过概念上更弱的模型，比如刚性模板[5]，或bag-of-features[23]。我们的一个主要目标就是处理这个性能的差距。

Our models include both a coarse global template covering an entire object and higher resolution part templates. The templates represent histogram of gradient features [5]. As in [14, 19, 21], we train models discriminatively. However, our system is semi-supervised, trained with a max-margin framework, and does not rely on feature detection. We also describe a simple and effective strategy for learning parts from weakly-labeled data. In contrast to computationally demanding approaches such as [4], we can learn a model in 3 hours on a single CPU.

我们的模型包括，一个粗糙的全局模板，覆盖了整个目标，和更高分辨率的部位模板。模板表示了梯度特征的直方图[5]。就像在[14,19,21]中一样，我们有区分的训练模型。但是，我们的系统是半监督的，使用max-margin框架进行训练，并不依赖于特征检测。我们还描述了一个简单有效的从弱标记图像中学习部位的策略。与[4]这种需要很大计算量的方法相比，我们可以在单个CPU上只用3小时就学习一个模型。

Another contribution of our work is a new methodology for discriminative training. We generalize SVMs for handling latent variables such as part positions, and introduce a new method for data mining “hard negative” examples during training. We believe that handling partially labeled data is a significant issue in machine learning for computer vision. For example, the PASCAL dataset only specifies a bounding box for each positive example of an object. We treat the position of each object part as a latent variable. We also treat the exact location of the object as a latent variable, requiring only that our classifier select a window that has large overlap with the labeled bounding box.

我们工作的另一个贡献，是区分性训练的一种新方法。我们将SVMs泛化到处理隐变量，比如部位位置，提出了一种新方法，在训练时来挖掘难分负样本数据。我们相信，处理部分标记的数据是用机器学习方法处理计算机视觉问题的一个主要问题。比如，PASCAL数据集对每个目标的正样本只指定了一个边界框。我们将每个目标部位的位置当作一个隐变量。我们还将目标的精确位置也当作一个隐变量，需要我们的分类器选择一个与标记的边界框有很大重叠的窗口。

A latent SVM, like a hidden CRF [19], leads to a non-convex training problem. However, unlike a hidden CRF, a latent SVM is semi-convex and the training problem becomes convex once latent information is specified for the positive training examples. This leads to a general coordinate descent algorithm for latent SVMs.

隐SVM，与隐CRF类似，会得到一个非凸训练问题。但是，与隐CRF不一样的是，隐SVM是半凸的，一旦隐信息指定了正训练样本，训练问题就变凸了。这就可以得到对隐SVMs得到一个通用坐标下降算法。

**System Overview**. Our system uses a scanning window approach. A model for an object consists of a global “root” filter and several part models. Each part model specifies a spatial model and a part filter. The spatial model defines a set of allowed placements for a part relative to a detection window, and a deformation cost for each placement.

系统概览。我们系统使用扫描窗口的方法。一个目标的模型，包含一个全局根滤波器，和几个部位模型。每个部位模型指定了一个空间模型和一个部位滤波器。空间模型对一个部位相对于检测窗口，定义了可允许的放置的集合，对每个放置定义了一个形变代价。

The score of a detection window is the score of the root filter on the window plus the sum over parts, of the maximum over placements of that part, of the part filter score on the resulting subwindow minus the deformation cost. This is similar to classical part-based models [10,13]. Both root and part filters are scored by computing the dot product between a set of weights and histogram of gradient (HOG) features within a window. The root filter is equivalent to a Dalal-Triggs model [5]. The features for the part filters are computed at twice the spatial resolution of the root filter. Our model is defined at a fixed scale, and we detect objects by searching over an image pyramid.

一个检测窗口的分数，是窗口中根滤波器的分数，和部位不同放置的最大值的分数和，部位滤波器在得到的子窗口分数，减去形变代价。这与经典的基于部位的模型[10,13]是类似的。根滤波器和部位滤波器的评分，都是计算一个窗口中的HOG特征和权重集合的点积。根滤波器与Dalal-Triggs模型是等价的。部位滤波器的特征的计算的图像的分辨率，是根滤波器特征计算图像的两倍。我们的模型是定义在固定尺度上的，我们通过搜索一个图像金字塔来检测目标。

In training we are given a set of images annotated with bounding boxes around each instance of an object. We reduce the detection problem to a binary classification problem. Each example x is scored by a function of the form, $f_β ( x ) = max_z β·Φ(x,z)$. Here β is a vector of model parameters and z are latent values (e.g. the part placements). To learn a model we define a generalization of SVMs that we call latent variable SVM (LSVM). An important property of LSVMs is that the training problem becomes convex if we fix the latent values for positive examples. This can be used in a coordinate descent algorithm.

在训练中，给定图像的集合，用边界框对目标的每个实例进行标注。我们将检测问题，描述为一个二值分类问题。每个样本x通过一个函数来进行评分，$f_β ( x ) = max_z β·Φ(x,z)$。这里β是模型参数的向量，z是隐值（如，部位放置）。为学习一个模型，我们定义了SVM的一种泛化，我们称之为隐变量SVM (LSVM)。LSVMs的一个重要性质是，如果我们用正样本固定隐值，那么训练问题就变凸了。这可以在坐标下降算法中使用。

In practice we iteratively apply classical SVM training to triples $(⟨x_1, z_1, y_1⟩, . . . , ⟨x_n, z_n, y_n⟩)$ where $z_i$ is selected
to be the best scoring latent label for $x_i$ under the model learned in the previous iteration. An initial root filter is generated from the bounding boxes in the PASCAL dataset. The parts are initialized from this root filter.

在实践中，我们对三元体$(⟨x_1, z_1, y_1⟩, . . . , ⟨x_n, z_n, y_n⟩)$迭代的应用经典SVM训练，其中$z_i$选择为，对前一次迭代中学习的模型，分数最好的隐标签。初始根滤波器，是从PASCAL数据集的边界框生成的。部位是从这个根滤波器初始化得到的。

## 2. Model

The underlying building blocks for our models are the Histogram of Oriented Gradient (HOG) features from [5]. We represent HOG features at two different scales. Coarse features are captured by a rigid template covering an entire detection window. Finer scale features are captured by part templates that can be moved with respect to the detection window. The spatial model for the part locations is equivalent to a star graph or 1-fan [3] where the coarse template serves as a reference position.

我们的模型的潜在组成部分是[5]的HOG特征。我们在两个不同的尺度表示HOG特征。粗糙的特征通过一个刚性模板捕获，覆盖了整个检测窗口。精细的尺度特征，通过部位模板捕获，可以相对于检测窗口移动。部位位置的空间模型等价于星形图或1-fan[3]，其中粗糙的模板起到参考位置的作用。

### 2.1. HOG Representation

We follow the construction in [5] to define a dense representation of an image at a particular resolution. The image is first divided into 8x8 non-overlapping pixel regions, or cells. For each cell we accumulate a 1D histogram of gradient orientations over pixels in that cell. These histograms capture local shape properties but are also somewhat invariant to small deformations.

我们按照[5]的构建，在特定分辨率上定义图像的密集表示。图像首先分成8x8的非重叠像素区域，或单元。对每个单元，我们对单元中的像素的梯度方向，聚积一个1D直方图。这些直方图捕获的是局部形状属性，但对小的形状有一定的不变性。

The gradient at each pixel is discretized into one of nine orientation bins, and each pixel “votes” for the orientation of its gradient, with a strength that depends on the gradient magnitude. For color images, we compute the gradient of each color channel and pick the channel with highest gradient magnitude at each pixel. Finally, the histogram of each cell is normalized with respect to the gradient energy in a neighborhood around it. We look at the four 2 × 2 blocks of cells that contain a particular cell and normalize the histogram of the given cell with respect to the total energy in each of these blocks. This leads to a vector of length 9 × 4 representing the local gradient information inside a cell.

在每个像素上的梯度离散到9个方向bins的一个中，每个像素对其梯度方向进行投票，其强度依赖于梯度幅度。对于彩色图像，我们对每个像素计算每个色彩通道的梯度，并选择最高梯度幅度的通道。最后，每个单元的直方图对其邻域的梯度能量进行归一化。我们查看包含特定单元的4个2x2单元模块，将给定单元的直方图，对每个这些模块的总能量进行归一化。这就得到了长度为9x4的向量，表示单元内部的局部梯度信息。

We define a HOG feature pyramid by computing HOG features of each level of a standard image pyramid (see Figure 2). Features at the top of this pyramid capture coarse gradients histogrammed over fairly large areas of the input image while features at the bottom of the pyramid capture finer gradients histogrammed over small areas.

我们定义一个HOG特征金字塔，在标准图像金字塔的每个层次上都计算HOG特征，见图2。在这个金字塔的顶端的特征，捕获的是输入图像非常大区域的粗糙梯度直方图，而在金字塔底端的特征，捕获的是在较小区域中精细的梯度的直方图。

### 2.2. Filters

Filters are rectangular templates specifying weights for subwindows of a HOG pyramid. A w by h filter F is a vector with w × h × 9 × 4 weights. The score of a filter is defined by taking the dot product of the weight vector and the features in a w × h subwindow of a HOG pyramid.

滤波器是矩形的模板，指定了HOG金字塔的子窗口的权重。一个wxh的滤波器F是一个有w × h × 9 × 4权重的向量。一个滤波器的分数，是由权重向量和HOG金字塔的wxh子窗口的特征的点积来定义的。

The system in [5] uses a single filter to define an object model. That system detects objects from a particular class by scoring every w × h subwindow of a HOG pyramid and thresholding the scores.

[5]中的系统使用了单个滤波器来定义一个目标模型。那个系统通过对每个HOG金字塔的wxh子窗口评分，并对分数应用阈值，来从一个特定类别中检测目标。

Let H be a HOG pyramid and p = (x, y, l) be a cell in the l-th level of the pyramid. Let φ(H, p, w, h) denote the vector obtained by concatenating the HOG features in the w × h subwindow of H with top-left corner at p. The score of F on this detection window is F · φ(H, p, w, h).

令H是一个HOG金字塔，p = (x, y, l)是在金字塔l级的一个单元。令φ(H, p, w, h)表示拼接H中左上角在p的wxh子窗口的HOG特征，而得到的向量。F在这个检测窗口上的分数是F · φ(H, p, w, h)。

Below we use φ(H , p) to denote φ(H, p, w, h) when the dimensions are clear from context. 下面当通过上下文维度很明确时，我们使用φ(H , p)来表示φ(H, p, w, h)。

### 2.3. Deformable Parts

Here we consider models defined by a coarse root filter that covers the entire object and higher resolution part filters covering smaller parts of the object. Figure 2 illustrates a placement of such a model in a HOG pyramid. The root filter location defines the detection window (the pixels inside the cells covered by the filter). The part filters are placed several levels down in the pyramid, so the HOG cells at that level have half the size of cells in the root filter level.

这里，我们考虑由覆盖了整个目标的粗糙的根滤波器，和覆盖了目标更小部位的更高分辨率的部位滤波器，来定义的模型。图2描述了这样一个模型在HOG金字塔中的放置。根滤波器位置定义了检测窗口（由滤波器覆盖的单元内部的像素）。部位滤波器是放在金字塔中几个级别之下的，所以在那个级别的HOG单元，其大小是根滤波器级的单元大小的一半。

We have found that using higher resolution features for defining part filters is essential for obtaining high recognition performance. With this approach the part filters represent finer resolution edges that are localized to greater accuracy when compared to the edges represented in the root filter. For example, consider building a model for a face. The root filter could capture coarse resolution edges such as the face boundary while the part filters could capture details such as eyes, nose and mouth.

我们发现，使用更高分辨率的特征来定义部位滤波器，对于得到高识别率性能来说，是非常关键的。采用这种方法，部位滤波器表示了更精细的边缘，与根滤波器中表示的边缘相比，定位更加准确。比如，考虑构建一个面部的模型。根滤波器可能捕获更粗糙的分辨率边缘，比如面部边缘，而部位滤波器则捕获的是细节，比如眼睛，鼻子和嘴巴。

The model for an object with n parts is formally defined by a root filter F0 and a set of part models (P1 , . . . , Pn) where Pi = (Fi, vi, si, ai, bi). Here Fi is a filter for the i-th part, vi is a two-dimensional vector specifying the center for a box of possible positions for part i relative to the root position, si gives the size of this box, while ai and bi are two-dimensional vectors specifying coefficients of a quadratic function measuring a score for each possible placement of the i-th part. Figure 1 illustrates a person model.

对于一个有n个部位的目标，其模型是由根滤波器F0和部位模型(P1 , . . . , Pn)的集合定义的，其中Pi = (Fi, vi, si, ai, bi)。这里Fi是第i个部位的滤波器，vi是一个二维向量，指定了部位i相对于根位置的可能的位置的中心的二维向量，si给出这个框的大小，ai和bi是二维向量，指定了二次函数的系数，度量第i部位每个可能放置的分数。图1描述了一个人的模型。

A placement of a model in a HOG pyramid is given by z = (p0, . . . , pn), where pi = (xi, yi, li) is the location of the root filter when i = 0 and the location of the i-th part when i > 0. We assume the level of each part is such that a HOG cell at that level has half the size of a HOG cell at the root level. The score of a placement is given by the scores of each filter (the data term) plus a score of the placement of each part relative to the root (the spatial term),

在一个HOG金字塔中的一个模型的放置，由z = (p0, . . . , pn)给定，其中pi = (xi, yi, li)，当i=0时是根滤波器的位置，当i>0时是第i个部位的位置。我们假设每个部位的层次是，HOG单元在这个层次中的大小，是在根级别的HOG单元的一半。一个放置的分数，是由每个滤波器的分数（数据项），加上每个部位相对于根的放置的分数（空间项）

$$\sum_{i=0}^n F_i · φ(H,p_i) + \sum_{i=1}^n a_i · (\tilde x_i, \tilde y_i) + b_i · (\tilde x_i^2, \tilde y_i^2)$$(1)

where $(\tilde x_i, \tilde y_i) = ((x_i, y_i) -2(x, y) + v_i)/s_i$ gives the location of the i-th part relative to the root location. Both $\tilde x_i$ and $\tilde y_i$ should be between − 1 and 1. 其中给出的是第i个部位相对于根位置的位置。$\tilde x_i$和$\tilde y_i$都应当是在-1和1之间的。

There is a large (exponential) number of placements for a model in a HOG pyramid. We use dynamic programming and distance transforms techniques [9, 10] to compute the best location for the parts of a model as a function of the root location. This takes O(nk) time, where n is the number of parts in the model and k is the number of cells in the HOG pyramid. To detect objects in an image we score root locations according to the best possible placement of the parts and threshold this score.

对一个模型在HOG金字塔中，有大量的（指数级的）可能的放置。我们使用动态规划和距离变换技术来计算模型的部位作为根位置函数的最佳位置。这耗时O(nk)，其中n是模型中部位的数量，k是HOG金字塔中单元的数量。为在图像中检测目标，我们根据最佳可能的部位放置来对根位置进行评分，并对这个分数使用阈值。

The score of a placement z can be expressed in terms of the dot product, β · ψ(H, z), between a vector of model parameters β and a vector ψ(H, z),

一个放置z的分数，可以以点积的方式的来表达，β · ψ(H, z)，在模型参数β向量和向量ψ(H, z)之间的点积，

$$β = (F_0, . . . , F_n, a_1, b_1 . . . , a_n, b_n)$$
$$ψ(H, z) = (φ(H, p_0), φ(H, p_1), . . . φ(H, p_n), \tilde x_1, \tilde y_1, \tilde x_1^2, \tilde y_1^2, ..., \tilde x_n, \tilde y_n, \tilde x_n^2, \tilde y_n^2$$

We use this representation for learning the model parameters as it makes a connection between our deformable models and linear classifiers. 我们使用这种表示来学习模型参数，因为这可以将我们的形变模型和线性分类器连接起来。

One interesting aspect of the spatial models defined here is that we allow for the coefficients (ai, bi) to be negative. This is more general than the quadratic “spring” cost that has been used in previous work. 这里定义的空间模型的一个有趣方面，是我们允许系数(ai, bi)为负的。这比之前工作中用的二次弹簧代价，要更加通用。

## 3. Learning

The PASCAL training data consists of a large set of images with bounding boxes around each instance of an object. We reduce the problem of learning a deformable part model with this data to a binary classification problem. Let D = (⟨x1, y1⟩ , . . . , ⟨xn, yn⟩) be a set of labeled examples where yi ∈ {− 1, 1} and xi specifies a HOG pyramid, H(xi), together with a range, Z(xi), of valid placements for the root and part filters. We construct a positive example from each bounding box in the training set. For these examples we define Z(xi) so the root filter must be placed to overlap the bounding box by at least 50%. Negative examples come from images that do not contain the target object. Each placement of the root filter in such an image yields a negative training example.

PASCAL训练数据包含大量标注的图像，用边界框框住了一个目标的每个实例。我们将用这种数据学习一个形变部位模型的问题，变为一个二分类的问题。令D = (⟨x1, y1⟩ , . . . , ⟨xn, yn⟩)为标注样本的集合，其中yi ∈ {− 1, 1}，xi指定了HOG金字塔H(xi)，与根滤波器和部位滤波器的有效放置范围Z(xi)。我们从训练集中的每个边界框，构建了一个正样本。对这些样本，我们定义了Z(xi)，这样根滤波器的放置，必须与边界框至少重叠50%。负样本来自于不包含目标的图像。根滤波器在这样一幅图像中的每个放置，都得到了一个负训练样本。

Note that for the positive examples we treat both the part locations and the exact location of the root filter as latent variables. We have found that allowing uncertainty in the root location during training significantly improves the performance of the system (see Section 4).

注意，对于正样本，我们将部位位置和根滤波器的精确位置都看作是隐变量。我们发现，在训练过程中，允许根位置有不确定性，会显著改进系统的性能（见第4部分）。

### 3.1. Latent SVMs

A latent SVM is defined as follows. We assume that each example x is scored by a function of the form, 隐SVM定义如下。我们假设，每个样本x由于下面形式的函数进行评分

$$ f_β(x) = max_{z ∈ Z(x)} β·Φ(x, z)$$(2)

where β is a vector of model parameters and z is a set of latent values. For our deformable models we define Φ(x, z) = ψ(H(x), z) so that β·Φ(x, z) is the score of placing the model according to z. 其中β是一个模型参数向量，z是隐值的集合。对我们的形变模型，我们定义Φ(x, z) = ψ(H(x), z)，这样β·Φ(x, z)是根据z放置模型的分数。

In analogy to classical SVMs we would like to train β from labeled examples D = (⟨x1, y1⟩, . . . , ⟨xn, yn⟩) by optimizing the following objective function, 与经典SVM类比，我们想要从标注的样本D = (⟨x1, y1⟩, . . . , ⟨xn, yn⟩)中训练β，优化下面的目标函数

$$β^∗(D) = argmin_β λ||β||^2 + \sum_{i=1}^n max (0, 1−y_i f_β (x_i))$$(3)

By restricting the latent domains $Z(x_i)$ to a single choice, $f_β$ becomes linear in β, and we obtain linear SVMs as a special case of latent SVMs. Latent SVMs are instances of the general class of energy-based models [18].

将隐域$Z(x_i)$限制到单个选项中，$f_β$变成了对β是线性的，我们得到线性SVM是隐SVMs的特殊情况。隐SVMs是基于能量的模型的通用类别的实例。

### 3.2. Semi-Convexity

Note that $f_β (x)$ as defined in (2) is a maximum of functions each of which is linear in β. Hence $f_β (x)$ is convex in β. This implies that the hinge loss $max(0, 1 − y_i f_β (x_i))$ is convex in β when $y_i = −1$. That is, the loss function is convex in β for negative examples. We call this property of the loss function semi-convexity.

注意，(2)中定义的$f_β (x)$，是很多函数的最大值，每个函数与β都是线性的关系。因此$f_β (x)$对β是线性的。这暗示了，hinge损失$max(0, 1 − y_i f_β (x_i))$在$y_i = −1$的时候，对β是凸的。即，损失函数在负样本的时候，对β是凸的。我们称损失函数的这种性质为半凸性。

Consider an LSVM where the latent domains Z(xi) for the positive examples are restricted to a single choice. The loss due to each positive example is now convex. Combined with the semi-convexity property, (3) becomes convex in β. If the labels for the positive examples are not fixed we can compute a local optimum of (3) using a coordinate descent algorithm:

考虑LSVM，对正样本的隐域Z(xi)限制到了一个选择。对每个正样本的损失现在是凸的。与半凸性相结合，(3)变得对β是凸的。如果正样本的标签不是固定的，我们就可以计算(3)的局部最优值，使用坐标下降算法：

1. Holding β fixed, optimize the latent values for the positive examples $z_i = argmax_{z ∈ Z(xi)} β·Φ(x, z)$.

2. Holding {$z_i$} fixed for positive examples, optimize β by solving the convex problem defined above.

It can be shown that both steps always improve or maintain the value of the objective function in (3). If both steps maintain the value we have a strong local optimum of (3), in the sense that Step 1 searches over an exponentially large space of latent labels for positive examples while Step 2 simultaneously searches over weight vectors and an exponentially large space of latent labels for negative examples.

可以证明，这两步都会保持或改进(3)中目标函数的值。如果两步都保持这个值，那么我们就有了(3)的很强的局部最优值，即步骤1对正样本搜索了指数级大的隐标签空间，步骤2同时搜索了权重向量和对负样本的指数级大的隐标签的空间。

### 3.3. Data Mining Hard Negatives

In object detection the vast majority of training examples are negative. This makes it infeasible to consider all negative examples at a time. Instead, it is common to construct training data consisting of the positive instances and “hard negative” instances, where the hard negatives are data mined from the very large set of possible negative examples.

在目标检测中，主要的训练样本都是负样本。这就不可能同时考虑所有的负样本。这样，构建的训练数据，是由正样本和难分负样本，这就很常见了，其中难分负样本是从大量可能的负样本中挖掘出来的。

Here we describe a general method for data mining examples for SVMs and latent SVMs. The method iteratively solves subproblems using only hard instances. The innovation of our approach is a theoretical guarantee that it leads to the exact solution of the training problem defined using the complete training set. Our results require the use of a margin-sensitive definition of hard examples.

这里我们描述了数据挖掘样本的一般方法，用于SVMs和隐SVMs。这个方法只使用难分样本来迭代的求解子问题。我们的方法的创新，是从理论上保证，会得到使用完整的训练集的训练问题的精确解。我们的结果需要使用对边界敏感的难分样本的定义。

The results described here apply both to classical SVMs and to the problem defined by Step 2 of the coordinate descent algorithm for latent SVMs. We omit the proofs of the theorems due to lack of space. These results are related to working set methods [17].

这里描述的结果，对经典SVMs使用，对步骤2定义的对隐SVMs的坐标下降算法也适用。我们因为篇幅原因省略了证明。这些结果与工作集方法相关[17]。

We define the hard instances of D relative to β as, 我们定义D对β的难分样本为

$$M(β, D) = \{ ⟨x, y⟩ ∈ D | yf_β (x) ≤ 1 \}$$(4)

That is, M(β, D) are training examples that are incorrectly classified or near the margin of the classifier defined by β. We can show that $β^∗(D)$ only depends on hard instances. 即，M(β, D)是不正确分类的训练样本，或接近由β定义的分类器的边界。我们可以证明，$β^∗(D)$只依赖于难分样本。

**Theorem 1**. Let C be a subset of the examples in D. If M(β∗(D), D) ⊆ C then β∗(C) = β∗(D).

This implies that in principle we could train a model using a small set of examples. However, this set is defined in terms of the optimal model β∗(D). 这暗示了，原则上我们可以利用小样本集合来训练模型。但是，这个集合要根据最优模型β∗(D)来定义。

Given a fixed β we can use M(β, D) to approximate M(β∗(D), D). This suggests an iterative algorithm where we repeatedly compute a model from the hard instances defined by the model from the last iteration. This is further justified by the following fixed-point theorem. 给定固定的β，我们可以使用M(β, D)来近似M(β∗(D), D)。这说明可以有一个迭代算法，我们从难分样本中重复的计算一个模型，这些难分样本是由上一次迭代的模型定义的。这进一步由下面的定点定理来证明是正确的。

**Theorem 2**. If β∗(M(β, D)) = β then β = β∗(D).

Let C be an initial “cache” of examples. In practice we can take the positive examples together with random negative examples. Consider the following iterative algorithm: 令C是样本的初始缓存。在实践中，我们将正样本与随机的负样本放到一起。考虑下面的迭代算法：

1. Let β: = β∗(C).
2. Shrink C by letting C: = M(β, C).
3. Grow C by adding examples from M(β, D) up to a memory limit L.

Theorem 3. If |C| < L after each iteration of Step 2, the algorithm will converge to β = β∗(D) in finite time.

### 3.4. Implementation details

Many of the ideas discussed here are only approximately implemented in our current system. In practice, when training a latent SVM we iteratively apply classical SVM training to triples ⟨x1, z1, y1⟩, . . ., ⟨xn, zn, yn⟩ where zi is selected to be the best scoring latent label for xi under the model trained in the previous iteration. Each of these triples leads to an example ⟨Φ(xi, zi), yi⟩ for training a linear classifier. This allows us to use a highly optimized SVM package (SVMLight [17]). On a single CPU, the entire training process takes 3 to 4 hours per object class in the PASCAL datasets, including initialization of the parts.

这里讨论的很多思想，在我们目前的系统中都只是大致实现的。在实践中，当训练一个隐SVM时，我们迭代的应用经典SVM来训练三元组⟨x1, z1, y1⟩, . . ., ⟨xn, zn, yn⟩，其中zi是xi在前一次迭代训练的模型下，最佳得分的隐标签。每个三元组都会得到一个样本⟨Φ(xi, zi), yi⟩，用于线性分类器的训练。这使我们可以使用高度优化的SVM包(SVMLight[17])。在单个CPU上，对PASCAL数据集中的每个目标类别，整个训练过程耗时3到4小时，包括部位的初始化。

**Root Filter Initialization**: For each category, we automatically select the dimensions of the root filter by looking at statistics of the bounding boxes in the training data. We train an initial root filter F0 using an SVM with no latent variables. The positive examples are constructed from the unoccluded training examples (as labeled in the PASCAL data). These examples are anisotropically scaled to the size and aspect ratio of the filter. We use random subwindows from negative images to generate negative examples.

根滤波器初始化。对每个类别，我们通过查看训练数据中的边界框的统计数据，自动选择根滤波器的维度。我们训练初始根滤波器F0，使用没有隐变量的SVM。正样本是从无遮挡的训练样本中构建的（就像在PASCAL数据中标注的）。这些样本经过各向异性缩放，达到滤波器的大小和纵横比。我们从负图像中使用随机子窗口，以生成负样本。

**Root Filter Update**: Given the initial root filter trained as above, for each bounding box in the training set we find the best-scoring placement for the filter that significantly overlaps with the bounding box. We do this using the original, un-scaled images. We retrain F0 with the new positive set and the original random negative set, iterating twice.

根滤波器更新。上面给定初始根滤波器后，对训练集中的每个边界框，我们对滤波器找到最佳评分的放置，与边界框与显著的重叠。我们使用原始的，未缩放的图像来进行。我们用新的正样本集合和原始的随机负样本集合重新训练F0，迭代两次。

**Part Initialization**: We employ a simple heuristic to initialize six parts from the root filter trained above. First, we select an area a such that 6a equals 80% of the area of the root filter. We greedily select the rectangular region of area a from the root filter that has the most positive energy. We zero out the weights in this region and repeat until six parts are selected. The part filters are initialized from the root filter values in the subwindow selected for the part, but filled in to handle the higher spatial resolution of the part. The initial deformation costs measure the squared norm of a displacement with ai = (0, 0) and bi = −(1, 1).

部位初始化。我们采用一个简单的启发式来从上面训练的根滤波器中初始化6个部位。首先，我们选择一个区域a，使6a要等于根滤波器80%的面积。我们从根滤波器中贪婪的选择a的矩形区域，使其有最多的正能量。我们将这个区域中的权重置为0，并重复，直到选择了6个部位。部位滤波器从根滤波器值中初始化，在选择用于该部位的子窗口中初始化，但填充以用于处理更高分辨率的该部位。初始形变代价度量的是ai = (0, 0)和bi = −(1, 1)的偏移的平方范数。

**Model Update**: To update a model we construct new training data triples. For each positive bounding box in the training data, we apply the existing detector at all positions and scales with at least a 50% overlap with the given bounding box. Among these we select the highest scoring placement as the positive example corresponding to this training bounding box (Figure 3). Negative examples are selected by finding high scoring detections in images not containing the target object. We add negative examples to a cache until we encounter file size limits. A new model is trained by running SVMLight on the positive and negative examples, each labeled with part placements. We update the model 10 times using the cache scheme described above. In each iteration we keep the hard instances from the previous cache and add as many new hard instances as possible within the memory limit. Toward the final iterations, we are able to include all hard instances, M(β, D), in the cache.

模型更新。为更新模型，我们构建新的训练数据三元组。对训练数据中每个正边界框，我们在所有位置使用现有的检测器，对给定的边界框有50%重叠的进行缩放。在这些中，我们选择评分最高的放置，作为对应这个训练边界框的正样本（图3）。负样本的选择，是通过找到在图像中不包含目标的高分检测。我们将负样本加入到缓存中，直到我们达到文件大小限制。新模型的训练是在正样本和负样本上运行SVMLight，每个都用部位放置进行标记。我们使用上面描述的缓存机制更新模型10次。在每次迭代中，我们保存上一次缓存的难分样本，加入尽可能多的新的难分样本，只要在内存限制范围内。直到最后的迭代，我们要可以包含所有难分样本M(β, D)在缓存中。

## 4. Results

We evaluated our system using the PASCAL VOC 2006 and 2007 comp3 challenge datasets and protocol. We refer to [7, 8] for details, but emphasize that both challenges are widely acknowledged as difficult testbeds for object detection. Each dataset contains several thousand images of real-world scenes. The datasets specify ground-truth bounding boxes for several object classes, and a detection is considered correct when it overlaps more than 50% with a ground-truth bounding box. One scores a system by the average precision (AP) of its precision-recall curve across a testset.

我们使用PASCAL 2006和2007 comp3挑战数据集和规则来评估我们的系统。我们参考[7,8]的细节，但强调两个挑战都是广泛承认的目标检测的困难测试。每个数据集包含数千幅真实世界场景的图像。数据集对几个目标类别指定真值边界框，一个检测结果，当其与真值边界框重叠超过50%时，就认为是正确的。对一个系统的评分，采用在测试集上的精度-召回曲线的平均精度(AP)。

Recent work in pedestrian detection has tended to report detection rates versus false positives per window, measured with cropped positive examples and negative images without objects of interest. These scores are tied to the resolution of the scanning window search and ignore effects of non-maximum suppression, making it difficult to compare different systems. We believe the PASCAL scoring method gives a more reliable measure of performance.

最近在行人检测上的工作，倾向于给出每个窗口中的检测率vs假阳性率，使用剪切的正样本和没有感兴趣目标的负图像进行度量。这些分数与扫描窗口搜索的分辨率是关联的，忽略了NMS的效果，使其难以比较不同的系统。我们相信PASCAL的评分方法会给出性能的更可靠度量。

The 2007 challenge has 20 object categories. We entered a preliminary version of our system in the official competition, and obtained the best score in 6 categories. Our current system obtains the highest score in 10 categories, and the second highest score in 6 categories. Table 1 summarizes the results.

2007挑战有20个目标类别。我们将我们的系统的初步版本送入了官方比赛，在6个类别中得到了最佳分数。我们目前的系统在10个类别中得到了最高的分数，在6个类别中得到了第二高的分数。表1总结了结果。

Our system performs well on rigid objects such as cars and sofas as well as highly deformable objects such as persons and horses. We also note that our system is successful when given a large or small amount of training data. There are roughly 4700 positive training examples in the person category but only 250 in the sofa category. Figure 4 shows some of the models we learned. Figure 5 shows some example detections.

我们的系统在刚性目标上表现很好，比如车辆和沙发，在高度可形变目标上表现也很好，如人和马。我们还指出，当训练数据量很多或较少时，我们的系统都是非常成功的。在人体的类别中，大约有4700幅正训练样本，但在沙发类别中，只有250个。图4展示了我们学习的一些模型。图5展示了一些例子检测结果。

We evaluated different components of our system on the longer-established 2006 person dataset. The top AP score in the PASCAL competition was .16, obtained using a rigid template model of HOG features [5]. The best previous result of .19 adds a segmentation-based verification step [20]. Figure 6 summarizes the performance of several models we trained. Our root-only model is equivalent to the model from [5] and it scores slightly higher at .18. Performance jumps to .24 when the model is trained with a LSVM that selects a latent position and scale for each positive example. This suggests LSVMs are useful even for rigid templates because they allow for self-adjustment of the detection window in the training examples. Adding deformable parts increases performance to .34 AP — a factor of two above the best previous score. Finally, we trained a model with parts but no root filter and obtained .29 AP. This illustrates the advantage of using a multiscale representation.

我们在更稳定的2006人体数据集上，评估了我们系统的不同组成部分。在PASCAL竞赛中，最高的AP分数是.16，使用HOG特征的刚性模板模型得到的。加上一个基于分割的验证步骤，之前最好的结果是.19。图6总结了我们训练的几个模型的性能。我们的只有根的模型等价于[5]的模型，评分略高，为.18。当模型使用LSVM训练，对每个正样本选择一个隐位置和尺度，性能提升到.24，这说明，即使是对于刚性模板，LSVMs也是有用的，因为允许训练样本中检测窗口的自我调整。加入形变部位将性能提升到.34 AP，这是之前最好分数的两倍。最后，我们训练了一个只有部位，没有根的滤波器，得到了.29 AP。这说明了使用多尺度表示的优势。

We also investigated the effect of the spatial model and allowable deformations on the 2006 person dataset. Recall that si is the allowable displacement of a part, measured in HOG cells. We trained a rigid model with high-resolution parts by setting si to 0. This model outperforms the root-only system by .27 to .24. If we increase the amount of allowable displacements without using a deformation cost, we start to approach a bag-of-features. Performance peaks at si = 1, suggesting it is useful to constrain the part displacements. The optimal strategy allows for larger displacements while using an explicit deformation cost. The following table shows AP as a function of freely allowable deformation in the first three columns. The last column gives the performance when using a quadratic deformation cost and an allowable displacement of 2 HOG cells.

我们还研究了空间模型和可允许的形变在2006人体数据集上的效果。回忆一下，si是一个部位的可允许的偏移，在HOG单元中度量的。我们用高分辨率部位训练一个刚性模型，设si为0。这个模型超过了只有根的系统，分数为.27比.24。如果我们增加可允许的偏移，不使用形变代价，我们开始接近bag-of-features。在si=1时性能达到最佳，说明对部位偏移进行约束是有用的。最佳策略允许更大的偏移，同时使用一个显式的形变代价。下表的前3列展示了AP作为自由可允许形变的函数。最后一列给出的性能是，使用二次形变代价和2个HOG单元的可允许的偏移时，得到的结果。

si | 0 | 1 | 2 | 3 | 2+quadratic cost
--- | --- | --- | --- | --- | ---
AP | .27 | .33 | .31 | .31 | .34

## 5. Discussion

We introduced a general framework for training SVMs with latent structure. We used it to build a recognition system based on multiscale, deformable models. Experimental results on difficult benchmark data suggests our system is the current state-of-the-art in object detection.

我们提出了一种通用框架，使用隐结构来训练SVMs。我们将其用于构建基于多尺度、形变模型的识别系统。在困难基准测试数据上的试验结果表明，我们的系统在目标检测中是目前最好的。

LSVMs allow for exploration of additional latent structure for recognition. One can consider deeper part hierarchies (parts with parts), mixture models (frontal vs. side cars), and three-dimensional pose. We would like to train and detect multiple classes together using a shared vocabulary of parts (perhaps visual words). We also plan to use A* search [11] to efficiently search over latent parameters during detection.

LSVMs允许探索额外的隐结构进行识别。可以考虑更深的部位层次结构（带有部位的部位），混合模型（正面的和侧面的车），和3D姿态。我们可以使用共享的部位词汇表（可能是视觉词）训练和检测同时多个类别。我们还计划使用A*搜索来在检测中高效的搜索隐参数。